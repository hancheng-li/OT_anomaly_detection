{"cells":[{"cell_type":"markdown","metadata":{"id":"a9Ag7aRGHD5u"},"source":["This notebook centrally defines the various functions that are used for plotting, handling data, and calculating OT distances."]},{"cell_type":"markdown","metadata":{"id":"7UpoaxeIHQzd"},"source":["Overview of functions (note that not all functions are used in this analysis):\n","\n","**OT calculating functions:**\n","\n","*   `calcGroundCostMatrix()`\n","    -   `calcGroundCostMatrix_1DpT()`\n","    -   `calcGroundCostMatrix_2D()`\n","    -   `calcGroundCostMatrix_3D()`\n","*   `calcOTDistance()`\n","*   `calcOTDistance_non_square()`\n","*   `parallel_OT_non_square()`\n","*   `event_to_ensemble_dist()`\n","*   `calcIndividualOTScores()`\n","*   `checkSCHEMES()`\n","*   `getMasses()`\n","\n","**Data handling functions:**\n","\n","*   `roundToSigFig()`\n","*   `randomDataSample()`\n","*   `calcROCmetrics()`\n","*   `calcAUC()`\n","*   `indxOfCertainTPR()`\n","*   `getRepeatAvStd()`\n","*   `calcWeightedComboOTscores()`\n","*   `getFractionsOfMax()`\n","*   `maxIndividualOTScore()`\n","*   `calcMultiplicityData()`\n","*   `calcMaxAvSI()`\n","*   `calcMaxAvSI_helper()`\n","*   `calcMaxAvF1()`\n","*   `calcMaxAvF1_helper()`\n","\n","**Background data augmentation functions:**\n","\n","*   `check_data()`\n","*   `add_one_hot_columns()`\n","*   `format_for_analysis()`\n","*   `augmentAndSaveData()`\n","*   `neg_augs()`\n","*   `add_objects()` (augmentation 1)\n","*   `add_objects_constptmet()` (augmentation 2)\n","      - `get_std_rivet()`\n","      - `etaphi_smear_events()`\n","      - `collinear_fill_e_mu()`\n","      - `collinear_fill_jets()`\n","*   `shift_met_or_pt()` (augmentation 3)\n","*   `aug_2_jetConditions()`\n","*   `aug_2_electronConditions()`\n","*   `aug_2_muonsConditions()`\n","*   `aug_2_conditions()`\n","\n","**Plotting functions:**\n","\n","*   Misc. helper functions\n","    - `RGBAtoRGBAtuple()`\n","*   Data plotting functions\n","    - `plotDataHists()`\n","    - `plotMultiplicityData()`\n","    - `plotDataAugHists()`\n","*   OT results plotting functions\n","    - `plotScoreHists()`\n","    - `plotMaxIndividualOTScoresPerEvent()`\n","*   Performance metric plotting functions\n","    - Repeat runs\n","      - `plotROCcurve_errorBand()`\n","      - `plotROCcurve_errorBand_specificMethod()`\n","      - `plotInvROCcurve_errorBand()`\n","      - `plotSIcurve_errorBand()`\n","      - `plotSIcurve_errorBand_specificMethod()`\n","    - Single test run\n","      - `plotROCcurve()`\n","      - `plotInvROCcurve()`\n","      - `plotSIcurve()`\n","\n","\n","**Machine learning functions:**\n","\n","* SVM Classification\n","   - `SVM_ROC_Metrics()`\n","   - `SVM_Classification_With_Best_Hyperparameters()`\n","* kNN Classification\n","   - `kNN_with_score_list()`\n","   - `kNN_with_distance_matrix()`\n","   - `kNN_cross_validation()`\n","   - `rNN_with_distance_matrix()`\n","   - `rNN_cross_validation()`\n","   - `SVM_with_distance_matrix()`\n","   - `SVM_cross_validation()`\n","   - `OneClassSVM_with_distance_matrix()`\n","   - `kNN_ROC_metrics()`"]},{"cell_type":"markdown","metadata":{"id":"HzPInTTSYm2D"},"source":["# Import libraries"]},{"cell_type":"code","source":["!pip install POT"],"metadata":{"id":"Y0rDfaaYUugl"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-55fN2QHrsq"},"outputs":[],"source":["import numpy as np\n","from numpy.random import RandomState\n","import numpy.ma as ma\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.gridspec import GridSpec\n","%matplotlib inline\n","\n","import h5py\n","import ot  # PyOT library: https://pythonot.github.io/index.html\n","from numpy.random import Generator, PCG64\n","from sklearn import metrics\n","import itertools\n","\n","from sklearn.neighbors import RadiusNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_curve, roc_auc_score\n","from sklearn.svm import LinearSVC\n","from sklearn.calibration import CalibratedClassifierCV\n","from sklearn.neighbors import KNeighborsClassifier\n","from tqdm import tqdm\n","from sklearn import svm\n","import concurrent.futures as cf"]},{"cell_type":"markdown","metadata":{"id":"8-zeI8htLCVt"},"source":["# OT calculating functions"]},{"cell_type":"markdown","metadata":{"id":"_0WhCombNy6Q"},"source":["#### `calcGroundCostMatrix()`\n","\n","Also contains\n","- `calcGroundCostMatrix_1DpT()`\n","- `calcGroundCostMatrix_2D()`\n","- `calcGroundCostMatrix_3D()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G7XCF_EiRKyM"},"outputs":[],"source":["def calcGroundCostMatrix(xs, xt, COSTSCHEME):\n","  \"\"\"\n","  Calculate ground cost matrix between xs and xt\n","\n","  Inputs:\n","             xs:  3D (pT, eta, phi) coordinates of nx source particles (shape=(nx,3));\n","                  nx = number of entries in xEvents\n","             xt:  3D (pT, eta, phi) coordinates of ny source particles (shape=(ny,3));\n","                  ny = number of entries in yEvents\n","     COSTSCHEME:  Determines what scheme will be used to calculate the ground cost matrix. Options are:\n","                  - 1DpT: Ground space is pT only\n","                  - 2D:   Ground space is 2D (eta,phi); note mass is pT\n","                  - 3D:   Ground space is 3D (pT,eta,phi); note mass is uniform\n","\n","  Output:\n","     Returns matrix of pair-wise |x - y|^2 distances\n","\n","  \"\"\"\n","  if COSTSCHEME=='1DpT':\n","    return calcGroundCostMatrix_1DpT(xs, xt)\n","  elif COSTSCHEME=='2D':\n","    return calcGroundCostMatrix_2D(xs, xt)\n","  elif COSTSCHEME=='3D':\n","    return calcGroundCostMatrix_3D(xs, xt)\n","  else:\n","    print(\"Error: Invalid COSTSCHEME\")\n","    return 0"]},{"cell_type":"markdown","metadata":{"id":"Wa-CSjeUijlJ"},"source":["##### Auxiliary functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gX0KC5vDLyX_"},"outputs":[],"source":["def calcGroundCostMatrix_1DpT(xs, xt):\n","  \"\"\"\n","  Auxiliary function, assumes ground space is pT only\n","  \"\"\"\n","  deltaPT      = xs[:,0,None] - xt[:,0]\n","  return deltaPT**2"]},{"cell_type":"markdown","metadata":{"id":"Q8gj9PezUvua"},"source":["**Note:** The following functions were modified from Tianji Cai's code below:\n","\n","```\n","d_phis     = np.pi - np.abs(np.pi - np.abs(jet1_coords[:,1,None]-jet2_coords[:,1]))\n","squareDist = (jet1_coords[:,0,None]-jet2_coords[:,0])**2 + d_phis**2\n","```\n","\n","But also note that according to [this thread](https://stackoverflow.com/questions/1878907/how-can-i-find-the-smallest-difference-between-two-angles-around-a-point), this way of calculating modular differences may fail if deltaPhi_raw > 360 deg (2 pi).\n","However, since the input is between -pi and pi, the max possible is 2pi so it shouldn't be a problem. It is also unsigned, however since we will square it that doesn't matter."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hlGHpzNxRGnL"},"outputs":[],"source":["def calcGroundCostMatrix_2D(xs, xt):\n","  \"\"\"\n","  Auxiliary function, assumes ground space is eta,phi.\n","  phi is 2pi modular which must be taken into account when computing the matrix of\n","  pair-wise |x - y|^2 distances\n","  \"\"\"\n","  deltaEta     = xs[:,1,None] - xt[:,1]\n","  deltaPhi_raw = xs[:,2,None] - xt[:,2]\n","  deltaPhi     = np.pi - np.abs(np.pi - np.abs(deltaPhi_raw))\n","  return deltaEta**2 + deltaPhi**2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6mkSW43ZTY7j"},"outputs":[],"source":["def calcGroundCostMatrix_3D(xs, xt):\n","  \"\"\"\n","  Auxiliary function, assumes ground space is pT,eta,phi.\n","  phi is 2pi modular which must be taken into account when computing the matrix of\n","  pair-wise |x - y|^2 distances\n","  \"\"\"\n","  deltaPT      = xs[:,0,None] - xt[:,0]\n","  deltaEta     = xs[:,1,None] - xt[:,1]\n","  deltaPhi_raw = xs[:,2,None] - xt[:,2]\n","  deltaPhi     = np.pi - np.abs(np.pi - np.abs(deltaPhi_raw))\n","  return deltaPT**2 + deltaEta**2 + deltaPhi**2"]},{"cell_type":"markdown","source":["#### `calcOTDistance()`"],"metadata":{"id":"sjOxuv0C03SM"}},{"cell_type":"markdown","metadata":{"id":"1m9QSH68XjcH"},"source":["**Note:** We're using the following references from the POT library ([here](https://pythonot.github.io/auto_examples/plot_OT_2D_samples.html#sphx-glr-auto-examples-plot-ot-2d-samples-py) and [here](https://pythonot.github.io/quickstart.html#solving-optimal-transport))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"roJdpjPGRAv9"},"outputs":[],"source":["def calcOTDistance(xEvents, yEvents, OTSCHEME, COSTSCHEME, kwargs={}, Matrix = False):\n","  \"\"\"\n","  Solve the optimal transport problem and find the 2-Wasserstein distance\n","  for a set of source events (xEvents) and target events (yEvents) for a\n","  given ground cost function.\n","\n","  Inputs:\n","        xEvents:  Array of sample of x-type events; shape (N, 19, 3)\n","        yEvents:  Array of sample of y-type events; shape (N, 19, 3)\n","       OTSCHEME:  Determines what scheme will be used to calculate the OT distance.\n","                  Note the exact meaning varies somewhat depending on the choice of COSTSCHEME.\n","                  It's a dictionary of 3 booleans cooresponding to whether the PT is normalized,\n","                  whether the OT calculation is balanced, and whether the zero padding should be\n","                  removed. Namely,\n","                  OTSCHEME['normPT']      ==True:  Means that the pT should be normalized;\n","                                         ==False:  Means that the pT should be unnormalized\n","                  OTSCHEME['balanced']    ==True:  Means that the OT calculation should be balanced;\n","                                         ==False:  Means that the OT calculation should be unbalanced\n","                  OTSCHEME['noZeroPad']   ==True:  Means that the zero padding should be removed;\n","                                         ==False:  Means that the zero padding should be kept\n","                  OTSCHEME['individualOT']==True:  Means that the OT calculation is done on each species separately;\n","                                         ==False:  Means that the OT calculation is done ignoring species type\n","     COSTSCHEME:  Determines what scheme will be used to calculate the ground cost matrix. Options are:\n","                  - 1DpT: Ground space is pT only\n","                  - 2D:   Ground space is 2D (eta,phi); note mass is pT\n","                  - 3D:   Ground space is 3D (pT,eta,phi); note mass is uniform\n","         kwargs:  Dictionary containing keyword arguments for unbalanced OT calculation; default is an empty dictionary\n","         MATRIX:  Whether to return wXY as an (N,N) array or as a (N*N,) array\n","\n","  Outputs\n","     wXY:  Array of the corresponding (squared) 2-Wasserstein distances\n","  \"\"\"\n","\n","  #-- Sanity checks on inputs --#\n","\n","  # Check schemes\n","  assert checkSCHEMES(OTSCHEME, COSTSCHEME)==True\n","  if OTSCHEME['balanced']==False:\n","    assert all([x in list(kwargs.keys()) for x in ['div', 'reg_m'] ])\n","\n","  # Get number of signal and background events #! Make more general later to handle different number of x and y events\n","  assert(xEvents.shape[0] == yEvents.shape[0])\n","  N = xEvents.shape[0]\n","\n","  #-- Preliminaries --#\n","  # Create objects for outputs\n","  wXY = np.zeros(shape=(N,N))\n","\n","  #-- Loop over pairs of events --#\n","  # We use itertools to make looping more efficient (i.e. do double for loops)\n","  dummyArr = np.arange(N)\n","  wXY_list = []\n","  for (i, j) in tqdm(itertools.product(dummyArr, dummyArr), total=len(dummyArr)**2): #! Implement diagonal+upper-triangle restriction to save on computation time\n","    #-- Get source and target data points--#\n","    # Remove zero-padding if specified\n","    if OTSCHEME['noZeroPad']==True:\n","      sMask = ~np.all(xEvents[i, :, :] == 0., axis=1) # Mask to remove zero rows\n","      tMask = ~np.all(yEvents[j, :, :] == 0., axis=1)\n","\n","      xs = xEvents[i, :, :][sMask] # \"source data points\"\n","      xt = yEvents[j, :, :][tMask] # \"target data points\"\n","    else:\n","      xs = xEvents[i, :, :] # \"source data points\"\n","      xt = yEvents[j, :, :] # \"target data points\"\n","\n","    #-- Get masses --#\n","    a,b = getMasses(xs, xt, OTSCHEME, COSTSCHEME)\n","\n","    #-- Normalize pT coordinate if specified --#\n","    #! Might be more efficient to do this before getMasses() but then will need to change getMasses()\n","    if OTSCHEME['normPT']==True:\n","      totPTs, totPTt = xs[:,0].sum(), xt[:,0].sum()\n","      xs[:,0], xt[:,0] = xs[:,0]/totPTs, xt[:,0]/totPTt\n","\n","    #-- Get cost function and append to list --#\n","    cxy = calcGroundCostMatrix(xs, xt, COSTSCHEME)\n","\n","    #-- Calculate the unbalanced Wasserstein distance --#\n","    if OTSCHEME['balanced']==False:\n","      if j >= i:\n","        wXY[i,j] = ot.unbalanced.mm_unbalanced2(a, b, cxy, reg_m=kwargs['reg_m'], div=kwargs['div'])\n","        wXY_list.append(wXY[i,j])\n","      else:\n","        wXY[i,j] = wXY[j,i]\n","    else:\n","      if j >= i:\n","        wXY[i,j] = ot.emd2(a, b, cxy)\n","        wXY_list.append(wXY[i,j])\n","      else:\n","        wXY[i,j] = wXY[j,i]\n","\n","  if Matrix == True:\n","    return wXY\n","\n","  else:\n","    return np.asarray(wXY_list)"]},{"cell_type":"markdown","metadata":{"id":"quKuyCqjN_4K"},"source":["#### `calcOTDistance_non_square()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KNsSSWtIrcyN"},"outputs":[],"source":["def calcOTDistance_non_square(xEvents, yEvents, OTSCHEME, COSTSCHEME, kwargs={}, Matrix = False):\n","  \"\"\"\n","  Solve the optimal transport problem and find the 2-Wasserstein distance\n","  for a set of source events (xEvents) and target events (yEvents) for a\n","  given ground cost function.\n","\n","  Inputs:\n","        xEvents:  Array of sample of x-type events; shape (N, 19, 3)\n","        yEvents:  Array of sample of y-type events; shape (M, 19, 3)\n","       OTSCHEME:  Determines what scheme will be used to calculate the OT distance.\n","                  Note the exact meaning varies somewhat depending on the choice of COSTSCHEME.\n","                  It's a dictionary of 3 booleans cooresponding to whether the PT is normalized,\n","                  whether the OT calculation is balanced, and whether the zero padding should be\n","                  removed. Namely,\n","                  OTSCHEME['normPT']      ==True:  Means that the pT should be normalized;\n","                                         ==False:  Means that the pT should be unnormalized\n","                  OTSCHEME['balanced']    ==True:  Means that the OT calculation should be balanced;\n","                                         ==False:  Means that the OT calculation should be unbalanced\n","                  OTSCHEME['noZeroPad']   ==True:  Means that the zero padding should be removed;\n","                                         ==False:  Means that the zero padding should be kept\n","                  OTSCHEME['individualOT']==True:  Means that the OT calculation is done on each species separately;\n","                                         ==False:  Means that the OT calculation is done ignoring species type\n","     COSTSCHEME:  Determines what scheme will be used to calculate the ground cost matrix. Options are:\n","                  - 1DpT: Ground space is pT only\n","                  - 2D:   Ground space is 2D (eta,phi); note mass is pT\n","                  - 3D:   Ground space is 3D (pT,eta,phi); note mass is uniform\n","         kwargs:  Dictionary containing keyword arguments for unbalanced OT calculation; default is an empty dictionary\n","         MATRIX:  Whether to return wXY as an (N,M) array or as a (N*M,) array\n","\n","  Outputs\n","\n","     wXY:  Matrix of the corresponding (squared) 2-Wasserstein distances, or list for default\n","  \"\"\"\n","\n","  #-- Sanity checks on inputs --#\n","\n","  # Check schemes\n","  assert checkSCHEMES(OTSCHEME, COSTSCHEME)==True\n","  if OTSCHEME['balanced']==False:\n","    assert all([x in list(kwargs.keys()) for x in ['div', 'reg_m'] ])\n","\n","  N = xEvents.shape[0]\n","  M = yEvents.shape[0]\n","\n","  #-- Preliminaries --#\n","  # Create objects for outputs\n","  wXY = np.zeros(shape=(N,M))\n","\n","  #-- Loop over pairs of events --#\n","  # We use itertools to make looping more efficient (i.e. do double for loops)\n","  dummyArr_1 = np.arange(N)\n","  dummyArr_2 = np.arange(M)\n","  wXY_list = []\n","  for (i, j) in itertools.product(dummyArr_1, dummyArr_2):\n","    #-- Get source and target data points--#\n","    # Remove zero-padding if specified\n","    if OTSCHEME['noZeroPad']==True:\n","      sMask = ~np.all(xEvents[i, :, :] == 0., axis=1) # Mask to remove zero rows\n","      tMask = ~np.all(yEvents[j, :, :] == 0., axis=1)\n","\n","      xs = xEvents[i, :, :][sMask] # \"source data points\"\n","      xt = yEvents[j, :, :][tMask] # \"target data points\"\n","    else:\n","      xs = xEvents[i, :, :] # \"source data points\"\n","      xt = yEvents[j, :, :] # \"target data points\"\n","\n","    #-- Get masses --#\n","    a,b = getMasses(xs, xt, OTSCHEME, COSTSCHEME)\n","\n","    #-- Normalize pT coordinate if specified --#\n","    #! Might be more efficient to do this before getMasses() but then will need to change getMasses()\n","    if OTSCHEME['normPT']==True:\n","      totPTs, totPTt = xs[:,0].sum(), xt[:,0].sum()\n","      xs[:,0], xt[:,0] = xs[:,0]/totPTs, xt[:,0]/totPTt\n","\n","    #-- Get cost function and append to list --#\n","    cxy = calcGroundCostMatrix(xs, xt, COSTSCHEME)\n","\n","    #-- Calculate the unbalanced Wasserstein distance --#\n","    if OTSCHEME['balanced']==False:\n","      wXY[i,j] = ot.unbalanced.mm_unbalanced2(a, b, cxy, reg_m=kwargs['reg_m'], div=kwargs['div'])\n","      wXY_list.append(wXY[i,j])\n","    else:\n","      wXY[i,j] = ot.emd2(a, b, cxy)\n","      wXY_list.append(wXY[i,j])\n","\n","  if Matrix == True:\n","    return wXY\n","\n","  else:\n","    return np.asarray(wXY_list)"]},{"cell_type":"markdown","source":["#### `parallel_OT_non_square()`"],"metadata":{"id":"MINxwHZT1BxY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JAtG6PHyrcyN"},"outputs":[],"source":["def parallel_OT_non_square(event_set1, event_set2, OTSCHEME, COSTSCHEME, kwargs, num_cores = 8):\n","    \"\"\"\n","    Calls calcOTDistance_non_square() in parallel\n","\n","    Inputs:\n","     event_set1:  ?TBD?\n","     event_set2:  ?TBD?\n","       OTSCHEME:  Determines what scheme will be used to calculate the OT distance.\n","                  Note the exact meaning varies somewhat depending on the choice of COSTSCHEME.\n","                  It's a dictionary of 3 booleans cooresponding to whether the PT is normalized,\n","                  whether the OT calculation is balanced, and whether the zero padding should be\n","                  removed. Namely,\n","                  OTSCHEME['normPT']      ==True:  Means that the pT should be normalized;\n","                                         ==False:  Means that the pT should be unnormalized\n","                  OTSCHEME['balanced']    ==True:  Means that the OT calculation should be balanced;\n","                                         ==False:  Means that the OT calculation should be unbalanced\n","                  OTSCHEME['noZeroPad']   ==True:  Means that the zero padding should be removed;\n","                                         ==False:  Means that the zero padding should be kept\n","                  OTSCHEME['individualOT']==True:  Means that the OT calculation is done on each species separately;\n","                                         ==False:  Means that the OT calculation is done ignoring species type\n","     COSTSCHEME:  Determines what scheme will be used to calculate the ground cost matrix. Options are:\n","                  - 1DpT: Ground space is pT only\n","                  - 2D:   Ground space is 2D (eta,phi); note mass is pT\n","                  - 3D:   Ground space is 3D (pT,eta,phi); note mass is uniform\n","         kwargs:  Dictionary containing keyword arguments for unbalanced OT calculation; default is an empty dictionary\n","      num_cores:  Number of cores to use in parallelization\n","\n","    \"\"\"\n","    num_events = len(event_set1)\n","    assert num_events%num_cores == 0, 'Number of events must be divisible by number of cores'\n","    num_per_core = num_events//num_cores\n","    with cf.ProcessPoolExecutor() as executor:\n","\n","        futures = [executor.submit(calcOTDistance_non_square, event_set1[num_per_core*i:num_per_core*(i+1)], event_set2, OTSCHEME, COSTSCHEME, kwargs=kwargs, Matrix = True) for i in range(num_cores)]\n","\n","    results = [f.result() for f in futures]\n","\n","    distance_matrix = np.vstack(results)\n","\n","    return distance_matrix"]},{"cell_type":"markdown","source":["#### `event_to_ensemble_dist()`"],"metadata":{"id":"YzCMyiFw1FKa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVPMezGGrcyN"},"outputs":[],"source":["def event_to_ensemble_dist(wXY, EVENT_TO_ENSEMBLE_TYPE, AXIS=0):\n","    \"\"\"\n","    Calculate the event to ensemble distance.\n","    wXY should be an NxN matrix where wXY[i,j] is the OT distance between X[i] and Y[j]\n","    AXIS=0 assumes that X events are the reference population (i.e. ensemble)\n","    \"\"\"\n","    if EVENT_TO_ENSEMBLE_TYPE=='MAX':\n","        return np.max(wXY, axis=AXIS)\n","    elif EVENT_TO_ENSEMBLE_TYPE=='MIN':\n","        return np.min(wXY, axis=AXIS)\n","    elif EVENT_TO_ENSEMBLE_TYPE=='MEAN':\n","        return np.mean(wXY, axis=AXIS)\n","    else:\n","        print(\"ERROR: Invalid EVENT_TO_ENSEMBLE_TYPE \",EVENT_TO_ENSEMBLE_TYPE)\n","        return 0"]},{"cell_type":"markdown","metadata":{"id":"U1UbpU7RPUkY"},"source":["#### `calcIndividualOTScores()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vsWBfMJWOiog"},"outputs":[],"source":["def calcIndividualOTScores(trimmedDataDict, sigAliasList, OTSCHEME, COSTSCHEME, kwargs={}, speciesList=['MET', 'e', 'mu', 'jet']):\n","  \"\"\"\n","  Calculate individual OT scores and store them in a score dictionary.\n","\n","  Inputs:\n","    trimmedDataDict:  Dictionary of data trimmed to size (N, 19, 3); contains\n","                      two samples of background data ('bkgEvents1' and 'bkgEvents2')\n","                      and one sample of signal data for each signal type ('sig_A',\n","                      'sig_h0', 'sig_hch', 'sig_LQ')\n","    sigAliasList:     List of signal type aliases; i.e. ['sig_A', 'sig_h0', 'sig_hch', 'sig_LQ']\n","    speciesList:      List of species; default ['MET', 'e', 'mu', 'jet']\n","\n","  Outputs:\n","    scoreDict: Dictionary of scores for OT on each particle species\n","  \"\"\"\n","  scoreDict = {}\n","\n","  #-- Loop over particle species\n","  for species in speciesList:\n","    scoreDict[species] = {}\n","\n","    #-- Set background data according to species type --#\n","    if species == 'MET':\n","      B1_data = trimmedDataDict['bkgEvents1'][:, 0, :].reshape(-1,1,3)\n","      B2_data = trimmedDataDict['bkgEvents2'][:, 0, :].reshape(-1,1,3)\n","    elif species == 'e':\n","      B1_data = trimmedDataDict['bkgEvents1'][:, 1:5, :]\n","      B2_data = trimmedDataDict['bkgEvents2'][:, 1:5, :]\n","    elif species == 'mu':\n","      B1_data = trimmedDataDict['bkgEvents1'][:, 5:9, :]\n","      B2_data = trimmedDataDict['bkgEvents2'][:, 5:9, :]\n","    elif species == 'jet':\n","      B1_data = trimmedDataDict['bkgEvents1'][:, 9:, :]\n","      B2_data = trimmedDataDict['bkgEvents2'][:, 9:, :]\n","\n","    #-- Calculate OT distance for background-to-background case --#\n","    _, scoreDict[species]['wBB'] = calcOTDistance(B1_data, B2_data, OTSCHEME=OTSCHEME, COSTSCHEME=COSTSCHEME, kwargs=kwargs)\n","\n","    #-- Loop over signal cases to calculate OT distance in background-to-signal case --#\n","    for alias in sigAliasList:\n","\n","      #-- Set signal data according to species type --#\n","      if species == 'MET':\n","        S_data = trimmedDataDict[alias][:, 0, :].reshape(-1,1,3)\n","      elif species == 'e':\n","        S_data = trimmedDataDict[alias][:, 1:5, :]\n","      elif species == 'mu':\n","        S_data = trimmedDataDict[alias][:, 5:9, :]\n","      elif species == 'jet':\n","        S_data = trimmedDataDict[alias][:, 9:, :]\n","\n","      #-- Calculate OT distance for background-to-signal case --#\n","      name_w = 'wBS_'+alias\n","      _, scoreDict[species][name_w] = calcOTDistance(B1_data, S_data, OTSCHEME=OTSCHEME, COSTSCHEME=COSTSCHEME, kwargs=kwargs)\n","\n","  return scoreDict"]},{"cell_type":"markdown","metadata":{"id":"yNsEvX-Siv9x"},"source":["#### `checkSCHEMES()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PldekATelQZJ"},"outputs":[],"source":["def checkSCHEMES(OTSCHEME, COSTSCHEME):\n","  \"\"\"\n","  Auxiliary function to make calcOTDistance() a bit tidier while still checking that the schemes make sense\n","\n","  See calcOTDistance for definition of OTSCHEME, COSTSCHEME\n","  \"\"\"\n","\n","  #-- Check that SCHEMES contain valid entries --#\n","  assert (COSTSCHEME in ['1DpT','2D', '3D']), \"Error: Invalid COSTSCHEME\"\n","  for key in OTSCHEME.keys():\n","    assert(OTSCHEME[key] in [True, False]), \"Error: Invalid OTSCHEME\"\n","\n","\n","  #-- Check that SCHEME combinations make sense --#\n","\n","  if COSTSCHEME == '1DpT':\n","\n","    # pT assumed to be unnormalized\n","    assert (OTSCHEME['normPT']==False), \"Error: Invalid OTSCHEME['normPT'] for COSTSCHEME==%s\"%(COSTSCHEME)\n","\n","    # either balanced or unbalanced is fine\n","    # => nothing to check\n","\n","    # if doing balanced then either removing or keeping zero padding is fine => nothing to check;\n","    # if doing unbalanced then zero padding should be removed\n","    if OTSCHEME['balanced']==False:\n","      assert (OTSCHEME['noZeroPad']==True), \"Error: Invalid OTSCHEME['noZeroPad'] for COSTSCHEME==%s\"%(COSTSCHEME)\n","\n","    # check that we're not doing individual OT\n","    assert (OTSCHEME['individualOT']==False), \"Error: Invalid OTSCHEME['individualOT'] for COSTSCHEME==%s\"%(COSTSCHEME)\n","\n","  elif COSTSCHEME == '2D':\n","\n","    # either normalized or unnormalized pT is fine\n","    # => nothing to check\n","\n","    # if pT is unnormalized we must be doing unbalanced\n","    # if pT is normalized we assume we're doing balanced\n","    if OTSCHEME['normPT']==False:\n","      assert (OTSCHEME['balanced']==False), \"Error: Invalid OTSCHEME['balanced'] for COSTSCHEME==%s\"%(COSTSCHEME)\n","    elif OTSCHEME['normPT']==True:\n","      assert (OTSCHEME['balanced']==True), \"Error: Invalid OTSCHEME['balanced'] for COSTSCHEME==%s\"%(COSTSCHEME)\n","\n","    # check that we're not doing individual OT\n","    assert (OTSCHEME['individualOT']==False), \"Error: Invalid OTSCHEME['individualOT'] for COSTSCHEME==%s\"%(COSTSCHEME)\n","\n","  elif COSTSCHEME == '3D':\n","\n","    # either normalized or unnormalized pT is fine\n","    # => nothing to check\n","\n","    # either balanced or unbalanced is fine\n","    # => nothing to check\n","\n","    # either removing or keeping zero padding is fine\n","    # => nothing to check\n","\n","    # if we're doing individual species we want balanced OT with unnormalized pT and zero padding intact\n","    if OTSCHEME['individualOT'] == True:\n","      assert (OTSCHEME['balanced']==True),   \"Error: Invalid OTSCHEME['balanced'] for individualOT and COSTSCHEME==%s\"%(COSTSCHEME)\n","      assert (OTSCHEME['normPT']==False),    \"Error: Invalid OTSCHEME['normPT'] for individualOT and COSTSCHEME==%s\"%(COSTSCHEME)\n","      assert (OTSCHEME['noZeroPad']==False), \"Error: Invalid OTSCHEME['noZeroPad'] for individualOT and COSTSCHEME==%s\"%(COSTSCHEME)\n","\n","\n","  # If all assert statements passed without issue then return True\n","  return True\n"]},{"cell_type":"markdown","source":["#### `getMasses()`"],"metadata":{"id":"dKTXfm0O1NHQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"14AxAkkPxMlN"},"outputs":[],"source":["def getMasses(xs, xt, OTSCHEME, COSTSCHEME):\n","  \"\"\"\n","  Auxiliary function to get probability masses based on OTSCHEME and COSTSCHEME.\n","  There are 5 different options.\n","  \"\"\"\n","  #-- Option 1:  m_i = 1/19 --#\n","  # There are 3 combinations that give this\n","  # They assume 1D or 3D, balanced, and zero-padded\n","  if (COSTSCHEME != '2D') and (OTSCHEME['balanced']==True) and (OTSCHEME['noZeroPad']==False) and (OTSCHEME['individualOT']==False):\n","    a = np.ones((19,)) / 19\n","    b = a\n","\n","  #-- Option 2: m_i = 1, 1/4, or 1/10 depending on individual species type --#\n","  elif (COSTSCHEME != '2D') and (OTSCHEME['balanced']==True) and (OTSCHEME['noZeroPad']==False) and (OTSCHEME['individualOT']==True):\n","    nx, ny = xs.shape[0], xt.shape[0]\n","    a = np.ones((nx,)) / nx\n","    b = np.ones((ny,)) / ny\n","\n","  #-- Option 3: m_i = 1/N, N=number of particles in the event --#\n","  # There are 3 combinations that give this\n","  # They assume 1D or 3D, balanced, and zero-padding removed\n","  elif (COSTSCHEME != '2D') and (OTSCHEME['balanced']==True) and (OTSCHEME['noZeroPad']==True):\n","    nx, ny = xs.shape[0], xt.shape[0]\n","    a = np.ones((nx,)) / nx\n","    b = np.ones((ny,)) / ny\n","\n","  #-- Option 4: m_i = pT --#\n","  # There is only 1 combination that gives this\n","  elif (COSTSCHEME == '2D') and (OTSCHEME['normPT']==False):\n","    a = xs[:,0]\n","    b = xt[:,0]\n","\n","  #-- Option 5: m_i = pT/sum(pT) --#\n","  # There is only 1 combination that gives this\n","  elif (COSTSCHEME == '2D') and (OTSCHEME['normPT']==True):\n","    totalpT_xs = np.sum(xs[:,0]) # Total pT in each x-event\n","    totalpT_xt = np.sum(xt[:,0]) # Total pT in each y-event\n","\n","    a = np.ascontiguousarray(xs[:,0]/totalpT_xs) # POT error will result if not C-contiguous\n","    b = np.ascontiguousarray(xt[:,0]/totalpT_xt)\n","\n","    # Assuming balanced OT, both total masses should be the same (up to some precision)\n","    # Using same precision considerations as POT library which is the default for np.testing.assert_almost_equal() (decimal=7)\n","    np.testing.assert_almost_equal(a.sum(0), b.sum(0,keepdims=True), err_msg='a and b vector must have the same sum')\n","\n","  #-- Option 6: m_i = 1 --#\n","  else:\n","    nx, ny = xs.shape[0], xt.shape[0]\n","    a = np.ones((nx,))\n","    b = np.ones((ny,))\n","\n","  return a, b"]},{"cell_type":"markdown","metadata":{"id":"URikDA8PRCr2"},"source":["# Data handling functions"]},{"cell_type":"markdown","source":["##### `roundToSigFig()`"],"metadata":{"id":"eVvhz4Jf_2Ie"}},{"cell_type":"code","source":["#-- Round num to n significant digits --#\n","# Note that half is not always rounded up (better strategy for scientific applications)\n","#   Ex. roundToSigFig(0.090215, 4)  ->   0.09022\n","#       roundToSigFig(0.090225, 4)  ->   0.09022\n","# https://stackoverflow.com/questions/3410976/how-to-round-a-number-to-significant-figures-in-python\n","def roundToSigFig(num, n):\n","  return '{:g}'.format(float('{:.{p}g}'.format(num, p=n)))"],"metadata":{"id":"7nUZmwe5_2ku"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### `loadJSONFile()`"],"metadata":{"id":"9LWIPmfFB6oY"}},{"cell_type":"code","source":["def loadJSONFile(filepath, NREPEAT=5, INVERTED=False):\n","  \"\"\"\n","  Loads JSON file with assumed structure, and returns the dictionary.\n","\n","  Structure of JSON:\n","    'repeat0'\n","       'sig_A'\n","          List of 'auc', 'fpr', 'tpr', 'SI', 'fprInv', 'F1' in that order.\n","       'sig_h0'\n","          ...\n","       'sig_hch'\n","          ...\n","       'sig_LQ'\n","          ...\n","    'repeat1'\n","    ...\n","    repeatN\n","\n","  Returned dictionary structure expands lists into separate keys with the corresponding names.\n","  \"\"\"\n","\n","  #-- Open file and load dictionary --#\n","  f    = open(filepath)\n","  data = json.load(f)\n","\n","  #-- Create new dictionary --#\n","  ROCmetricList = ['auc', 'fpr', 'tpr', 'SI', 'fprInv', 'F1']\n","  newDict = {}\n","  if INVERTED:\n","    REPEATLIST = ['repeat%d'%i for i in range(NREPEAT)] # ['repeat0', ...,]\n","  else:\n","    REPEATLIST = list(data.keys()) # ['repeat0', ...,]\n","\n","  for key in REPEATLIST:\n","    newDict[key] = {}\n","\n","    for subkey in ['sig_A', 'sig_h0', 'sig_hch', 'sig_LQ']:\n","      newSubkeyName = 'ROC_metric_'+subkey\n","      newDict[key][newSubkeyName] = {}\n","      for i in range(len(ROCmetricList)):\n","\n","        if INVERTED:\n","          ROCmetric = np.array(data[subkey][key][i], dtype=np.float64)\n","        else:\n","          ROCmetric = np.array(data[key][subkey][i], dtype=np.float64)\n","\n","        if ROCmetricList[i] == 'fprInv':\n","          # None entries come from division by zero, so for these cases we impose a regulator (inf approx 1/0.0001)\n","          ROCmetric = np.nan_to_num(ROCmetric, nan=1/0.0001)\n","          # maskNones = ROCmetric==None\n","          # ROCmetric[maskNones] = np.repeat(1/0.0001, ROCmetric[maskNones].shape[0])\n","\n","        newDict[key][newSubkeyName][ROCmetricList[i]] = ROCmetric\n","\n","  return newDict"],"metadata":{"id":"o0kjbjlUPwya"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lmx6vvZlOv52"},"source":["##### `randomDataSample()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m4-zA4qQOW_C"},"outputs":[],"source":["def randomDataSample(data, nEvents, random_state):\n","  \"\"\"\n","  Generate a random sample of data by generating a 1D array of nEvents uniform\n","  random integers and returning the events corresponding to these integers.\n","\n","  Inputs:\n","    data:          Total data array of shape (nTotal, 19, 3)\n","    nEvents:       Number of events to sample\n","    random_state:  The generator to use to generate the samples (for reproducibility)\n","\n","  Outputs:\n","    Selected events; shape (nEvents, 19, 3)\n","  \"\"\"\n","  #! Pretty slow in practice, make it faster later\n","\n","  nTotal = data.shape[0]\n","  randomIntArray = random_state.integers(0,nTotal,nEvents)\n","\n","  return data[randomIntArray, :, :]"]},{"cell_type":"markdown","metadata":{"id":"3FBgm3ycrcyP"},"source":["##### `calcROCmetrics()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VabVDs9hrcyP"},"outputs":[],"source":["def calcROCmetrics(scoreBkg, scoreSigList, SIreg=0.0001, INTERPOLATE=True):\n","    \"\"\"\n","    Calculate 4 metrics related to ROC curve components:\n","        - AUC\n","        - Background efficiency (aka FPR or eps_B), Signal efficiency (aka TPR or eps_S)\n","        - Significance Improvement (SI) defined as eps_S/sqrt(eps_B + SIreg) <=> TPR/sqrt(FPR + SIreg)\n","          where SIreg is a regulator for statistical fluctuations at low efficiency; SIreg=0.01% by default\n","          Reference: https://arxiv.org/pdf/2001.05001.pdf\n","        - Inverse Background efficiency (aka FPR^{-1} or eps_B^{-1}); division by zero is masked\n","\n","    Inputs:\n","      scoreBkg:      Anomaly score values for N background events; shape (N,)\n","      scoreSigList:  List of anomaly score values for each signal type case; Length=nCases\n","                    scoreSigList[i] is the anomaly score for M signal events of the ith signal type case; shape (M,)\n","      SIreg:         Regulator to prevent division by zero when calculating SI metric; default 0.01%\n","      INTERPOLATE:   Whether to interpolate to a standard array of TPR values\n","\n","    Outputs:\n","      aucList:       List of AUC scores for each background to signal type pairing; Length=nCases\n","      fprList:       List of FPR arrays for each background to signal type pairing; Length=nCases\n","                    fprList[i] is an array of shape (Q,) with Q>2\n","      tprList:       List of TPR arrays for each background to signal type pairing; Length=nCases\n","                    tprList[i] is an array of shape (Q,) with Q>2\n","      SIList:        List of SI arrays for each background to signal type pairing; Length=nCases\n","                    SIList[i] is an array of shape (Q,) with Q>2\n","      fprInvList:    List of inverse FPR arrays for each background to signal type pairing; Length=nCases\n","                    fprInvList[i] is a masked array of shape (Q,) with Q>2 with division by zero cases masked\n","      F1List:        List of F1 score arrays for each background to signal type pairing; Length=nCases\n","                    F1List[i] is an array of shape (Q,) with Q>2\n","    \"\"\"\n","\n","    #-- Preliminaries --#\n","    nCases    = len(scoreSigList) # Number of signal cases\n","    aucList, fprList, tprList, SIList, fprInvList, F1List = [], [], [], [], [], [] # Create lists to store results\n","\n","    #-- Loop over signal cases --#\n","    for i in range(nCases):\n","      scoreSig = scoreSigList[i]\n","\n","      #-- Calculate AUC (and, internally, label and score inputs for sklearn's function) --#\n","      auc, labels, scores = calcAUC(scoreBkg, scoreSig)\n","      aucList.append(auc)\n","\n","      #-- Calculate other ROC curve metrics --#\n","      fpr_raw, tpr_raw, _ = metrics.roc_curve(labels, scores)\n","\n","      if INTERPOLATE:\n","        #https://stats.stackexchange.com/questions/186337/average-roc-for-repeated-10-fold-cross-validation-with-probability-estimates\n","        base_tpr = np.linspace(0, 1, 101) # 0.00, 0.01, ..., 1.0\n","        tpr = base_tpr\n","        fpr = np.interp(base_tpr, tpr_raw, fpr_raw)\n","      else:\n","        fpr = fpr_raw\n","        tpr = tpr_raw\n","\n","      fprList.append(fpr)\n","      tprList.append(tpr)\n","\n","      #-- Calculate SI metric --#\n","      # Def: eps_S/sqrt(eps_B + SIreg) <=> TPR/sqrt(FPR + SIreg)\n","      fpr_sqrt = np.sqrt(fpr + SIreg)\n","      SI = tpr/fpr_sqrt\n","      SIList.append(SI)\n","\n","      #-- Calculate inverse FPR metric --#\n","      #! Should we also use regulator here? Is this typical?\n","      fpr_masked = ma.masked_where(fpr==0., fpr) # Get rid of possibility of dividing by zero\n","      fprInv = 1./fpr_masked\n","      fprInvList.append(fprInv)\n","\n","      #-- Calculate F1 score metric --#\n","      # Recall that\n","      #   P   is the number of signal and N is the number of background\n","      #   TP  is the number of true positives, FP is the number of false positives, and FN is the number of false negatives\n","      #   TPR = TP/P, FPR = FP/N, and FNR = FN/P\n","      #   FNR = 1 - TPR\n","      #\n","      # Def of F1: (2*TP)/(2*TP + FP + FN)\n","      P   = scoreSig.shape[0] # Number of signal\n","      N   = scoreBkg.shape[0] # Number of background\n","      tp  = P*tpr\n","      fp  = N*fpr\n","      fn  = P*(1-tpr) # P*fnr\n","      F1  = (2*tp)/(2*tp + fp + fn)\n","      F1List.append(F1)\n","\n","    return aucList, fprList, tprList, SIList, fprInvList, F1List"]},{"cell_type":"markdown","metadata":{"id":"OUqniKxLPC0G"},"source":["##### `calcAUC()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BoWismZgOZ5I"},"outputs":[],"source":["def calcAUC(scoreBkg, scoreSig):\n","  \"\"\"\n","  Given the anomaly scores for background and signal events, calculate the AUC.\n","\n","  Inputs:\n","    scoreBkg:   Anomaly score values for N background events; shape (N,)\n","    scoreSig:   Anomaly score values for M signal events; shape (M,)\n","\n","  Outputs:\n","    auc:        Area Under the Curve (AUC) value; float\n","    labels:     Numeric background/signal labels; 0 for background, 1 for signal\n","                (necessary for ROC and AUC calculations); shape (N+M,)\n","    scores:     Concatenated anomaly scores from background and signal events;\n","                (necessary for ROC and AUC calculations); shape (N+M,)\n","\n","  NOTE: Using the sklearn.metrics.roc_auc_score() function\n","  https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n","  \"\"\"\n","\n","  labelsB = np.repeat(0, scoreBkg.shape[0]) # Background labels\n","  labelsS = np.repeat(1, scoreSig.shape[0]) # Signal labels\n","  labels  = np.concatenate((labelsB, labelsS))\n","\n","  scores  =  np.concatenate((scoreBkg, scoreSig))\n","\n","  auc = metrics.roc_auc_score(labels, scores)\n","\n","  return auc, labels, scores"]},{"cell_type":"markdown","metadata":{"id":"stH-CGFTPQcM"},"source":["##### `indxOfCertainTPR()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HZb_tbbOf0t"},"outputs":[],"source":["def indxOfCertainTPR(tprList, TPRval = 0.3):\n","  \"\"\"\n","  For each TPR array in tprList, get the index corresponding to the TPR value\n","  which is closest to TPRval. This can be used to examine the other metrics at\n","  a certain TPR (signal efficiency, \\eps_S) value.\n","\n","  Inputs:\n","    tprList:   List of TPR arrays for each background to signal type pairing; Length=number of signal cases\n","               tprList[i] is an array of shape (Q,) with Q>2\n","    TPRval:    Fixed reference TPR value; default is TPR = \\eps_S = 30%\n","\n","  Outputs:\n","    indxList:  List of indices corresponding to TPR ~= TPRval; length = number of signal cases\n","  \"\"\"\n","  indxList = []\n","  nCases = len(tprList) # Number of signal cases\n","\n","  for i in range(nCases):\n","    tprArr = tprList[i]\n","    difference_array = np.absolute(tprArr-TPRval)\n","    indx = difference_array.argmin()\n","    indxList.append(indx)\n","\n","  return indxList"]},{"cell_type":"markdown","metadata":{"id":"727XoxLorcyQ"},"source":["##### `getRepeatAvStd()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mk53knlprcyQ"},"outputs":[],"source":["def getRepeatAvStd(scoreDict, sigAliasList=['sig_A', 'sig_h0', 'sig_hch', 'sig_LQ'], NREPEAT=5):\n","  \"\"\"\n","  Calculates average and standard deviation over repeats for the various ROC metrics: auc, fpr, SI, fprInv, F1.\n","  For auc, the av and std are each a single number. Whereas, for fpr, SI, fprInv, F1 the av and std will each be arrays of the same respective size.\n","\n","  Inputs:\n","      scoreDict:  Dictionary containing ROC metrics (auc, fpr, SI, fprInv, F1) for all signal cases. There are NREPEAT copies.\n","                  Structure is assumed to be the following:\n","                      scoreDict['repeat0']\n","                                          ['ROC_metric_%s']%sigAliasList[0]\n","                                                          ['auc']    # Shape = (NREPEAT,)\n","                                                          ['fpr']    # Shape = (NREPEAT,n_Thresholds)\n","                                                          ['SI']     # Shape = (NREPEAT,n_Thresholds)\n","                                                          ['fprInv'] # Shape = (NREPEAT,n_Thresholds)\n","                                                          ['F1']     # Shape = (NREPEAT,n_Thresholds)\n","                                          ...\n","                                          ['ROC_metric_%s']%sigAliasList[3]\n","                      scoreDict['repeat1']\n","                      ...\n","                      scoreDict['repeat%s'%NREPEAT]\n","\n","      sigAliasList:  List of signal type alias\n","          NREPEAT:  Number of repeated test sample sets.\n","\n","  NOTE: Assumes that calcROCmetrics() was calculated with INTERPOLATE=True => TPR is fixed to baseTPR.\n","          This means n_Thresholds=101, corresponding to using baseTPR = np.linspace(0,1,101).\n","\n","  Outputs:\n","      N/a, updates scoreDict with values under key 'avStdQuantities' for each signal type. For example,\n","          scoreDict['avStdQuantities']['sig_A']['auc']['mean']\n","                                                      ['std']\n","  \"\"\"\n","\n","  REPEATLIST = ['repeat%d'%i for i in range(NREPEAT)]\n","  scoreDict['avStdQuantities'] = {}\n","\n","  # Loop over signal types\n","  for alias in sigAliasList:\n","    print(\"Analyzing signal type = %s \"%alias)\n","    name = 'ROC_metric_%s'%(alias)\n","    scoreDict['avStdQuantities'][alias]                   = {}\n","\n","    # Get average and std of desired quantities\n","    for quantity in ['auc', 'fpr', 'SI', 'fprInv', 'F1']:\n","\n","      qArr = np.array([scoreDict[key][name][quantity] for key in REPEATLIST ])\n","      scoreDict['avStdQuantities'][alias][quantity]         = {}\n","      scoreDict['avStdQuantities'][alias][quantity]['mean'] = qArr.mean(axis=0)\n","      scoreDict['avStdQuantities'][alias][quantity]['std']  = qArr.std(axis=0)"]},{"cell_type":"markdown","metadata":{"id":"w7-3CDlKPgTV"},"source":["##### `calcWeightedComboOTscores()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qIV2ogm1OkwR"},"outputs":[],"source":["def calcWeightedComboOTscores(scoreDict, wList=[1., 1., 1., 1.]):\n","  \"\"\"\n","  calculate a combination of individual species OT scores for each subkey case\n","\n","  Inputs:\n","    scoreDict:  Dictionary of scores for OT on each particle species; e.g. scoreDict['MET']['wBB']\n","    wList:      How much to weight 'MET', 'e', 'mu', 'jet' information, respectively, in the sum;\n","                default equal weighting wList=[1., 1., 1., 1.]\n","  Outputs:\n","    Updated scoreDict\n","  \"\"\"\n","  #-- Calculate and store combination (sum) --#\n","  scoreDict['combo'] = {}\n","  for subkey in scoreDict['MET'].keys():\n","    nameCombo = 'combo_'+str(subkey)\n","    scoreDict['combo'][nameCombo]  = wList[0]*scoreDict['MET'][subkey] + wList[1]*scoreDict['e'][subkey] + wList[2]*scoreDict['mu'][subkey] + wList[3]*scoreDict['jet'][subkey]\n","\n","  return scoreDict"]},{"cell_type":"markdown","metadata":{"id":"GDRscCjwPkb7"},"source":["##### `getFractionsOfMax()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GX_VJScoPqg3"},"outputs":[],"source":["def getFractionsOfMax(indxs, val):\n","  \"\"\"\n","  Calculates the fraction of indxs entries that equal val\n","  \"\"\"\n","\n","  indxs_masked = ma.masked_where(indxs==val, indxs)\n","  total        = indxs.shape[0]\n","  fraction     = float(np.sum(indxs_masked.mask))/ float(total)\n","\n","  return fraction"]},{"cell_type":"markdown","metadata":{"id":"wNSBamdZPswD"},"source":["##### `maxIndividualOTScore()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dubD4k-tOnEP"},"outputs":[],"source":["def maxIndividualOTScore(scoreDict, alias):\n","  \"\"\"\n","  Calculates which individual OT score is the largest for each event in signal type alias\n","  \"\"\"\n","\n","  # Get individual OT scores from dictionary\n","  metArr = scoreDict['MET'][alias].reshape(-1, 1)\n","  eArr   = scoreDict['e'][alias].reshape(-1, 1)\n","  muArr  = scoreDict['mu'][alias].reshape(-1, 1)\n","  jetArr = scoreDict['jet'][alias].reshape(-1, 1)\n","\n","  comboArr = np.concatenate((metArr, eArr, muArr, jetArr), axis=1)\n","\n","  # Get index of max individual OT score for each event\n","  # 0=met, 1=e, 2=mu, 3=jet\n","  indxs  = np.argmax(comboArr, axis=1)\n","  maxArr = np.max(comboArr, axis=1)\n","\n","  # Print fractions\n","  print(\"Signal Type = \", alias)\n","  print(\"Fraction of times that each individual OT score was the maximum for a given event:\")\n","  print(\"   MET: %s \"%str(getFractionsOfMax(indxs, 0)*100))\n","  print(\"     e: %s \"%str(getFractionsOfMax(indxs, 1)*100))\n","  print(\"    mu: %s \"%str(getFractionsOfMax(indxs, 2)*100))\n","  print(\"   jet: %s \"%str(getFractionsOfMax(indxs, 3)*100))\n","\n","  return indxs, maxArr"]},{"cell_type":"markdown","metadata":{"id":"rH88oRz6PzHo"},"source":["##### `calcMultiplicityData()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_DYktU1Oncp"},"outputs":[],"source":["def calcMultiplicityData(objectsBkg, objectsSigList):\n","  \"\"\"\n","  Inputs:\n","    objectsBkg:       pT of all objects for each background event; ndarray of shape\n","                      (Nb, 19) where Nb is the number of background events\n","    objectsSigList:   List of arrays of pT of all objects for each signal event;\n","                      element of list is ndarray of shape (Ns, 19), where Ns, the\n","                      number of signal events, varies depending on the signal type;\n","                      signal types in list is 'sig_A', 'sig_h0', 'sig_hch', 'sig_LQ'\n","  Outputs:\n","    multBkgList:      List of arrays corresponding to electron, muon, jet, and total multiplicities\n","    multSigList:      List of list of arrays corresponding to electron, muon, jet, and total multiplicities for each signal type\n","                      E.g. multSigList[0] is the list of electron multiplicities for all signal types\n","                      Total multiplicity is defined as multiplicity of all particle-type objects (i.e. excluding MET which is always present)\n","  \"\"\"\n","\n","  #-- Get multiplicities for background data --#\n","  multElectrons_Bkg = np.count_nonzero(objectsBkg[:, 1:5], axis=1)\n","  multMuons_Bkg     = np.count_nonzero(objectsBkg[:, 5:9], axis=1)\n","  multJets_Bkg      = np.count_nonzero(objectsBkg[:, 9:19], axis=1)\n","  multTotal_Bkg     = np.count_nonzero(objectsBkg[:, 1:19], axis=1) # Exclude MET since it is always present\n","\n","  multBkgList = [multElectrons_Bkg, multMuons_Bkg, multJets_Bkg, multTotal_Bkg]\n","\n","  #-- Get multiplicities for signal data --#\n","  listMultElectrons_Sig, listMultMuons_Sig, listMultJets_Sig, listMultTotal_Sig = [],[],[],[]\n","  nSignalCategories = len(objectsSigList)\n","  for i in range(nSignalCategories):\n","    objectsSig = objectsSigList[i]\n","\n","    listMultElectrons_Sig.append(np.count_nonzero(objectsSig[:, 1:5], axis=1))\n","    listMultMuons_Sig.append(np.count_nonzero(objectsSig[:, 5:9], axis=1))\n","    listMultJets_Sig.append(np.count_nonzero(objectsSig[:, 9:19], axis=1))\n","    listMultTotal_Sig.append(np.count_nonzero(objectsSig[:, 1:19], axis=1))\n","\n","  multSigList = [listMultElectrons_Sig, listMultMuons_Sig, listMultJets_Sig, listMultTotal_Sig]\n","\n","  return multBkgList, multSigList"]},{"cell_type":"markdown","source":["##### `calcMaxAvSI()`"],"metadata":{"id":"DNhOtzwh4Qe5"}},{"cell_type":"code","source":["def calcMaxAvSI(scoreDict, sigAliasList=['sig_A', 'sig_h0', 'sig_hch', 'sig_LQ'], minTPR=0.05, NSIGFIGS=4, SIMPLE=False):\n","\n","  print(\"Max SI (TPR):\")\n","  # Loop over signal types\n","  for alias in sigAliasList:\n","\n","    #-- Get SI curve components --#\n","    av_si  = scoreDict['avStdQuantities'][alias]['SI']['mean']\n","    std_si = scoreDict['avStdQuantities'][alias]['SI']['std']\n","\n","    #-- Get max av SI, std, and corresponding TPR in range [minTPR, 1] --#\n","    av_maxSI, std_maxSI, corr_tpr = calcMaxAvSI_helper(av_si, std_si, minTPR=minTPR)\n","\n","    # Report max in trimmed TPR range\n","    if SIMPLE:\n","      print(\"    %s $\\pm$ %s (%s)\"%(roundToSigFig(av_maxSI, NSIGFIGS), roundToSigFig(std_maxSI, NSIGFIGS), roundToSigFig(corr_tpr, NSIGFIGS)))\n","    else:\n","      print(\"    %s is %s $\\pm$ %s (TPR = %s)\"%(alias, roundToSigFig(av_maxSI, NSIGFIGS), roundToSigFig(std_maxSI, NSIGFIGS), roundToSigFig(corr_tpr, NSIGFIGS)))\n"],"metadata":{"id":"r0F7RHsz4OdW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### `calcMaxAvSI_helper()`"],"metadata":{"id":"HrCtOisVMBfw"}},{"cell_type":"code","source":["def calcMaxAvSI_helper(av_si, std_si, minTPR=0.05):\n","\n","  #-- Set base TPR --#\n","  base_tpr = np.linspace(0, 1, 101)\n","\n","  #-- Trim SI curve components to account for minTPR --#\n","  assert (minTPR >=0 and minTPR <=1)  # minTPR must be in range [0,1]\n","  minMask = minTPR <= base_tpr        # Sets TPR values less than minTPR to False\n","\n","  av_si_trimmed   = av_si[minMask]\n","  std_si_trimmed  = std_si[minMask]\n","  tpr_trimmed     = base_tpr[minMask]\n","\n","  #-- Get max in trimmed TPR range --#\n","  aMaxSI = np.argmax(av_si_trimmed)\n","\n","  return av_si_trimmed[aMaxSI], std_si_trimmed[aMaxSI], tpr_trimmed[aMaxSI]"],"metadata":{"id":"koy0FrOFR9wH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### `calcMaxAvF1()`"],"metadata":{"id":"9mRmS9DpO4ZV"}},{"cell_type":"code","source":["def calcMaxAvF1(scoreDict, sigAliasList=['sig_A', 'sig_h0', 'sig_hch', 'sig_LQ'], minTPR=0.05, NSIGFIGS=4, SIMPLE=False):\n","\n","  print(\"Max F1 (TPR):\")\n","  # Loop over signal types\n","  for alias in sigAliasList:\n","\n","    #-- Get F1 curve components --#\n","    av_f1  = scoreDict['avStdQuantities'][alias]['F1']['mean']\n","    std_f1 = scoreDict['avStdQuantities'][alias]['F1']['std']\n","\n","    #-- Get max av F1, std, and corresponding TPR in range [minTPR, 1] --#\n","    av_maxF1, std_maxF1, corr_tpr = calcMaxAvSI_helper(av_f1, std_f1, minTPR=minTPR)\n","\n","    # Report max in trimmed TPR range\n","    if SIMPLE:\n","      print(\"    %s $\\pm$ %s (%s)\"%(roundToSigFig(av_maxF1, NSIGFIGS), roundToSigFig(std_maxF1, NSIGFIGS), roundToSigFig(corr_tpr, NSIGFIGS)))\n","    else:\n","      print(\"    %s is %s $\\pm$ %s (TPR = %s)\"%(alias, roundToSigFig(av_maxF1, NSIGFIGS), roundToSigFig(std_maxF1, NSIGFIGS), roundToSigFig(corr_tpr, NSIGFIGS)))\n"],"metadata":{"id":"O_c_h5EYO4Ze"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### `calcMaxAvF1_helper()`"],"metadata":{"id":"iVNytCrhO74d"}},{"cell_type":"code","source":["def calcMaxAvF1_helper(av_f1, std_f1, minTPR=0.05):\n","\n","  #-- Set base TPR --#\n","  base_tpr = np.linspace(0, 1, 101)\n","\n","  #-- Trim SI curve components to account for minTPR --#\n","  assert (minTPR >=0 and minTPR <=1)  # minTPR must be in range [0,1]\n","  minMask = minTPR <= base_tpr        # Sets TPR values less than minTPR to False\n","\n","  av_f1_trimmed   = av_f1[minMask]\n","  std_f1_trimmed  = std_f1[minMask]\n","  tpr_trimmed     = base_tpr[minMask]\n","\n","  #-- Get max in trimmed TPR range --#\n","  aMaxF1 = np.argmax(av_f1_trimmed)\n","\n","  return av_f1_trimmed[aMaxF1], std_f1_trimmed[aMaxF1], tpr_trimmed[aMaxF1]"],"metadata":{"id":"LdNYUda9O74m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Background data augmentation functions"],"metadata":{"id":"29d8cmzIvH_k"}},{"cell_type":"markdown","source":["Modified from the [original code](https://github.com/bmdillon/AnomalyCLR/blob/main/EventLevelAnomalyAugmentations.py).\n","Implemented bug fixes (noted with `#!` comments) and put in comment descriptions of what is happening."],"metadata":{"id":"vZgr_7kHMFRU"}},{"cell_type":"markdown","source":["### Validation functions to ensure selection requirements are satisfied"],"metadata":{"id":"F6cbh6WbXbTw"}},{"cell_type":"markdown","source":["##### `check_data()`"],"metadata":{"id":"eoTxdzmFzFx8"}},{"cell_type":"code","source":["def check_data(data):\n","  \"\"\"\n","  data: shape (nEvents, 19, 4)\n","  \"\"\"\n","\n","  #-- Check phi --#\n","  # All phi angles should be in the range [-pi, pi)\n","  phiCheck = np.all(data[:,:,2] < np.pi) and np.all(data[:,:,2] >= -np.pi)\n","  print(\"             phiCheck pass? \",phiCheck)\n","\n","  #-- Check eta --#\n","  # Electrons must have |eta|<3.\n","  # Muons must have     |eta|<2.1\n","  # Jets must have      |eta|<4.\n","  eta_eleCheck = np.all(np.abs(data[:,1:5,1]) < 3.)\n","  eta_muCheck = np.all(np.abs(data[:,5:9,1]) < 2.1)\n","  eta_jetCheck = np.all(np.abs(data[:,9:,1]) < 4.)\n","  print(\"         eta_eleCheck pass? \",eta_eleCheck)\n","  print(\"          eta_muCheck pass? \",eta_muCheck)\n","  print(\"         eta_jetCheck pass? \",eta_jetCheck)\n","\n","  #-- Lepton pT check--#\n","  # All leptons must have pT >= 3.\n","  # The leading lepton must have pT >= 23.\n","\n","  mask = data[:,1:9,0] != 0. # Get rid of zero-padded objects\n","  pT_eleMuCheck         = np.all(data[:,1:9,0][mask] > 3. )\n","  pT_leadingLeptonCheck = np.all((data[:,1:9,0][:,0] > 23.) | (data[:,1:9,0][:,4] > 23.))\n","  print(\"        pT_eleMuCheck pass? \",pT_eleMuCheck)\n","  print(\"pT_leadingLeptonCheck pass? \",pT_leadingLeptonCheck)\n","\n","  #-- Jet pT check--#\n","  # All jets must have pT >= 15.\n","\n","  mask = data[:,9:,0] != 0. # Get rid of zero-padded objects\n","  pT_jetCheck = np.all(data[:,9:,0][mask] > 3. )\n","  print(\"          pT_jetCheck pass? \",pT_jetCheck)\n","\n","  return [phiCheck, eta_eleCheck, eta_muCheck, eta_jetCheck, pT_eleMuCheck, pT_leadingLeptonCheck, pT_jetCheck]"],"metadata":{"id":"nTQ31Lea_9hy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Reformatting data to pass to code and back again"],"metadata":{"id":"6Ez2m-Ms1WSe"}},{"cell_type":"markdown","source":["The code assumes a different structure for the events.  \n","\n","By default the events are in an `[nEvents, 19, 4]` array, where the last 4 columns are pT, eta, phi, identity. This means `[:,0,:]` is the information of MET from each event. Similarly `[:,1:5,:]` are electrons, `[:,5:9,:]` are muons, `[:,9:,:]` are jets.\n","\n","In the anomaly augmentation code, the last two axes are swapped and a one-hot encoding is added to indicate whether a particle of a certain type is there. So the shape is `[nEvents, 7, 19]` where `[nEvents, 0:4, 19]` are pT, eta, phi, identity and\n","```\n","[nEvents, 4, i] is non-zero if the ith particle is an electron with a non-zero entry\n","[nEvents, 5, i] is non-zero if the ith particle is a muon with a non-zero entry\n","[nEvents, 6, i] is non-zero if the ith particle is a jet with a non-zero entry\n","```\n","\n","\n"],"metadata":{"id":"4R113WIf1faY"}},{"cell_type":"markdown","source":["#### Pass to the code"],"metadata":{"id":"1880fRqEBDt0"}},{"cell_type":"markdown","source":["Above we reformatted a single event, but now we need to reformat a block of events. We'll start with 2 for easy visualization, but the code itself should generalize"],"metadata":{"id":"5cgdTtiX1mEs"}},{"cell_type":"markdown","source":["##### `add_one_hot_columns()`"],"metadata":{"id":"FEBQ8PcPzRYc"}},{"cell_type":"code","source":["def add_one_hot_columns(batch):\n","  \"\"\"\n","  batch:  shape=(nEvents, 19, 4),\n","          The last 4 columns are: pT, eta, phi, ID.\n","          Where ID = 1. for MET, 2. for electron, 3. for muon, and 4. for jet\n","\n","  returns:  shape=(nEvents, 19, 7)\n","  \"\"\"\n","  oneHotCols = np.zeros(shape=(batch.shape[0],batch.shape[1],3))\n","  dummyOnes  = np.ones(shape=(batch.shape[0],batch.shape[1],3))\n","\n","  # Select locations of existing electrons, muons, jets\n","  mask_els  = (batch[:, :, 3] == 2.)\n","  mask_mus  = (batch[:, :, 3] == 3.)\n","  mask_jets = (batch[:, :, 3] == 4.)\n","\n","  # Switch the corresponding one-hot column entry from 0 to 1\n","  oneHotCols[mask_els, 0]  = dummyOnes[mask_els, 0]\n","  oneHotCols[mask_mus, 1]  = dummyOnes[mask_mus, 1]\n","  oneHotCols[mask_jets, 2] = dummyOnes[mask_jets, 2]\n","\n","  # Concatenate the one-hot columns with the original data\n","  return np.concatenate((batch, oneHotCols), axis=-1)\n","\n","def format_for_anomalyAugmentations(bkgData):\n","  \"\"\"\n","  bkgData:  shape=(nEvents, 19, 4)\n","\n","  returns:  shape=(nEvents, 7, 19)\n","  \"\"\"\n","  batch = bkgData.copy() # so that original isn't modified\n","\n","  # Add one hot columns for each particle type (i.e. electrons, muons, jets)\n","  new_batch = add_one_hot_columns(batch)\n","\n","  # Swap last two axes of the data\n","  newBkgData = np.swapaxes(new_batch, 1, 2)\n","\n","  return newBkgData"],"metadata":{"id":"wq1tlej43FN6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Test that the above code behaves as expected"],"metadata":{"id":"KB-Q6dGIA-FT"}},{"cell_type":"code","source":["# # TEST\n","# bkg = dataDict['bkg']['Particles'][11:13,:,:].reshape(-1,19,4)\n","# print(\"Original event\")\n","# print(bkg)\n","# print(\"(nEvents, 19, 4) = \",bkg.shape)\n","# print(\"\")\n","\n","# newBkgData = format_for_anomalyAugmentations(bkg)\n","# print(\"Reformatted event\")\n","# print(\"(nEvents, 7, 19) = \", newBkgData.shape)\n","# print(\"\")\n","\n","# print(\"There is no one-hot encoding for MET\")\n","# print(\"pT, eta, phi, ID = \")\n","# print(newBkgData[:, 0:4, 0])\n","# print(\"one-hot = \")\n","# print(newBkgData[:, 4:, 0])\n","# print(\"\")\n","\n","# print(\"The one-hot encoding for existing electrons is the first row\")\n","# print(\"pT, eta, phi, ID = \")\n","# print(newBkgData[:, 0:4, 1:5])\n","# print(\"one-hot = \")\n","# print(newBkgData[:, 4:, 1:5])\n","# print(\"\")\n","\n","# print(\"The one-hot encoding for existing muons is the second row\")\n","# print(\"pT, eta, phi, ID = \")\n","# print(newBkgData[:, 0:4, 5:9])\n","# print(\"one-hot = \")\n","# print(newBkgData[:, 4:, 5:9])\n","# print(\"\")\n","\n","# print(\"The one-hot encoding for existing jets is the third row\")\n","# print(\"pT, eta, phi, ID = \")\n","# print(newBkgData[:, 0:4, 9:])\n","# print(\"one-hot = \")\n","# print(newBkgData[:, 4:, 9:])\n","# print(\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B0yCzlV0-VKM","executionInfo":{"status":"ok","timestamp":1701122867315,"user_tz":480,"elapsed":1398,"user":{"displayName":"Jessica N. Howard","userId":"15045818586830339905"}},"outputId":"4af08dd2-72ac-45d4-ff1c-08afbe36288b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original event\n","[[[15.50150108  0.          2.45500445  1.        ]\n","  [34.09182739  2.4137466   1.00465226  2.        ]\n","  [20.25606728  2.41362333  1.00502455  2.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [30.45305634  1.62085462 -2.00475979  4.        ]\n","  [18.47612     1.86474538 -2.63420057  4.        ]\n","  [18.43354225 -1.26597369 -2.27271795  4.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]]\n","\n"," [[48.11958313  0.         -1.62553179  1.        ]\n","  [10.63295937  0.67247498 -1.44579279  2.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [36.16453171  0.57986325 -1.54481351  3.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [56.26683807 -3.89509201  2.10061932  4.        ]\n","  [54.93286133 -2.30908561  0.65563697  4.        ]\n","  [29.11453629  0.20144162 -2.03941965  4.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]]]\n","(nEvents, 19, 4) =  (2, 19, 4)\n","\n","Reformatted event\n","(nEvents, 7, 19) =  (2, 7, 19)\n","\n","There is no one-hot encoding for MET\n","pT, eta, phi, ID = \n","[[15.50150108  0.          2.45500445  1.        ]\n"," [48.11958313  0.         -1.62553179  1.        ]]\n","one-hot = \n","[[0. 0. 0.]\n"," [0. 0. 0.]]\n","\n","The one-hot encoding for existing electrons is the first row\n","pT, eta, phi, ID = \n","[[[34.09182739 20.25606728  0.          0.        ]\n","  [ 2.4137466   2.41362333  0.          0.        ]\n","  [ 1.00465226  1.00502455  0.          0.        ]\n","  [ 2.          2.          0.          0.        ]]\n","\n"," [[10.63295937  0.          0.          0.        ]\n","  [ 0.67247498  0.          0.          0.        ]\n","  [-1.44579279  0.          0.          0.        ]\n","  [ 2.          0.          0.          0.        ]]]\n","one-hot = \n","[[[1. 1. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]]\n","\n"," [[1. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]]]\n","\n","The one-hot encoding for existing muons is the second row\n","pT, eta, phi, ID = \n","[[[ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]]\n","\n"," [[36.16453171  0.          0.          0.        ]\n","  [ 0.57986325  0.          0.          0.        ]\n","  [-1.54481351  0.          0.          0.        ]\n","  [ 3.          0.          0.          0.        ]]]\n","one-hot = \n","[[[0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 0.]\n","  [1. 0. 0. 0.]\n","  [0. 0. 0. 0.]]]\n","\n","The one-hot encoding for existing jets is the third row\n","pT, eta, phi, ID = \n","[[[30.45305634 18.47612    18.43354225  0.          0.\n","    0.          0.          0.          0.          0.        ]\n","  [ 1.62085462  1.86474538 -1.26597369  0.          0.\n","    0.          0.          0.          0.          0.        ]\n","  [-2.00475979 -2.63420057 -2.27271795  0.          0.\n","    0.          0.          0.          0.          0.        ]\n","  [ 4.          4.          4.          0.          0.\n","    0.          0.          0.          0.          0.        ]]\n","\n"," [[56.26683807 54.93286133 29.11453629  0.          0.\n","    0.          0.          0.          0.          0.        ]\n","  [-3.89509201 -2.30908561  0.20144162  0.          0.\n","    0.          0.          0.          0.          0.        ]\n","  [ 2.10061932  0.65563697 -2.03941965  0.          0.\n","    0.          0.          0.          0.          0.        ]\n","  [ 4.          4.          4.          0.          0.\n","    0.          0.          0.          0.          0.        ]]]\n","one-hot = \n","[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]]]\n","\n"]}]},{"cell_type":"markdown","source":["#### Get from the code"],"metadata":{"id":"oRnGpQuLBOmo"}},{"cell_type":"markdown","source":["We need to undo the formatting done previously, but also account for the fact that the augmentation code causes particles within categories to be ordered a bit weirdly (i.e. some entries and lots of zeros in between)."],"metadata":{"id":"-rUdqwLDBXNn"}},{"cell_type":"code","source":["# augBkg = neg_augs( newBkgData, scaler_pt=1.0, scale_angle=False, etaphi_smear_strength=1.0, addobj=True, addobj_wcpm=True, shpt=True, shmet=True, shporm=True, seed=0)\n","\n","# print(augBkg.shape)\n","# print(augBkg[:,0:4,1:5])\n","# print(augBkg[:,0:4,1:5].shape)\n","# print(augBkg[:,0:4,5:9].shape)\n","# print(augBkg[:,0:4,9:].shape)"],"metadata":{"id":"TwFFNDk6gBKY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We essentially want to\n","\n","1.   Make sure all objects are sorted (within their type) in ascending pT order\n","2.   Remove one-hot encoding\n","3.   Reshape to `(nEvents, 4, 19)`\n","\n"],"metadata":{"id":"K9f3pGNXihBm"}},{"cell_type":"markdown","source":["##### `format_for_analysis()`"],"metadata":{"id":"lfSd929HzZsg"}},{"cell_type":"code","source":["def format_for_analysis(augBkgData):\n","  \"\"\"\n","  augBkgData:  shape = (nEvents, 7, 19)\n","  bkgData:     shape = (nEvents, 19, 4)\n","  \"\"\"\n","  # Copy so original isn't modified\n","  augBkg = augBkgData.copy()\n","\n","  # Make sure each category (electrons, muons, jets) are sorted high pT to low pT\n","  indEle = np.flip(np.argsort(augBkg[:,0,1:5], axis=1), axis=1)\n","  indMu  = np.flip(np.argsort(augBkg[:,0,5:9], axis=1), axis=1)\n","  indJet = np.flip(np.argsort(augBkg[:,0,9:], axis=1), axis=1)\n","\n","  augBkg[:,:,1:5] = np.take_along_axis(augBkg[:,:,1:5], indEle[:, np.newaxis, :], axis=-1)\n","  augBkg[:,:,5:9] = np.take_along_axis(augBkg[:,:,5:9], indMu[:, np.newaxis, :], axis=-1)\n","  augBkg[:,:,9:]  = np.take_along_axis(augBkg[:,:,9:],  indJet[:, np.newaxis, :], axis=-1)\n","\n","  # Return augBkg with last two axes swapped and one-hot encodings removed\n","  return np.swapaxes(augBkg[:,0:4,:], 1, 2)"],"metadata":{"id":"jcOMZ2GNBVH0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# newBkgData = format_for_analysis(augBkg)\n","\n","# print(augBkg.shape)\n","# print(newBkgData.shape)\n","# print(\"\")\n","# print(\"Electrons\")\n","# print(augBkg[:,0:4,1:5])\n","# print(newBkgData[:,1:5,:])\n","# print(\"\")\n","# print(\"Muons\")\n","# print(augBkg[:,0:4,5:9])\n","# print(newBkgData[:,5:9,:])\n","# print(\"\")\n","# print(\"Jets\")\n","# print(augBkg[:,0:4,9:])\n","# print(newBkgData[:,9:,:])"],"metadata":{"id":"2Gmg34JwxP5W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Augmentation functions"],"metadata":{"id":"u-gA_xRxzAkb"}},{"cell_type":"markdown","source":["##### `augmentAndSaveData()`"],"metadata":{"id":"CTmkRWKDiU3U"}},{"cell_type":"code","source":["def augmentAndSaveData(dataDict, saveFilePath='Data/anomalyAugmented_background_for_training.h5', batchSize=5000, nEvents=None, DEBUG=False):\n","\n","  #-- Preliminaries --#\n","  if DEBUG:\n","    nEvents   = batchSize\n","    SEED      = 0\n","  elif nEvents is not None:\n","    SEED      = None\n","  else:\n","    SEED      = None\n","    nEvents   = dataDict['bkg']['Particles'].shape[0]\n","  print(\"Processing %d events\"%nEvents)\n","\n","  if (nEvents % batchSize) == 0:\n","    nBatches = (nEvents // batchSize)\n","    LEFTOVERBATCH = False\n","    print(\"%d batches with size %d with no events remaining\"%(nBatches, batchSize))\n","  else:\n","    nBatches = (nEvents // batchSize) + 1\n","    LEFTOVERBATCH = True\n","    print(\"%d batches with size %d with one more batch of size %d\"%(nBatches-1, batchSize, nEvents % batchSize))\n","\n","  #-- Create/open file to store data --#\n","  print(\"Saving data to \", saveFilePath)\n","  f = h5py.File(saveFilePath, 'a')\n","  f.create_dataset('augBkg', (nEvents,19,4))\n","\n","  #-- Loop over events --#\n","  #arr = np.arange(nEvents*19*4).reshape(nEvents,19,4)\n","  end=0\n","  for i in tqdm(range(nBatches)):\n","\n","    # Set batch range\n","    start = end\n","    if i==nBatches-1 and LEFTOVERBATCH:\n","      end   = start+(nEvents % batchSize)\n","    else:\n","      end  = start+batchSize\n","\n","    #-- Augment data --#\n","    # Transform into expected batch structure\n","    # Augment\n","    # Transform back into regular structure\n","    bkgData = dataDict['bkg']['Particles'][start:end,:,:]\n","    newBkgData = format_for_anomalyAugmentations(bkgData)\n","    augBkgData = neg_augs( newBkgData, scaler_pt=1.0, scale_angle=False, etaphi_smear_strength=1.0, addobj=True, addobj_wcpm=True, shpt=True, shmet=True, shporm=True, seed=SEED)\n","    newAugBkgData = format_for_analysis(augBkgData)\n","\n","    #-- Save to file--#\n","    f['augBkg'][start:end,:,:] = newAugBkgData\n","\n","  # Close file\n","  f.close()\n","\n","  if DEBUG:\n","    return newAugBkgData"],"metadata":{"id":"S6WzImwNZaqf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### `neg_augs()`"],"metadata":{"id":"-L74l0-zElxr"}},{"cell_type":"code","source":["def neg_augs( batch, scaler_pt, scale_angle, etaphi_smear_strength, addobj=True, addobj_wcpm=True, shpt=True, shmet=True, shporm=False, seed=None, DEBUG=False):\n","\n","  # Set seed for reproducibility\n","  if seed != None:\n","    np.random.seed(seed)\n","\n","  # Copy so that original isn't modified\n","  batch_aug = batch.copy()\n","\n","  # Minimum pT requirements\n","  minLeptonBase, minLeadingpT, minJetBase  = 3., 23., 15. # GeV\n","\n","  # Split batch_aug into (2) is okay and (2) is not okay groups\n","  #   For some events, augmentation (2) will result in the transformed event\n","  #   being the same as the original (which is not what we want)\n","  #   To avoid this, we want to make sure that (2) is only applied to events\n","  #   which will transform under (2)\n","  aug2isOkay = aug_2_conditions(batch_aug, minJetBase=minJetBase, minLeptonBase=minLeptonBase, minLeadingpT=minLeadingpT) # mask; shape=(nEvents,)\n","\n","\n","  #-- Transform (2) is NOT okay batch --#\n","\n","  batch_aug2isNotOkay = batch_aug[~aug2isOkay,:,:]\n","\n","  # Check that either (1) or (3) is specified (i.e. if only (2) is specified don't transform)\n","  if addobj or shporm:\n","    # Decide which and how many augmentations to do\n","    #   (1)      addobj=True <-> \"ao\"\n","    #   (3)      shporm=True <-> \"spm\"\n","    n_augs = 0\n","    aug_list = []\n","    if addobj: n_augs+=1; aug_list.append(\"ao\")\n","    if shporm: n_augs+=1; aug_list.append(\"spm\")\n","\n","    # Randomly generate an integer (0,...,n_augs-1)for each event\n","    #    Each event only recieves one augmentation type\n","    rands = np.random.randint( low=0, high=n_augs, size=batch_aug2isNotOkay.shape[0] )\n","\n","    # Select events for each augmentation type and augment those events with the corresponding function\n","    rand_opts = range( n_augs )\n","    for j in range( n_augs ):\n","        aug = aug_list[j]\n","        n = rand_opts[j]\n","        if aug==\"ao\":       batch_aug2isNotOkay[ np.where(rands==n) ] = add_objects( batch_aug2isNotOkay[ np.where(rands==n) ], minJetBase=minJetBase, minLeptonBase=minLeptonBase, DEBUG=DEBUG )\n","        if aug==\"spm\":      batch_aug2isNotOkay[ np.where(rands==n) ] = shift_met_or_pt( batch_aug2isNotOkay[ np.where(rands==n) ], DEBUG=DEBUG )\n","\n","\n","  #-- Transform (2) is okay batch --#\n","\n","  batch_aug2isOkay    = batch_aug[aug2isOkay,:,:]\n","\n","  # Decide which and how many augmentations to do\n","  #   (1)      addobj=True <-> \"ao\"\n","  #   (2) addobj_wcpm=True <-> \"aowcpm\"\n","  #   (3)      shporm=True <-> \"spm\"\n","  n_augs = 0\n","  aug_list = []\n","  if addobj: n_augs+=1; aug_list.append(\"ao\")\n","  if addobj_wcpm: n_augs+=1; aug_list.append(\"aowcpm\")\n","  if shporm: n_augs+=1; aug_list.append(\"spm\")\n","\n","  # Randomly generate an integer (0,...,n_augs-1)for each event\n","  #    Each event only recieves one augmentation type\n","  rands = np.random.randint( low=0, high=n_augs, size=batch_aug2isOkay.shape[0] )\n","\n","  # Select events for each augmentation type and augment those events with the corresponding function\n","  rand_opts = range( n_augs )\n","  for j in range( n_augs ):\n","      aug = aug_list[j]\n","      n = rand_opts[j]\n","      if aug==\"ao\":       batch_aug2isOkay[ np.where(rands==n) ] = add_objects( batch_aug2isOkay[ np.where(rands==n) ], minJetBase=minJetBase, minLeptonBase=minLeptonBase, DEBUG=DEBUG )\n","      if aug==\"aowcpm\":   batch_aug2isOkay[ np.where(rands==n) ] = add_objects_constptmet( batch_aug2isOkay[ np.where(rands==n) ], scaler_pt, scale_angle, etaphi_smear_strength,\n","                                                                                          minJetBase=minJetBase, minLeptonBase=minLeptonBase, minLeadingpT=minLeadingpT, DEBUG=DEBUG )\n","      if aug==\"spm\":      batch_aug2isOkay[ np.where(rands==n) ] = shift_met_or_pt( batch_aug2isOkay[ np.where(rands==n) ], DEBUG=DEBUG )\n","\n","  #-- Reform events into the original shape --# #! Done automatically? Need to copy?\n","  batch_aug[~aug2isOkay,:,:] = batch_aug2isNotOkay\n","  batch_aug[aug2isOkay,:,:]  = batch_aug2isOkay\n","\n","  return batch_aug"],"metadata":{"id":"H9CNPUsUvq75"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### augmentation (1): `add_objects()`"],"metadata":{"id":"bbLoBjP0vq75"}},{"cell_type":"code","source":["# (1)\n","def add_objects(batch, minJetBase=15., minLeptonBase=3., DEBUG=False):\n","  # Make repeatable\n","  if DEBUG: np.random.seed(0)\n","\n","  # Copy so that original isn't modified\n","  batch_filled = batch.copy()\n","  if DEBUG:\n","    print(\"Original batch_filled\")\n","    print(batch_filled)\n","    print(\"\")\n","\n","  # ! These aren't used... hardcoded instead below\n","  n_els = 4\n","  n_mus = 4\n","  n_jets = 10\n","\n","  # Calculate number of nonzero electrons, muons, and jets for each event\n","  # Shape = (nEvents, 1)\n","  n_nonzero_els  = np.count_nonzero(batch_filled[:, 4, 1:5], axis=1)\n","  n_nonzero_mus  = np.count_nonzero(batch_filled[:, 5, 5:9], axis=1)\n","  n_nonzero_jets = np.count_nonzero(batch_filled[:, 6, 9:], axis=1)\n","\n","  # Randomly generate how many new objects of each type should be added (without exceeding maximum)\n","  # Shape = (nEvents, 1)\n","  n_new_els  = np.random.randint( 0, high=4-n_nonzero_els+1 )\n","  n_new_mus  = np.random.randint( 0, high=4-n_nonzero_mus+1 )\n","  n_new_jets = np.random.randint( 0, high=10-n_nonzero_jets+1 )\n","  if DEBUG:\n","    for (s,x) in zip([\"n_new_els\",\"n_new_mus\",\"n_new_jets\"],[n_new_els, n_new_mus, n_new_jets]):\n","      print(\"%s = \"%s,x)\n","    print(\"\")\n","\n","  # Get maximum pT of all objects for each event\n","  # Shape = (nEvents, 1)\n","  maxpts = np.max( batch[:,0,:], axis=-1 )\n","  if DEBUG:\n","    print(\"maxpts\")\n","    print(maxpts)\n","    print(\"\")\n","\n","  # Loop over events\n","  for n in range( batch_filled.shape[0] ):\n","\n","    # Generate and add new electrons satisfying: pT > 3, -pi <= phi pi, -3 < eta < 3\n","    # New pT is random fraction of max pT in event\n","    el_pts = np.expand_dims( minLeptonBase + (maxpts[n]-minLeptonBase) * np.random.rand( n_new_els[n] ), axis=1 )\n","    el_phis = np.expand_dims( 2*np.pi * ( np.random.rand(n_new_els[n]) - 0.5 ), axis=1 )\n","    el_etas = np.expand_dims( 2*3 * ( np.random.rand(n_new_els[n]) - 0.5 ), axis=1 )\n","    el_one_hot = np.concatenate( [np.zeros(shape=(n_new_els[n],1)), np.ones(shape=(n_new_els[n],1)), np.zeros(shape=(n_new_els[n],1)), np.zeros(shape=(n_new_els[n],1))], axis=1 )\n","    #els = np.concatenate( [el_pts, el_phis, el_etas, el_one_hot], axis=1 ) #! ERROR here: should be pT, eta, phi ....\n","    els = np.concatenate( [el_pts, el_etas, el_phis, el_one_hot], axis=1 )\n","    el_start = 1 + n_nonzero_els[n]\n","    el_end   = 1 + n_nonzero_els[n] + n_new_els[n]\n","    batch_filled[n,:,el_start:el_end] = np.transpose( els )\n","\n","    # Generate and add new muons satisfying: pT > 3, -pi <= phi pi, -2.1 < eta < 2.1\n","    # New pT is random fraction of max pT in event\n","    mu_pts = np.expand_dims( minLeptonBase + (maxpts[n]-minLeptonBase) * np.random.rand( n_new_mus[n] ), axis=1 )\n","    mu_phis = np.expand_dims( 2*np.pi * ( np.random.rand(n_new_mus[n]) - 0.5 ), axis=1 )\n","    mu_etas = np.expand_dims( 2*2.1 * ( np.random.rand(n_new_mus[n]) - 0.5 ), axis=1 )\n","    mu_one_hot = np.concatenate( [np.zeros(shape=(n_new_mus[n],1)), np.zeros(shape=(n_new_mus[n],1)), np.ones(shape=(n_new_mus[n],1)), np.zeros(shape=(n_new_mus[n],1))], axis=1 )\n","    #mus = np.concatenate( [mu_pts, mu_phis, mu_etas, mu_one_hot], axis=1 ) #! ERROR: should be pT, eta, phi ....\n","    mus = np.concatenate( [mu_pts, mu_etas, mu_phis, mu_one_hot], axis=1 )\n","    mu_start = 5 + n_nonzero_mus[n]\n","    mu_end = 5 + n_nonzero_mus[n] + n_new_mus[n]\n","    batch_filled[n,:,mu_start:mu_end] = np.transpose( mus )\n","\n","    # Generate and add new jets satisfying: pT > 15, -pi <= phi pi, -4 < eta < 4\n","    # New pT is random fraction of max pT in event\n","    jet_pts = np.expand_dims( minJetBase + (maxpts[n]-minJetBase) * np.random.rand( n_new_jets[n] ), axis=1 )\n","    jet_phis = np.expand_dims( 2*np.pi * ( np.random.rand(n_new_jets[n]) - 0.5 ), axis=1 )\n","    jet_etas = np.expand_dims( 2*4 * ( np.random.rand(n_new_jets[n]) - 0.5 ), axis=1 )\n","    jet_one_hot = np.concatenate( [np.zeros(shape=(n_new_jets[n],1)), np.zeros(shape=(n_new_jets[n],1)), np.zeros(shape=(n_new_jets[n],1)), np.ones(shape=(n_new_jets[n],1))], axis=1 )\n","    #jets = np.concatenate( [jet_pts, jet_phis, jet_etas, jet_one_hot], axis=1 ) #! ERROR: should be pT, eta, phi ....\n","    jets = np.concatenate( [jet_pts, jet_etas, jet_phis, jet_one_hot], axis=1 )\n","    jet_start = 9 + n_nonzero_jets[n]\n","    jet_end   = 9 + n_nonzero_jets[n] + n_new_jets[n]\n","    batch_filled[n,:,jet_start:jet_end] = np.transpose( jets )\n","\n","    # Recalculate MET\n","    # NOTE: There is a sign error in this code\n","    old_met_pt = batch_filled[n,0,0]\n","    old_met_phi = batch_filled[n,2,0]\n","    old_met = np.array( [ old_met_pt * np.sin(old_met_phi), old_met_pt * np.cos(old_met_phi) ] )\n","    if DEBUG:\n","      print(\"old_met\")\n","      print(old_met)\n","      print(\"\")\n","\n","    new_obj = np.concatenate( [ els[:,0:3], mus[:,0:3], jets[:,0:3] ], axis=0 )\n","    if DEBUG:\n","      print(\"Total (px,py) of new objects\")\n","      print(np.array( [ new_obj[:,0] * np.sin(new_obj[:,2]), new_obj[:,0] * np.cos(new_obj[:,2]) ] ).sum(axis=-1))\n","      print(\"\")\n","\n","    new_met = old_met - np.array( [ new_obj[:,0] * np.sin(new_obj[:,2]), new_obj[:,0] * np.cos(new_obj[:,2]) ] ).sum(axis=-1) #! Sign ERROR, should be old_met - ... not old_met + ...\n","    if DEBUG:\n","      print(\"MET change (note the sign difference)\")\n","      print(new_met - old_met)\n","      print(\"\")\n","\n","    new_met_pt = np.sqrt( new_met[0]**2 + new_met[1]**2 )\n","    #new_met_phi = np.arcsin( new_met[0]/new_met_pt ) #! This doesn't adequately reconstruct the full range -pi to pi\n","    if new_met[1]<0. and new_met[0]>0.:\n","      new_met_phi =  np.pi - np.arcsin( new_met[0]/new_met_pt )\n","    elif new_met[1]<0. and new_met[0]<0.:\n","      new_met_phi = -np.pi - np.arcsin( new_met[0]/new_met_pt )\n","    else:\n","      new_met_phi = np.arcsin( new_met[0]/new_met_pt )\n","\n","    batch_filled[n,0,0] = new_met_pt\n","    batch_filled[n,2,0] = new_met_phi\n","    if DEBUG:\n","      print(\"Modified batch_filled\")\n","      print(batch_filled)\n","      print(\"\")\n","  return batch_filled"],"metadata":{"id":"Vd2Exwmdvq75"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### augmentation (2): `add_objects_constptmet()` + helper functions: `get_std_rivet()`, `etaphi_smear_events()`, `collinear_fill_e_mu()`, `collinear_fill_jets()`"],"metadata":{"id":"XXn4_bmHvq76"}},{"cell_type":"code","source":["def get_std_rivet(pTs, scaler_pt, A=0.028, B=25, C=0.1):\n","    #  standard deviation for the Rivet detector simulation\n","\n","    # Only select pT > 0 entries for all events\n","    # (nEvents,1)\n","    mask = (pTs > 0)\n","    np_sett_dict = np.seterr(over = 'ignore')\n","\n","    if scaler_pt != None:\n","        std_rivet  = A/(1+np.exp( ( (pTs *scaler_pt) -B)/C) )\n","    else:\n","        std_rivet  = A/(1+np.exp( ( pTs -B)/C) )\n","    std_rivet[~mask] = 0\n","    np.seterr(over = np_sett_dict['over'])\n","    return std_rivet"],"metadata":{"id":"fs-78fxPvq76"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def etaphi_smear_events(batch, scaler_pt, scale_angle, strength=1.0, DEBUG=False):\n","  # Make repeatable\n","  if DEBUG: np.random.seed(0)\n","\n","  # Copy so that original isn't modified\n","  batch_distorted = batch.copy()\n","  if DEBUG:\n","    print(\"Original batch_distorted\")\n","    print(batch_distorted)\n","    print(\"\")\n","\n","  # Get standard deviation for pT smear amount\n","  # The std is determined by a step function so that pT in the range (0, B - window) will all be scaled by A\n","  # After B, the std is much less or zero\n","  # => Larger pT will be scaled less or not at all\n","  std = get_std_rivet( batch_distorted[:,0, 1:], scaler_pt )\n","  noise_eta = np.random.normal( loc=0.0, scale=strength*std )\n","  noise_phi = np.random.normal( loc=0.0, scale=strength*std )\n","  noise     = np.stack( [noise_eta, noise_phi], axis=1 )\n","\n","  # Smear eta, phi of all particle objects (i.e. not MET)\n","  # Note that this is effectively doing a resampling of eta, phi from a Gaussian with mean eta, phi and variance strength*std, strength*std\n","  batch_distorted[:,1:3,1:] += noise\n","\n","  # scale_angle seems to be whether phi's are assumed to be scaled to a different range in the data, so default should be False\n","  if scale_angle:\n","    # Ensure all smeared phis are in the range (-1,1)\n","    batch_distorted[:, 2, 1:] = np.where(batch_distorted[:, 2, 1:]>1, batch_distorted[:, 2, 1:]-2, batch_distorted[:, 2, 1:])\n","    batch_distorted[:, 2, 1:] = np.where(batch_distorted[:, 2, 1:]< -1, batch_distorted[:, 2, 1:]+2, batch_distorted[:, 2, 1:])\n","\n","    # Check that all smeared electron etas are in the range (-3,3)/4\n","    crosses_upper_bound_e = batch_distorted[:, 1, 1:5] > (3./4)\n","    crosses_lower_bound_e = batch_distorted[:, 1, 1:5] < (-3./4.)\n","    crosses_e = crosses_lower_bound_e | crosses_upper_bound_e # Logical OR\n","    # Check that all smeared electron etas are in the range (-2.1,2.1)/4\n","    crosses_upper_bound_mu = batch_distorted[:, 1, 5:9] > (2.1/4.)\n","    crosses_lower_bound_mu = batch_distorted[:, 1, 5:9] < (-2.1/4.)\n","    crosses_mu = crosses_lower_bound_mu | crosses_upper_bound_mu\n","    # Check that all smeared jet etas are in the range (-1,1)\n","    crosses_upper_bound_jet = batch_distorted[:, 1, 9:] > 1.\n","    crosses_lower_bound_jet = batch_distorted[:, 1, 9:] < -1.\n","    crosses_jet = crosses_lower_bound_jet | crosses_upper_bound_jet\n","    # If objects violate these conditions, set their pT, eta, phi, etc. all to zero\n","    for i in range( np.shape(batch_distorted)[1] ):\n","        batch_distorted[:, i, 1:5][crosses_e]  = 0.\n","        batch_distorted[:, i, 5:9][crosses_mu] = 0.\n","        batch_distorted[:, i, 9:][crosses_jet] = 0.\n","  else:\n","    # Ensure all smeared phis are in the range [-pi,pi]\n","    #! Really should be [-pi,pi)\n","    #! Doesn't correctly handle the case if smeared phi is outside the range [-2pi,2pi].  This is very unlikely, but technically possible...\n","    batch_distorted[:, 2, 1:] = np.where(batch_distorted[:, 2, 1:]>np.pi, batch_distorted[:, 2, 1:]-np.pi, batch_distorted[:, 2, 1:])\n","    batch_distorted[:, 2, 1:] = np.where(batch_distorted[:, 2, 1:]< -np.pi, batch_distorted[:, 2, 1:]+np.pi, batch_distorted[:, 2, 1:])\n","\n","    # Check that all smeared electron etas are in the range (-3,3)\n","    crosses_upper_bound_e = batch_distorted[:, 1, 1:5] > 3.\n","    crosses_lower_bound_e = batch_distorted[:, 1, 1:5] < -3.\n","    crosses_e = crosses_lower_bound_e | crosses_upper_bound_e\n","    if DEBUG:\n","      print(\"crosses_e\")\n","      print(crosses_e)\n","      print(\"\")\n","    # Check that all smeared muon etas are in the range (-2.1,2.1)\n","    crosses_upper_bound_mu = batch_distorted[:, 1, 5:9] > 2.1\n","    crosses_lower_bound_mu = batch_distorted[:, 1, 5:9] < -2.1\n","    crosses_mu = crosses_lower_bound_mu | crosses_upper_bound_mu\n","    if DEBUG:\n","      print(\"crosses_mu\")\n","      print(crosses_mu)\n","      print(\"\")\n","    # Check that all smeared jet etas are in the range (-4,4)\n","    crosses_upper_bound_jet = batch_distorted[:, 1, 9:] > 4.\n","    crosses_lower_bound_jet = batch_distorted[:, 1, 9:] < -4.\n","    crosses_jet = crosses_lower_bound_jet | crosses_upper_bound_jet\n","    if DEBUG:\n","      print(\"crosses_jet\")\n","      print(crosses_jet)\n","      print(\"\")\n","    # Fix objects that violate these conditions to be at but not over the edge\n","    #   Note that the former fix was to eliminate the violating particles\n","    #   but that strategy is problematic if the leading lepton got eliminated\n","    epsilon = 1e-6 #np.finfo(np.float64).eps\n","    batch_distorted[:, 1, 1:5][crosses_lower_bound_e]  = -3.  + epsilon\n","    batch_distorted[:, 1, 1:5][crosses_upper_bound_e]  =  3.  - epsilon\n","    batch_distorted[:, 1, 5:9][crosses_lower_bound_mu] = -2.1 + epsilon\n","    batch_distorted[:, 1, 5:9][crosses_upper_bound_mu] =  2.1 - epsilon\n","    batch_distorted[:, 1, 9:][crosses_lower_bound_jet] = -4.  + epsilon\n","    batch_distorted[:, 1, 9:][crosses_upper_bound_jet] =  4.  - epsilon\n","\n","  return batch_distorted"],"metadata":{"id":"Cd4M0eoVvq76"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collinear_fill_e_mu(batch, minBase=3., minLeading=23., DEBUG=False):\n","\n","  # Set minimum pT allowed (minBase + epsilon)\n","  # https://stackoverflow.com/questions/48382823/generating-random-numbers-in-numpy-with-strict-lower-bounds\n","  epsilon = np.finfo(np.float64).eps\n","  min_pT  = minBase + epsilon\n","\n","  # Make repeatable\n","  if DEBUG: np.random.seed(0)\n","\n","  batch_filled = batch.copy()\n","\n","  # ELECTRONS\n","  n_constit = 4\n","  n_nonzero = np.count_nonzero(batch_filled[:, 4, 1:5], axis=1)\n","  n_split = np.minimum(n_nonzero, n_constit-n_nonzero)\n","  idx_flip = np.where(n_nonzero != n_split)\n","  mask_split = batch_filled[:, 4, 1:5] != 0\n","  mask_split [idx_flip] = np.flip(mask_split[idx_flip], axis=1)\n","  mask_split[idx_flip]  = np.invert(mask_split[idx_flip])\n","\n","  #! Start bug fix to maintain pT selection criteria after splitting\n","\n","  # Get theoretical minima based on selection criteria\n","  #    pT for leptons must be > min_pT=3GeV and for the leading lepton pT must be >23GeV\n","  minA_pT = min_pT*np.ones(shape=mask_split.shape)              # Shape=(nEvents,4)\n","  minA_pT[:,0][(batch_filled[:,0,1] >= batch_filled[:,0,5])] = minLeading+epsilon    # batch[:,0,1] >= batch[:,0,5] selects leading electrons\n","  if DEBUG:\n","    print(\"minA_pT for Electrons\")\n","    print(minA_pT)\n","    print(\"\")\n","  mask_pT_largeEnoughToSplit = batch_filled[:, 0, 1:5] > minA_pT + min_pT\n","  new_mask_split = mask_split & mask_pT_largeEnoughToSplit\n","  extra = batch_filled[:, 0, 1:5] - (minA_pT + min_pT)\n","  if DEBUG:\n","    print(\"extra\")\n","    print(extra)\n","    print(\"\")\n","  r_split = np.random.uniform(size=new_mask_split.shape)\n","  if DEBUG:\n","    print(\"r_split\")\n","    print(r_split)\n","    print(\"\")\n","  a = minA_pT * new_mask_split +       r_split * new_mask_split*extra\n","  b = min_pT * new_mask_split + (1.0-r_split) * new_mask_split*extra\n","  c =                ~new_mask_split*batch_filled[:, 0, 1:5]\n","  batch_filled[:, 0, 1:5] = a + c + np.flip(b, axis=1)\n","  batch_filled[:, 1, 1:5] += np.flip(new_mask_split*batch_filled[:, 1, 1:5], axis=1)\n","  batch_filled[:, 2, 1:5] += np.flip(new_mask_split*batch_filled[:, 2, 1:5], axis=1)\n","  batch_filled[:, 3, 1:5] += np.flip(new_mask_split*batch_filled[:, 3, 1:5], axis=1) #! Bug fix to also fill identity\n","  batch_filled[:, 4, 1:5] += np.flip(new_mask_split*batch_filled[:, 4, 1:5], axis=1)\n","\n","  #! End bug fix to maintain pT selection criteria after splitting\n","\n","  # MUONS\n","  n_constit = 4\n","  n_nonzero = np.count_nonzero(batch_filled[:, 5, 5:9], axis=1)\n","  n_split = np.minimum(n_nonzero, n_constit-n_nonzero)\n","  idx_flip = np.where(n_nonzero != n_split)\n","  mask_split = batch_filled[:, 5, 5:9] != 0\n","  mask_split [idx_flip] = np.flip(mask_split[idx_flip], axis=1)\n","  mask_split[idx_flip] = np.invert(mask_split[idx_flip])\n","\n","  #! Start bug fix to maintain pT selection criteria after splitting\n","\n","  # Get theoretical minima based on selection criteria\n","  #    pT for leptons must be > min_pT=3GeV and for the leading lepton pT must be >23GeV\n","  minA_pT = min_pT*np.ones(shape=mask_split.shape)              # Shape=(nEvents,4)\n","  minA_pT[:,0][(batch_filled[:,0,1] < batch_filled[:,0,5])] = minLeading+epsilon    # batch[:,0,1] < batch[:,0,5] selects leading muons\n","  if DEBUG:\n","    print(\"minA_pT for Muons\")\n","    print(minA_pT)\n","    print(\"\")\n","  mask_pT_largeEnoughToSplit = batch_filled[:, 0, 5:9] > minA_pT + min_pT\n","  new_mask_split = mask_split & mask_pT_largeEnoughToSplit\n","  extra = batch_filled[:, 0, 5:9] - (minA_pT + min_pT)\n","  r_split = np.random.uniform(size=new_mask_split.shape)\n","  a = minA_pT * new_mask_split +       r_split * new_mask_split*extra\n","  b = min_pT * new_mask_split + (1.0-r_split) * new_mask_split*extra\n","  c =                ~new_mask_split*batch_filled[:, 0, 5:9]\n","  batch_filled[:, 0, 5:9] = a + c + np.flip(b, axis=1)\n","  batch_filled[:, 1, 5:9] += np.flip(new_mask_split*batch_filled[:, 1, 5:9], axis=1)\n","  batch_filled[:, 2, 5:9] += np.flip(new_mask_split*batch_filled[:, 2, 5:9], axis=1)\n","  batch_filled[:, 3, 5:9] += np.flip(new_mask_split*batch_filled[:, 3, 5:9], axis=1) #! Bug fix to also fill identity\n","  batch_filled[:, 5, 5:9] += np.flip(new_mask_split*batch_filled[:, 5, 5:9], axis=1)\n","\n","  #! End bug fix to maintain pT selection criteria after splitting\n","\n","  return batch_filled"],"metadata":{"id":"-2JZ4elU5ZIQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collinear_fill_jets(batch, minBase=15., DEBUG=False):\n","\n","  # Set minimum pT allowed (minBase + epsilon)\n","  # https://stackoverflow.com/questions/48382823/generating-random-numbers-in-numpy-with-strict-lower-bounds\n","  epsilon = np.finfo(np.float64).eps\n","  min_pT  = minBase + epsilon\n","\n","  # Make repeatable\n","  if DEBUG: np.random.seed(0)\n","\n","  # Copy so that original isn't modified\n","  batch_filled = batch.copy()\n","  if DEBUG:\n","    print(\"Original batch_filled\")\n","    print(batch_filled)\n","    print(\"\")\n","\n","  # Get number of zero and non-zero objects\n","  # (nEvents,1)\n","  n_constit = 10\n","  n_nonzero = np.count_nonzero(batch_filled[:, 6, 9:], axis=1)\n","  if DEBUG:\n","    print(\"n_nonzero, n_constit - n_nonzero = \",n_nonzero, n_constit-n_nonzero)\n","    print(\"\")\n","\n","  # Get the number of objects that will be split.\n","  # Note we can't split more objects than are there so n_split <= n_nonzero\n","  # And we also can't split more than there are spaces to add n_split <= n_constit-n_nonzero\n","  # So we choose n_split to be the lesser of the two\n","  # (nEvents,1)\n","  n_split = np.minimum(n_nonzero, n_constit-n_nonzero)\n","  if DEBUG:\n","    print(\"n_split (min of the two) = \",n_split)\n","    print(\"\")\n","\n","  # Empty array if n_nonzero == n_split for all events\n","  # Otherwise returns the event index where there are more non-zero jets than zero jets\n","  # (nEvents,1)\n","  idx_flip = np.where(n_nonzero != n_split)\n","  if DEBUG:\n","    print(\"idx_flip\", idx_flip)\n","    print(\"\")\n","\n","  # Mask jet entries that are not zero for all events\n","  # (nEvents, n_constit=10)\n","  mask_split = batch_filled[:, 6, 9:] != 0\n","  if DEBUG:\n","    print(\"mask_split.shape\", mask_split.shape)\n","    for i in range(batch.shape[0]):\n","      print(\"mask_split[%d,:]\"%i)\n","      print(mask_split[i,:])\n","    print(\"\")\n","\n","  # Select the events where there are more non-zero jets than zero jets (idx_flip)\n","  if DEBUG:\n","    print(\"mask_split[idx_flip]\")\n","    print(mask_split[idx_flip])\n","    print(\"np.flip(...)\")   # Reverse the order of elements in an array along the given axis\n","  mask_split [idx_flip] = np.flip(mask_split[idx_flip], axis=1)\n","  if DEBUG:\n","    print(mask_split[idx_flip])\n","    print(\"np.invert(...)\") # Change True <-> False\n","  mask_split[idx_flip] = np.invert(mask_split[idx_flip])\n","  if DEBUG:\n","    print(mask_split[idx_flip])\n","    print(\"\")\n","\n","  #! Bug fix\n","  # Ensure that entries to be split also have pT > 2*min, set the mask of any that fail to false\n","  # pT = batch_filled[:, 0, 9:]\n","  # mask_split.shape = (nEvents, n_constit=10)\n","  mask_pT_largeEnoughToSplit = batch_filled[:, 0, 9:] > 2*min_pT\n","  new_mask_split = mask_split & mask_pT_largeEnoughToSplit\n","  if DEBUG:\n","    print(\"mask_split\")\n","    print(mask_split)\n","    print(\"mask_pT_largeEnoughToSplit\")\n","    print(mask_pT_largeEnoughToSplit)\n","    print(\"new_mask_split\")\n","    print(new_mask_split)\n","    print(\"\")\n","\n","  # Get random split fractions\n","  r_split = np.random.uniform(size=new_mask_split.shape)\n","  if DEBUG:\n","    print(\"r_split.shape\",r_split.shape) # (nEvents, n_constit=10)\n","    print(\"r_split\")\n","    print(r_split)\n","    print(\"\")\n","\n","  # Get the extra amount to be split\n","  #   Note that for some of these, extra will be negative\n","  #   but those entries will be set to False and thus not contribute by new_mask_split\n","  extra = batch_filled[:, 0, 9:] - 2*min_pT\n","\n","  # Split the pT for valid objects (i.e. with at least twice the minimum)\n","  # For each event split the first n_split pT=min+extra into min + r*extra and min + (1-r)*extra\n","  a = min_pT * new_mask_split +       r_split * new_mask_split*extra\n","  b = min_pT * new_mask_split + (1.0-r_split) * new_mask_split*extra\n","  if DEBUG:\n","    print(\"a+b = pT for the first n_split entries\")\n","    print(\"a   = \",a)\n","    print(\"b   = \",b)\n","    print(\"c captures the remaining pT entries\")\n","  c =                ~new_mask_split*batch_filled[:, 0, 9:]\n","  if DEBUG:\n","    print(\"c   = \",c)\n","    print(\"a+b+c = \",a+b+c)\n","    print(\"pT    = \",batch_filled[:, 0, 9:])\n","    print(\"\")\n","\n","  # Now take one part (i.e. b) and assign it to the last n_split zero entries and recombine\n","  if DEBUG:\n","    print(\"np.flip(b, axis=1)\")\n","    print(np.flip(b, axis=1))\n","    print(\"\")\n","  batch_filled[:, 0, 9:] = a + c + np.flip(b, axis=1)\n","\n","  # Set all other quantities (i.e. eta, phi) to be the same\n","  batch_filled[:, 1, 9:] += np.flip(new_mask_split*batch_filled[:, 1, 9:], axis=1)\n","  batch_filled[:, 2, 9:] += np.flip(new_mask_split*batch_filled[:, 2, 9:], axis=1)\n","  batch_filled[:, 3, 9:] += np.flip(new_mask_split*batch_filled[:, 3, 9:], axis=1) #! Bug fix to also fill identity\n","  batch_filled[:, 6, 9:] += np.flip(new_mask_split*batch_filled[:, 6, 9:], axis=1)\n","  if DEBUG:\n","    print(\"New batch_filled\")\n","    print(batch_filled)\n","    print(\"\")\n","\n","  return batch_filled"],"metadata":{"id":"yLWEFX286NIT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def add_objects_constptmet( batch, scaler_pt, scale_angle, etaphi_smear_strength, minJetBase=15., minLeptonBase=3., minLeadingpT=23., DEBUG=False):\n","  batch_filled = batch.copy()\n","  batch_filled = collinear_fill_jets( batch_filled, minBase=minJetBase, DEBUG=DEBUG ) # Minimum pT for jets is 15 GeV\n","  batch_filled = collinear_fill_e_mu( batch_filled, minBase=minLeptonBase, minLeading=minLeadingpT, DEBUG=DEBUG ) # Minimum pT for leptons is 3 GeV, min for leading lepton is 23 GeV\n","  batch_filled = etaphi_smear_events( batch_filled, scaler_pt, scale_angle, strength=etaphi_smear_strength, DEBUG=DEBUG )\n","  return batch_filled"],"metadata":{"id":"fqOIkak_Q5il"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### augmentation (3): `shift_met_or_pt()`"],"metadata":{"id":"DLBhS2bBvq77"}},{"cell_type":"code","source":["# (3)\n","def shift_met_or_pt( batch, DEBUG=False):\n","  # Make repeatable\n","  if DEBUG: np.random.seed(0)\n","\n","  # Copy so that original isn't modified\n","  batch_shifted = batch.copy()\n","  if DEBUG:\n","    print(\"Original batch_shifted\")\n","    print(batch_shifted)\n","    print(\"\")\n","\n","  # Random numbers for each event to decide whether to shift the MET (0), pT (1), or both (2)\n","  # Shape = (nEvents, 1)\n","  rands = np.random.randint( low=0, high=3, size=batch_shifted.shape[0] )\n","  if DEBUG:\n","    print(\"If rands==0, shift MET; ==1, shift pT; ==2, shift both\")\n","    print(\"rands = \",rands)\n","    print(\"\")\n","\n","  # Randomly generate a constant multiplicative shift factor for each event for pT and MET\n","  shifts     =  1.0 + np.random.rand( batch_shifted.shape[0] ) * 4.0  # Uniform range [  1, 5)\n","  shifts_met =  0.5 + np.random.rand( batch_shifted.shape[0] ) * 4.5  # Uniform range [0.5, 5)\n","  if DEBUG:\n","    print(\"    shifts = \", shifts)\n","    print(\"shifts_met = \",shifts_met)\n","    print(\"\")\n","\n","  # Shift MET, pT, or both for each event depending on rands\n","  batch_shifted[np.where(rands==0),0,0 ] *= shifts_met[np.where(rands==0)]\n","  batch_shifted[np.where(rands==1),0,1:] *= np.expand_dims( shifts[np.where(rands==1)], axis=-1 )\n","  batch_shifted[np.where(rands==2),0,: ] *= np.expand_dims( shifts[np.where(rands==2)], axis=-1 )\n","  if DEBUG:\n","    print(\"New batch_shifted\")\n","    print(batch_shifted)\n","    print(\"\")\n","  return batch_shifted"],"metadata":{"id":"2iDsUtiIvq77"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Functions to check whether augmentation (2) is okay for that event (i.e. okay => the event will transform under (2) )"],"metadata":{"id":"dWZ8eE3BEq_8"}},{"cell_type":"markdown","source":["##### `aug_2_jetConditions()`"],"metadata":{"id":"8zlqGDAvvrFm"}},{"cell_type":"code","source":["def aug_2_jetConditions(batch_aug, minBase=15.):\n","  \"\"\"\n","  Checks whether any jet in an event will transform under augmentation (2).\n","  If the jets of an event will transform, then the mask is True for that event.\n","  \"\"\"\n","  epsilon = np.finfo(np.float64).eps\n","  min_pT  = minBase + epsilon\n","\n","  # Figure out which jets might be split based on filled and available slots\n","  #   mask_split is True for jets that might be split False otherwise; shape=(nEvents, n_constit)\n","  n_constit = 10\n","  n_nonzero = np.count_nonzero(batch_aug[:, 6, 9:], axis=1)\n","  n_split   = np.minimum(n_nonzero, n_constit-n_nonzero)\n","  idx_flip  = np.where(n_nonzero != n_split)\n","  mask_split = batch_aug[:, 6, 9:] != 0\n","  mask_split [idx_flip] = np.flip(mask_split[idx_flip], axis=1)\n","  mask_split[idx_flip] = np.invert(mask_split[idx_flip])\n","  mask_pT_largeEnoughToSplit = batch_aug[:, 0, 9:] > 2*min_pT\n","  new_mask_split = mask_split & mask_pT_largeEnoughToSplit\n","\n","  # Decide whether any jets in the event transform\n","  #    passJet shape is (nEvents,)\n","  passJet = np.any(new_mask_split, axis=1)\n","\n","  return passJet\n","\n","## Example\n","# EVENTNUMBER = 5\n","# testEvent   = dataDict['bkg']['Particles'][0:EVENTNUMBER,:,:].reshape(EVENTNUMBER,-1,4)\n","# batch = format_for_anomalyAugmentations(testEvent)\n","# print(aug_2_jetConditions(batch, minBase=15.)) # array([False, False, False,  True,  True])\n","# print(batch[:,:,9:13])# 13 is just to make print-out more readable since the rest are zero"],"metadata":{"id":"aA7YxnTyEHQn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### `aug_2_electronConditions()`"],"metadata":{"id":"qBupMYc8v0Ei"}},{"cell_type":"code","source":["def aug_2_electronConditions(batch_aug, minBase=3., minLeading=23.):\n","\n","  epsilon = np.finfo(np.float64).eps\n","  min_pT  = minBase + epsilon\n","\n","  n_constit = 4\n","  n_nonzero = np.count_nonzero(batch_aug[:, 4, 1:5], axis=1)\n","  n_split = np.minimum(n_nonzero, n_constit-n_nonzero)\n","  idx_flip = np.where(n_nonzero != n_split)\n","  mask_split = batch_aug[:, 4, 1:5] != 0\n","  mask_split [idx_flip] = np.flip(mask_split[idx_flip], axis=1)\n","  mask_split[idx_flip]  = np.invert(mask_split[idx_flip])\n","\n","\n","  # Get theoretical minima based on selection criteria\n","  #    pT for leptons must be > min_pT=3GeV and for the leading lepton pT must be >23GeV\n","  minA_pT = min_pT*np.ones(shape=mask_split.shape)   # Shape=(nEvents,4)\n","  minA_pT[:,0][(batch_aug[:,0,1] >= batch_aug[:,0,5])] = minLeading+epsilon    # batch[:,0,1] >= batch[:,0,5] selects leading electrons\n","  mask_pT_largeEnoughToSplit = batch_aug[:, 0, 1:5] > minA_pT + min_pT\n","  new_mask_split = mask_split & mask_pT_largeEnoughToSplit\n","\n","  # Decide whether any electrons in the event transform\n","  #    passElectron shape is (nEvents,)\n","  passElectron = np.any(new_mask_split, axis=1)\n","\n","  return passElectron\n","\n","## Example\n","# EVENTNUMBER = 5\n","# testEvent   = dataDict['bkg']['Particles'][0:EVENTNUMBER,:,:].reshape(EVENTNUMBER,-1,4)\n","# batch = format_for_anomalyAugmentations(testEvent)\n","# print(aug_2_electronConditions(batch, minBase=3.,minLeading=23.)) # array([False False  True False False])\n","# print(batch[:,:,1:5])"],"metadata":{"id":"E_NW3A5Lc8un"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### `aug_2_muonsConditions()`"],"metadata":{"id":"34X1_SjMv48u"}},{"cell_type":"code","source":["def aug_2_muonsConditions(batch_aug, minBase=3., minLeading=23.):\n","\n","  epsilon = np.finfo(np.float64).eps\n","  min_pT  = minBase + epsilon\n","\n","  n_constit = 4\n","  n_nonzero = np.count_nonzero(batch_aug[:, 5, 5:9], axis=1)\n","  n_split = np.minimum(n_nonzero, n_constit-n_nonzero)\n","  idx_flip = np.where(n_nonzero != n_split)\n","  mask_split = batch_aug[:, 5, 5:9] != 0\n","  mask_split [idx_flip] = np.flip(mask_split[idx_flip], axis=1)\n","  mask_split[idx_flip] = np.invert(mask_split[idx_flip])\n","\n","  # Get theoretical minima based on selection criteria\n","  #    pT for leptons must be > min_pT=3GeV and for the leading lepton pT must be >23GeV\n","  minA_pT = min_pT*np.ones(shape=mask_split.shape)  # Shape=(nEvents,4)\n","  minA_pT[:,0][(batch_aug[:,0,1] < batch_aug[:,0,5])] = minLeading+epsilon    # batch[:,0,1] < batch[:,0,5] selects leading muons\n","  mask_pT_largeEnoughToSplit = batch_aug[:, 0, 5:9] > minA_pT + min_pT\n","  new_mask_split = mask_split & mask_pT_largeEnoughToSplit\n","\n","  # Decide whether any muons in the event transform\n","  #    passMuon shape is (nEvents,)\n","  passMuon = np.any(new_mask_split, axis=1)\n","\n","  return passMuon\n","\n","## Example\n","# EVENTNUMBER = 5\n","# testEvent   = dataDict['bkg']['Particles'][0:EVENTNUMBER,:,:].reshape(EVENTNUMBER,-1,4)\n","# batch = format_for_anomalyAugmentations(testEvent)\n","# print(aug_2_muonsConditions(batch, minBase=3., minLeading=23.)) # array([False False False  True False])\n","# print(batch[:,:,5:9])"],"metadata":{"id":"A79fItH_eKIe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### `aug_2_conditions()`"],"metadata":{"id":"knDi4AeXv9jB"}},{"cell_type":"code","source":["def aug_2_conditions(batch_aug, minJetBase=15., minLeptonBase=3., minLeadingpT=23.):\n","  \"\"\"\n","  Function to determine which events in batch_aug will fail to transform under augmentation (2).\n","  Since these events won't transform, augmentation (2) should not be performed.\n","\n","  Inputs:\n","    batch_aug:  Batch of events, shape = (nEvents, 7, 19)\n","  Outputs:\n","    Mask with shape=(nEvents,1). True indicates that augmentation (2) can be performed. False indicates that it cannot.\n","  \"\"\"\n","\n","  # Will jets transform\n","  passJet = aug_2_jetConditions(batch_aug, minBase=minJetBase) # shape=(nEvents,)\n","\n","  # Will electrons transform\n","  passElectron = aug_2_electronConditions(batch_aug, minBase=minLeptonBase, minLeading=minLeadingpT) # shape=(nEvents,)\n","\n","  # Will muons transform\n","  passMuon = aug_2_muonsConditions(batch_aug, minBase=minLeptonBase, minLeading=minLeadingpT) # shape=(nEvents,)\n","\n","  # If any of the above conditions are True for an event then the overall mask should be True for that event\n","  return passJet | passElectron | passMuon\n","\n","## Example\n","# EVENTNUMBER = 5\n","# testEvent   = dataDict['bkg']['Particles'][0:EVENTNUMBER,:,:].reshape(EVENTNUMBER,-1,4)\n","# batch = format_for_anomalyAugmentations(testEvent)\n","# print(aug_2_conditions(batch)) # array([False False  True  True  True])\n","# print(batch)\n","#\n","## To get events that are able to be transformed by (2)\n","# aug2condPass = aug_2_conditions(batch_aug)\n","# print(batch[aug2condPass,:,:])\n","## To get events that are unable to be transformed by (2)\n","# print(batch[~aug2condPass,:,:])"],"metadata":{"id":"m9coYW73FVB0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JLhA5t-yRNB5"},"source":["# Plotting functions"]},{"cell_type":"markdown","metadata":{"id":"9seIfw2pJNmq"},"source":["## Misc. helper functions"]},{"cell_type":"markdown","metadata":{"id":"dK5LQhiZH0kv"},"source":["##### `RGBAtoRGBAtuple()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dfhj_RXcH-OG"},"outputs":[],"source":["def RGBAtoRGBAtuple(color, TYPE='tuple'):\n","  \"\"\"\n","  Quick function to convert human RGBA to python RGBA tuple format. Example:\n","     Human RGBA  = (120,15,116,1)\n","     Python RGBA = RGBAtoRGBAtuple((120,15,116,1))\n","\n","  Inputs:\n","    color:   Human RGBA tuple\n","    TYPE:    Flag to indicate whether you want function to return a list or tuple; default is tuple\n","  Outputs:\n","    Python RGBA tuple (or list)\n","  \"\"\"\n","  r = color[0]/255\n","  g = color[1]/255\n","  b = color[2]/255\n","  a = color[3]\n","\n","  if TYPE=='list':\n","    return [r, g, b, a]\n","  else:\n","    return (r, g, b, a)"]},{"cell_type":"markdown","metadata":{"id":"PB_iQRWuJae4"},"source":["## Data plotting functions"]},{"cell_type":"markdown","metadata":{"id":"qsMMsL9qH_h7"},"source":["##### `plotDataHists() `"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SZDecAowIMKl"},"outputs":[],"source":["def plotDataHists(objectBkg, objectSigList, plotArgDict):\n","  \"\"\"\n","  Plot histograms of an object's pT, eta, phi for Signal and Signal+Background\n","  for each signal case.\n","\n","  Inputs:\n","    objectBkg:      A background event's object's (e.g. MET) pT, eta, phi;\n","                    shape = (Nb, 3); Nb = number of background events\n","    objectSigList:  A list of signal event object's (e.g. MET) pT, eta, phi for\n","                    nCases number of signal cases; List of tuples with shapes\n","                    (Ns, 3); Ns = number of signal events\n","    plotArgDict:    Dictionary of plotting arguments (see example below)\n","\n","  Outputs:\n","    Nothing returned; Show plot\n","\n","\n","  Example plotArgDict:\n","  # Define colors to use for 4 signal types\n","      SIGNAL_COLOR_ARR = np.array([ RGBAtoRGBAtuple((231, 186, 81, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((140, 162, 82, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((165, 81, 148, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((214, 97, 107, 1), TYPE='list')\n","                                  ])\n","  # Or use a matplotlib color map\n","      SIGNAL_COLOR_ARR = plt.colormaps['rainbow'].reversed()\n","\n","  # Then define dictionary as follows\n","      plotArgDict = {}\n","      plotArgDict['pltDim']             = (1,3) # i.e. 1 row, 3 columns\n","      plotArgDict['xAxisLimsList']      = [(0, 1500), (-5, 5), (-np.pi, np.pi)]\n","      plotArgDict['title']              = r'MET'\n","      plotArgDict['nBins']              = 50\n","      plotArgDict['CMAP']               = SIGNAL_COLOR_ARR\n","      plotArgDict['sigObjectNameList']  = [r'$A$', r'$h^0$', r'$h^\\pm$', r'$LQ$']\n","  \"\"\"\n","\n","  #-- Preliminary Figure Setup --#\n","  pltDim = plotArgDict['pltDim']\n","  fig_size = (7*pltDim[1], 7*pltDim[0])\n","  fig = plt.figure(constrained_layout=True, figsize=fig_size)\n","  gs = GridSpec(pltDim[0], pltDim[1], figure=fig, hspace=0.1)\n","  axes = []\n","  for i in range(pltDim[1]):\n","    axes.append(fig.add_subplot(gs[:, i]))\n","\n","  xLabelList = [r'$p_{\\rm T}$', r'$\\eta$', r'$\\phi$']\n","\n","  #-- Define color map --#\n","  # Reference: https://stackoverflow.com/questions/38208700/matplotlib-plot-lines-with-colors-through-colormap\n","  nCases    = len(objectSigList)  # Number of cases to plot\n","  if isinstance(plotArgDict['CMAP'], np.ndarray) and (plotArgDict['CMAP'].shape[0]==nCases):\n","    colorList = plotArgDict['CMAP']\n","  else:\n","    CMAP      = plotArgDict['CMAP']\n","    colorList = CMAP(np.linspace(0,1,nCases))\n","\n","  #-- Combine data for S+B plots for all signal types --#\n","  objectBkgSigList = []\n","  for objectSig in objectSigList:\n","    objectBkgSigList.append(np.concatenate((objectBkg, objectSig), axis=0))\n","\n","\n","  #-- Loop over axes to make plots --#\n","  for i in range(pltDim[1]):\n","\n","    ax = axes[i]\n","\n","    # Set axis and title information\n","    xmin, xmax = plotArgDict['xAxisLimsList'][i]\n","    ax.set_xlim(xmin, xmax)\n","    ax.set(xlabel=xLabelList[i])\n","    ax.xaxis.label.set_size(16)\n","\n","    if i==0:\n","      ax.set(ylabel=r'Simulated events')\n","      ax.yaxis.label.set_size(16)\n","\n","    if i==1:\n","      ax.set_title(plotArgDict['title'], fontsize=20)\n","\n","    # Set bin information\n","    bins = np.linspace(xmin, xmax, plotArgDict['nBins'])\n","\n","    #-- Plot background case --#\n","    _, _, _,   = ax.hist(objectBkg[:,i], bins=bins, histtype = 'step', edgecolor='black', linestyle='-', linewidth=2, fill=False, log=True, label='Background')\n","\n","    #-- Loop over signal cases --#\n","    for j in range(nCases):\n","      objectBkgSig = objectBkgSigList[j]\n","      objectSig    = objectSigList[j]\n","\n","      # Get data\n","      SB = objectBkgSig[:,i]\n","      S  = objectSig[:,i]\n","\n","      # Make plot\n","      Slabel = plotArgDict['sigObjectNameList'][j]+' Signal'\n","      SBlabel = Slabel+' + Background'\n","      _, _, _,   = ax.hist(SB, bins=bins, histtype = 'step', edgecolor=colorList[j], linestyle='--', linewidth=2, fill=False, log=True, label=SBlabel)\n","\n","    if i==0:\n","      ax.legend(loc='upper right')\n","\n","  #-- Show the plot --#\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"xhySvQshIyN_"},"source":["##### `plotMultiplicityData() `"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hoBn1FAHRiQd"},"outputs":[],"source":["def plotMultiplicityData(multBkgList, multSigList, plotArgDict):\n","  \"\"\"\n","  Plot histograms of the multiplicity of electrons, muons, and jets for Background and each Signal case.\n","\n","  Inputs:\n","    multBkgList:      List of arrays corresponding to electron, muon, and jet multiplicities\n","                      E.g. multBkgList[0] is an array of shape (Nb,) where  Nb = number of background events\n","                      multBkgList[0][i] is the electron multiplicity for the ith event\n","    multSigList:      List of list of arrays corresponding to electron, muon, and jet multiplicities for each signal type\n","                      E.g. multSigList[0] is the list of electron multiplicities for all signal types\n","                      multSigList[0][j] is an array of shape (Ns,) where Ns = number of signal events in the jth signal type\n","                      multSigList[0][j][i] is the electron multiplicity for the ith event of signal type j\n","    plotArgDict:    Dictionary of plotting arguments (see example below)\n","\n","  Outputs:\n","    Nothing returned; Show plot\n","\n","\n","  Example plotArgDict:\n","  # Define colors to use for 4 signal types\n","      SIGNAL_COLOR_ARR = np.array([ RGBAtoRGBAtuple((231, 186, 81, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((140, 162, 82, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((165, 81, 148, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((214, 97, 107, 1), TYPE='list')\n","                                  ])\n","  # Or use a matplotlib color map\n","      SIGNAL_COLOR_ARR = plt.colormaps['rainbow'].reversed()\n","\n","  # Then define dictionary as follows\n","      plotArgDict = {}\n","      plotArgDict['pltDim']             = (1,3) # i.e. 1 row, 3 columns\n","      plotArgDict['xAxisLimsList']      = [(-0.5, 5.5), (-0.5, 5.5), (-0.5, 10.5)]\n","      plotArgDict['title']              = r'Multiplicity'\n","      plotArgDict['CMAP']               = SIGNAL_COLOR_ARR\n","      plotArgDict['sigObjectNameList']  = [r'$A$', r'$h^0$', r'$h^\\pm$', r'$LQ$']\n","  \"\"\"\n","\n","  #-- Preliminary Figure Setup --#\n","  pltDim = plotArgDict['pltDim']\n","  fig_size = (7*pltDim[1], 7*pltDim[0])\n","  fig = plt.figure(constrained_layout=True, figsize=fig_size)\n","  gs = GridSpec(pltDim[0], pltDim[1], figure=fig, hspace=0.1)\n","  axes = []\n","  for i in range(pltDim[1]):\n","    axes.append(fig.add_subplot(gs[:, i]))\n","\n","  xLabelList = [r'$e$ multiplicity', r'$\\mu$ multiplicity', r'${\\rm jet}$ multiplicity']\n","\n","\n","  #-- Define color map --#\n","  # Reference: https://stackoverflow.com/questions/38208700/matplotlib-plot-lines-with-colors-through-colormap\n","  nCases    = len(multSigList[0])  # Number of cases to plot\n","  if isinstance(plotArgDict['CMAP'], np.ndarray) and (plotArgDict['CMAP'].shape[0]==nCases):\n","    colorList = plotArgDict['CMAP']\n","  else:\n","    CMAP      = plotArgDict['CMAP']\n","    colorList = CMAP(np.linspace(0,1,nCases))\n","\n","\n","  #-- Loop over axes to make plots --#\n","  # i=0 => Electrons\n","  # i=1 => Muons\n","  # i=2 => Jets\n","  for i in range(pltDim[1]):\n","    ax = axes[i]\n","\n","    # Set axis and title information\n","    xmin, xmax = plotArgDict['xAxisLimsList'][i]\n","    ax.set_xlim(xmin, xmax)\n","    ax.set(xlabel=xLabelList[i])\n","    ax.xaxis.label.set_size(16)\n","\n","    if i==0:\n","      ax.set(ylabel=r'Percent of simulated events')\n","      ax.yaxis.label.set_size(16)\n","\n","    if i==1:\n","      ax.set_title(plotArgDict['title'], fontsize=20)\n","\n","    # Set bin information\n","    if i!=2:\n","      bins = np.linspace(0, 5, 6)-0.5\n","    else:\n","      bins = np.linspace(0, 11, 12)-0.5\n","\n","    #-- Plot background case --#\n","    _, _, _,   = ax.hist(multBkgList[i], bins=bins, histtype = 'step', edgecolor='black', linestyle='-', linewidth=2, fill=False, log=True, label='Background', density=True)\n","\n","    #-- Loop over signal cases --#\n","    for j in range(nCases):\n","      multSigList_ = multSigList[i]\n","\n","      # Make plot\n","      Slabel = plotArgDict['sigObjectNameList'][j]+' Signal'\n","      _, _, _,   = ax.hist(multSigList_[j], bins=bins, histtype = 'step', edgecolor=colorList[j], linestyle='--', linewidth=2, fill=False, log=True, label=Slabel, density=True)\n","\n","    if i==2:\n","      ax.legend(loc='upper right')\n","\n","  #-- Show the plot --#\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"nYuNY2kZrcyj"},"source":["##### `plotDataAugHists()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_VB1p64orcyk"},"outputs":[],"source":["def plotDataAugHists(objectBkg, objectAugBkg, plotArgDict):\n","    \"\"\"\n","    Plot histograms of an object's pT, eta, phi for Background and Augmented Bkg.\n","\n","    Inputs:\n","        objectBkg:      A background event's object's (e.g. MET) pT, eta, phi;\n","                        shape = (Nb, 3); Nb = number of background events\n","        objectAugBkg:   An augmented background event's object's (e.g. MET) pT, eta, phi;\n","                        shape = (Ns, 3); Ns = number of augmented background events (fake signal)\n","        plotArgDict:    Dictionary of plotting arguments (see example below)\n","\n","    Outputs:\n","        Nothing returned; Show plot\n","\n","\n","    Example plotArgDict:\n","        plotArgDict = {}\n","        plotArgDict['pltDim']             = (1,3) # i.e. 1 row, 3 columns\n","        plotArgDict['xAxisLimsList']      = [(0, 1500), (-5, 5), (-np.pi, np.pi)]\n","        plotArgDict['title']              = r'MET'\n","        plotArgDict['nBins']              = 50\n","        plotArgDict['sigObjectNameList']  = [r'$A$', r'$h^0$', r'$h^\\pm$', r'$LQ$']\n","    \"\"\"\n","    #-- Preliminary Figure Setup --#\n","    pltDim = plotArgDict['pltDim']\n","    fig_size = (7*pltDim[1], 7*pltDim[0])\n","    fig = plt.figure(constrained_layout=True, figsize=fig_size)\n","    gs = GridSpec(pltDim[0], pltDim[1], figure=fig, hspace=0.1)\n","    axes = []\n","    for i in range(pltDim[1]):\n","        axes.append(fig.add_subplot(gs[:, i]))\n","\n","    xLabelList = [r'$p_{\\rm T}$', r'$\\eta$', r'$\\phi$']\n","\n","    #-- Loop over axes to make plots --#\n","    for i in range(pltDim[1]):\n","\n","        ax = axes[i]\n","\n","        # Set axis and title information\n","        xmin, xmax = plotArgDict['xAxisLimsList'][i]\n","        ax.set_xlim(xmin, xmax)\n","        ax.set(xlabel=xLabelList[i])\n","        ax.xaxis.label.set_size(16)\n","\n","        if i==0:\n","            ax.set(ylabel=r'Simulated events')\n","            ax.yaxis.label.set_size(16)\n","\n","        if i==1:\n","            ax.set_title(plotArgDict['title'], fontsize=20)\n","\n","        # Set bin information\n","        bins = np.linspace(xmin, xmax, plotArgDict['nBins'])\n","\n","        #-- Plot background case --#\n","        _, _, _,   = ax.hist(objectBkg[:,i], bins=bins, histtype = 'step', edgecolor='black', linestyle='-', linewidth=2, fill=False, log=True, label='Background')\n","        _, _, _,   = ax.hist(objectAugBkg[:,i], bins=bins, histtype = 'step', edgecolor='purple', linestyle='--', linewidth=2, fill=False, log=True, label='Augmented Background')\n","\n","        if i==0:\n","            ax.legend(loc='upper right')\n","\n","    #-- Show the plot --#\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"P50l8xVPJs2w"},"source":["## OT results plotting functions"]},{"cell_type":"markdown","metadata":{"id":"wl2ft0VRILr5"},"source":["##### `plotScoreHists() `"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vHZAGzmOIZH0"},"outputs":[],"source":["def plotScoreHists(scoreBkg, scoreSigList, plotArgDict):\n","  \"\"\"\n","  Plot a 1D histogram of the anomaly score for the background and each signal case\n","\n","  Inputs:\n","    scoreBkg:      An array of anomaly scores for a set of background events;\n","                   shape = (Nb, ); Nb = number of background events\n","    scoreSigList:  A list of arrays of anomaly scores for nCases number of signal cases;\n","                   List of length nCases of arrays with shapes (Ns, 3);\n","                   Ns = number of signal events\n","    plotArgDict:    Dictionary of plotting arguments (see example below)\n","\n","  Outputs:\n","    Nothing returned; Show plot\n","\n","  Example plotArgDict:\n","  # Define colors to use for 4 signal types\n","      SIGNAL_COLOR_ARR = np.array([ RGBAtoRGBAtuple((231, 186, 81, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((140, 162, 82, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((165, 81, 148, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((214, 97, 107, 1), TYPE='list')\n","                                  ])\n","  # Or use a matplotlib color map\n","      SIGNAL_COLOR_ARR = plt.colormaps['rainbow'].reversed()\n","\n","  # Then define dictionary as follows\n","  plotArgDict = {}\n","  plotArgDict['pltDim']             = (3,3)\n","  plotArgDict['xAxisLims']          = (0, 17500)\n","  plotArgDict['xLabel']             = r'Anomaly Score: $W_2^2(\\cdot, \\cdot)$'\n","  plotArgDict['yAxisLims']          = (1, 1e4)\n","  plotArgDict['yLabel']             = r'Counts'\n","  plotArgDict['title']              = r''\n","  plotArgDict['nBins']              = 100\n","  plotArgDict['logY']               = True\n","  plotArgDict['CMAP']               = SIGNAL_COLOR_ARR\n","  plotArgDict['sigObjectNameList']  = [r'$A$', r'$h^0$', r'$h^\\pm$', r'$LQ$']\n","  \"\"\"\n","\n","  #-- Preliminary Figure Setup --#\n","  pltDim = plotArgDict['pltDim']\n","  fig_size = (3*pltDim[1], 3*pltDim[0])\n","\n","  fig = plt.figure(constrained_layout=False, figsize=fig_size)\n","\n","  gs = GridSpec(pltDim[0], pltDim[1], figure=fig, hspace=0.1)\n","  ax = fig.add_subplot(gs[:, :])\n","\n","  xmin, xmax = plotArgDict['xAxisLims']\n","  ax.set_xlim(xmin, xmax)\n","  ax.set(xlabel=plotArgDict['xLabel'])\n","  ax.xaxis.label.set_size(16)\n","\n","  ymin, ymax = plotArgDict['yAxisLims']\n","  ax.set_ylim(ymin, ymax)\n","  ax.set(ylabel=plotArgDict['yLabel'])\n","  ax.yaxis.label.set_size(16)\n","\n","  ax.set_title(plotArgDict['title'], fontsize=20)\n","\n","  if 'density' in  list(plotArgDict.keys()):\n","    DENSITY = plotArgDict['density']\n","  else:\n","    DENSITY = False\n","\n","  #-- Define color map --#\n","  # https://stackoverflow.com/questions/38208700/matplotlib-plot-lines-with-colors-through-colormap\n","  nCases    = len(scoreSigList)  # Number of cases to plot\n","  if isinstance(plotArgDict['CMAP'], np.ndarray) and (plotArgDict['CMAP'].shape[0]==nCases):\n","    colorList = plotArgDict['CMAP']\n","  else:\n","    CMAP      = plotArgDict['CMAP']\n","    colorList = CMAP(np.linspace(0,1,nCases))\n","\n","  #-- Make histogram plot --#\n","  binsArr = np.linspace(xmin, xmax, plotArgDict['nBins'])\n","  _, _, _   = ax.hist(scoreBkg, bins=binsArr, histtype = 'step', edgecolor='black', linestyle='-', linewidth=2, fill=False, log=True, label='Background', density=DENSITY)\n","\n","  for i in range(nCases):\n","    label = plotArgDict['sigObjectNameList'][i]\n","    _, _, _,   = ax.hist(scoreSigList[i], bins=binsArr, histtype = 'step', edgecolor=colorList[i], linestyle='--', linewidth=2, fill=False, log=True, label=label, density=DENSITY)\n","\n","  #-- Show the plot with legend --#\n","  ax.legend()\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"NGBfwbvcIyE6"},"source":["##### `plotMaxIndividualOTScoresPerEvent() `"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLxrE_n2I2H3"},"outputs":[],"source":["def plotMaxIndividualOTScoresPerEvent(indxs, maxArr, plotArgDict):\n","  \"\"\"\n","  Plot the (regulated) Significance Improvement (SI) curve from values\n","  precalculated using the calcROCmetrics() function.\n","\n","      x-axis is the Signal (Acceptance) Efficiency <=> TPR <=> \\eps_S\n","      y-axis is the (regulated) Significance Improvement <=> SI := TPR/sqrt(FPR + SIreg) <=> SI := eps_S/sqrt(eps_B + SIreg)\n","\n","  Inputs:\n","    tprList:      List of TPR arrays for each background to signal type pairing; Length=nCases\n","                  tprList[i] is an array of shape (Q,) with Q>2\n","    SIList:       List of SI arrays for each background to signal type pairing; Length=nCases\n","                  SIList[i] is an array of shape (Q,) with Q>2\n","    plotArgDict:  Dictionary of plotting arguments (see example below)\n","\n","\n","  Example plotArgDict:\n","  # Define colors to use for 4 signal types\n","      SIGNAL_COLOR_ARR = np.array([ RGBAtoRGBAtuple((231, 186, 81, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((140, 162, 82, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((165, 81, 148, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((214, 97, 107, 1), TYPE='list')\n","                                  ])\n","  # Or use a matplotlib color map\n","      SIGNAL_COLOR_ARR = plt.colormaps['rainbow'].reversed()\n","\n","  # Then define dictionary as follows\n","    plotArgDict = {}\n","    plotArgDict['pltDim']    = (3,3)\n","    plotArgDict['xAxisLims'] = (0, 1.05)\n","    plotArgDict['xLabel']    = r'Event Pairs'\n","    plotArgDict['yAxisLims'] = (0, 10)\n","    plotArgDict['yLabel']    = r'Max OT distance'\n","    plotArgDict['title']     = r'wBS_sig_A'\n","    plotArgDict['coreColorList'] = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n","  \"\"\"\n","  nEventPairs = maxArr.shape[0]\n","\n","  #-- Preliminary Figure Setup --#\n","  pltDim = plotArgDict['pltDim']\n","  fig_size = (3*pltDim[1], 3*pltDim[0])\n","\n","  fig = plt.figure(constrained_layout=False, figsize=fig_size)\n","\n","  gs = GridSpec(pltDim[0], pltDim[1], figure=fig, hspace=0.1)\n","  ax = fig.add_subplot(gs[:, :])\n","\n","  xmin, xmax = plotArgDict['xAxisLims']\n","  ax.set_xlim(xmin, xmax)\n","  ax.set(xlabel=plotArgDict['xLabel'])\n","  ax.xaxis.label.set_size(16)\n","\n","  ymin, ymax = plotArgDict['yAxisLims']\n","  ax.set_ylim(ymin, ymax)\n","  ax.set(ylabel=plotArgDict['yLabel'])\n","  ax.yaxis.label.set_size(16)\n","\n","  ax.set_title(plotArgDict['title'], fontsize=20)\n","\n","  #-- Get color list for plotting --#\n","  coreColorList = plotArgDict['coreColorList']\n","  colorList     = []\n","  for i in range(maxArr.shape[0]):\n","    indx = indxs[i]\n","    colorList.append(coreColorList[indx])\n","\n","  #-- Make plot --#\n","  eventPairsArr = np.arange(nEventPairs)\n","  ax.scatter(eventPairsArr, maxArr, color=colorList)\n","\n","  #-- Plot key --#\n","  deltaY = (ymax - ymin)\n","  deltaX = (xmax - xmin)\n","  ax.text(xmin + 0.20*deltaX, ymin + 0.95*deltaY, r'IOT$_{\\rm MET}$', color=coreColorList[0], fontsize=16, fontweight='bold')\n","  ax.text(xmin + 0.40*deltaX, ymin + 0.95*deltaY,   r'IOT$_{\\rm e}$', color=coreColorList[1], fontsize=16, fontweight='bold')\n","  ax.text(xmin + 0.60*deltaX, ymin + 0.95*deltaY, r'IOT$_{\\rm \\mu}$', color=coreColorList[2], fontsize=16, fontweight='bold')\n","  ax.text(xmin + 0.80*deltaX, ymin + 0.95*deltaY, r'IOT$_{\\rm jet}$', color=coreColorList[3], fontsize=16, fontweight='bold')\n","\n","  #-- Show plot --#\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"xeRSnwilrcyr"},"source":["## Performance metric plotting functions"]},{"cell_type":"markdown","metadata":{"id":"L5H1DaJ_rcys"},"source":["### Repeat runs"]},{"cell_type":"markdown","metadata":{"id":"ZuV-MooHrcys"},"source":["##### `plotROCcurve_errorBand() `"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P2bUwVX0rcys"},"outputs":[],"source":["def plotROCcurve_errorBand(av_fprList, std_fprList, plotArgDict, TYPE='Classic'):\n","  \"\"\"\n","  Plot ROC curves with error bands from values precalculated using the calcROCmetrics(..., INTERPOLATE=True) function.\n","\n","  Inputs:\n","    av_fprList:   List of average FPR arrays for each background to signal type pairing; Length=nCases\n","                  av_fprList[i] is an array of shape (Q,) with Q>2\n","    std_fprList:  List of std FPR arrays for each background to signal type pairing; Length=nCases\n","                  std_fprList[i] is an array of shape (Q,) with Q>2\n","    plotArgDict:  Dictionary of plotting arguments (see example below)\n","    TYPE:         TYPE of ROC curve to plot; choices are 'Classic' (default) or 'Modern'\n","                  (see below for explanation)\n","\n","\n","  Explanation of TYPE choices:\n","    For all TYPE choices the x-axis is defined as the Signal (Acceptance)\n","    Efficiency <=> TPR <=> \\eps_S\n","\n","    if TYPE == 'Classic':\n","        y-axis is the Background Rejection Efficiency <=> TNR <=> 1 - FPR <=> 1 - \\eps_B\n","    otherwise:\n","        y-axis is the Background (Acceptance) Efficiency <=> FPR <=> \\eps_B\n","\n","\n","  Example plotArgDict:\n","  # Define colors to use for 4 signal types\n","      SIGNAL_COLOR_ARR = np.array([ RGBAtoRGBAtuple((231, 186, 81, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((140, 162, 82, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((165, 81, 148, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((214, 97, 107, 1), TYPE='list')\n","                                  ])\n","  # Or use a matplotlib color map\n","      SIGNAL_COLOR_ARR = plt.colormaps['rainbow'].reversed()\n","\n","  # Then define dictionary as follows (assuming TYPE == 'Classic')\n","      plotArgDict = {}\n","      plotArgDict['pltDim']    = (3,3)\n","      plotArgDict['xAxisLims'] = (0, 1.05)\n","      plotArgDict['xLabel']    = r'Signal Efficiency (TPR)' # OR r'$\\eps_S$'\n","      plotArgDict['yAxisLims'] = (0, 1.05)\n","      plotArgDict['yLabel']    = r'Background Rejection (TNR)' # OR r'$1 - \\eps_B$'\n","      plotArgDict['title']     = r'ROC curve, $W_2^2(\\cdot, \\cdot)$ anomaly score'\n","      plotArgDict['CMAP']               = SIGNAL_COLOR_ARR\n","      plotArgDict['sigObjectNameList']  = [r'$A$', r'$h^0$', r'$h^\\pm$', r'$LQ$']\n","  \"\"\"\n","  #-- Set base TPR --#\n","  base_tpr = np.linspace(0, 1, 101)\n","\n","  #-- Preliminary Figure Setup --#\n","  pltDim = plotArgDict['pltDim']\n","  fig_size = (3*pltDim[1], 3*pltDim[0])\n","\n","  fig = plt.figure(constrained_layout=False, figsize=fig_size)\n","\n","  gs = GridSpec(pltDim[0], pltDim[1], figure=fig, hspace=0.1)\n","  ax = fig.add_subplot(gs[:, :])\n","\n","  xmin, xmax = plotArgDict['xAxisLims']\n","  ax.set_xlim(xmin, xmax)\n","  ax.set(xlabel=plotArgDict['xLabel'])\n","  ax.xaxis.label.set_size(16)\n","\n","  ymin, ymax = plotArgDict['yAxisLims']\n","  ax.set_ylim(ymin, ymax)\n","  ax.set(ylabel=plotArgDict['yLabel'])\n","  ax.yaxis.label.set_size(16)\n","\n","  ax.set_title(plotArgDict['title'], fontsize=20)\n","\n","  #-- Define color map --#\n","  # https://stackoverflow.com/questions/38208700/matplotlib-plot-lines-with-colors-through-colormap\n","  nCases    = len(av_fprList)  # Number of cases to plot\n","  if isinstance(plotArgDict['CMAP'], np.ndarray) and (plotArgDict['CMAP'].shape[0]==nCases):\n","    colorList = plotArgDict['CMAP']\n","  else:\n","    CMAP      = plotArgDict['CMAP']\n","    colorList = CMAP(np.linspace(0,1,nCases))\n","\n","  #-- Loop over signal cases --#\n","  for i in range(nCases):\n","\n","    #-- Get ROC curve components --#\n","    av_fpr, std_fpr, tpr = av_fprList[i], std_fprList[i], base_tpr\n","\n","    #-- Get appropriate shading color --#\n","    colorAlpha = list(colorList[i])\n","    colorAlpha[3] = 0.5\n","    colorAlpha = tuple(colorAlpha)\n","\n","    #-- Plot ROC curve --#\n","    legendLabel = plotArgDict['sigObjectNameList'][i]\n","    if TYPE == 'Classic':\n","      av_tnr = 1. - av_fpr\n","      _   = ax.plot(tpr, av_tnr, color=colorList[i], linestyle='-', linewidth=2, label=legendLabel)\n","      _   = ax.fill_between(tpr, av_tnr-std_fpr, av_tnr+std_fpr, color=colorAlpha) # Note: std_tnr = std_fpr\n","    else:\n","      _   = ax.plot(tpr, av_fpr, color=colorList[i], linestyle='-', linewidth=2, label=legendLabel)\n","      _   = ax.fill_between(tpr, av_fpr-std_fpr, av_fpr+std_fpr, color=colorAlpha)\n","\n","\n","  #-- Show the plot with legend --#\n","  if TYPE == 'Classic':\n","    ax.legend(loc='lower left')\n","  else:\n","    ax.legend(loc='upper left')\n","  plt.show()"]},{"cell_type":"markdown","source":["##### `plotROCcurve_errorBand_specificMethod() `"],"metadata":{"id":"uuFpRPoleR9q"}},{"cell_type":"code","source":["def plotROCcurve_errorBand_specificMethod(av_fprList, std_fprList, plotArgDict, TYPE='Classic', NSIGFIGS=4, saveFigPath=''):\n","  \"\"\"\n","  Plot ROC curves with error bands from values precalculated using the calcROCmetrics(..., INTERPOLATE=True) function.\n","  Instead of comparing one method on all signal types, we compare all methods on each signal type.\n","  Also adds ability to report AUC on plot.\n","\n","  Inputs:\n","    av_fprList:   List of average FPR arrays for each background to signal type pairing; Length=nCases\n","                  av_fprList[i] is an array of shape (Q,) with Q>2\n","    std_fprList:  List of std FPR arrays for each background to signal type pairing; Length=nCases\n","                  std_fprList[i] is an array of shape (Q,) with Q>2\n","    plotArgDict:  Dictionary of plotting arguments (see example below)\n","    TYPE:         TYPE of ROC curve to plot; choices are 'Classic' (default) or 'Modern'\n","                  (see below for explanation)\n","\n","\n","  Explanation of TYPE choices:\n","    For all TYPE choices the x-axis is defined as the Signal (Acceptance)\n","    Efficiency <=> TPR <=> \\eps_S\n","\n","    if TYPE == 'Classic':\n","        y-axis is the Background Rejection Efficiency <=> TNR <=> 1 - FPR <=> 1 - \\eps_B\n","    otherwise:\n","        y-axis is the Background (Acceptance) Efficiency <=> FPR <=> \\eps_B\n","\n","\n","  Example plotArgDict:\n","  # Define colors to use for 4 signal types\n","      METHOD_COLOR_ARR = np.array([ RGBAtoRGBAtuple((1, 27, 62, 1), TYPE='list'),\n","                              RGBAtoRGBAtuple((81, 39, 54, 1), TYPE='list'),\n","                              RGBAtoRGBAtuple((48, 53, 13, 1), TYPE='list'),\n","                              RGBAtoRGBAtuple((141, 109, 58, 1), TYPE='list')\n","                              ])\n","  # Or use a matplotlib color map\n","      METHOD_COLOR_ARR = plt.colormaps['rainbow'].reversed()\n","\n","  # Then define dictionary as follows (assuming TYPE == 'Classic')\n","      plotArgDict = {}\n","      plotArgDict['pltDim']    = (3,3)\n","      plotArgDict['xAxisLims'] = (0, 1.05)\n","      plotArgDict['xLabel']    = r'$\\epsilon_S$' # OR  r'Signal Efficiency (TPR)'\n","      plotArgDict['yAxisLims'] = (0, 1.05)\n","      plotArgDict['yLabel']    = r'$1 - \\epsilon_B$' # OR r'Background Rejection (TNR)'\n","      plotArgDict['title']     = r'Performance on signal case %s'%(sigName)\n","      plotArgDict['CMAP']            = METHOD_COLOR_ARR\n","\n","      plotArgDict['methodNameList']  = methodNameList\n","      plotArgDict['avAUCList']       = av_aucList\n","      plotArgDict['stdAUCList']      = std_aucList\n","      plotArgDict['delta_yposPerc'] = 0.04\n","      plotArgDict['xposPerc']       = 0.05\n","      plotArgDict['yposPerc']       = 0.2\n","  \"\"\"\n","  #-- Set base TPR --#\n","  base_tpr = np.linspace(0, 1, 101)\n","\n","  #-- Preliminary Figure Setup --#\n","  pltDim = plotArgDict['pltDim']\n","  fig_size = (3*pltDim[1], 3*pltDim[0])\n","\n","  fig = plt.figure(constrained_layout=False, figsize=fig_size)\n","\n","  gs = GridSpec(pltDim[0], pltDim[1], figure=fig, hspace=0.1)\n","  ax = fig.add_subplot(gs[:, :])\n","\n","  xmin, xmax = plotArgDict['xAxisLims']\n","  ax.set_xlim(xmin, xmax)\n","  ax.set(xlabel=plotArgDict['xLabel'])\n","  ax.xaxis.label.set_size(16)\n","\n","  ymin, ymax = plotArgDict['yAxisLims']\n","  ax.set_ylim(ymin, ymax)\n","  ax.set(ylabel=plotArgDict['yLabel'])\n","  ax.yaxis.label.set_size(16)\n","\n","  #-- Define color map --#\n","  # https://stackoverflow.com/questions/38208700/matplotlib-plot-lines-with-colors-through-colormap\n","  nCases    = len(av_fprList)  # Number of cases to plot\n","  if isinstance(plotArgDict['CMAP'], np.ndarray) and (plotArgDict['CMAP'].shape[0]==nCases):\n","    colorList = plotArgDict['CMAP']\n","  else:\n","    CMAP      = plotArgDict['CMAP']\n","    colorList = CMAP(np.linspace(0,1,nCases))\n","\n","\n","  #-- Preliminary alternate legend setting --#\n","  # Set initial position of text positions\n","  delta_ypos = plotArgDict['delta_yposPerc']*plotArgDict['yAxisLims'][1]\n","  if TYPE == 'Classic':\n","    xpos = plotArgDict['xposPerc']*plotArgDict['xAxisLims'][1]\n","    ypos = plotArgDict['yposPerc']*plotArgDict['yAxisLims'][1]\n","    ax.text(xpos, ypos, plotArgDict['title'], size=14, weight=\"bold\")\n","    ypos -= delta_ypos\n","\n","  #-- Loop over signal cases --#\n","  for i in range(nCases):\n","\n","    #-- Get ROC curve components --#\n","    av_fpr, std_fpr, tpr = av_fprList[i], std_fprList[i], base_tpr\n","\n","    #-- Get appropriate shading color --#\n","    colorAlpha = list(colorList[i])\n","    colorAlpha[3] = 0.5\n","    colorAlpha = tuple(colorAlpha)\n","\n","    #-- Plot ROC curve --#\n","    aucMean = roundToSigFig(plotArgDict['avAUCList'][i], NSIGFIGS)\n","    aucStd  = roundToSigFig(plotArgDict['stdAUCList'][i], NSIGFIGS)\n","    legendLabel = plotArgDict['methodNameList'][i] + r', AUC: %s $\\pm$ %s'%(aucMean, aucStd)\n","\n","    if plotArgDict['methodNameList'][i] != '3D OT+One-class SVM':\n","\n","      if TYPE == 'Classic':\n","        av_tnr = 1. - av_fpr\n","        _   = ax.plot(tpr, av_tnr, color=colorList[i], linestyle='-', linewidth=2, label=legendLabel)\n","        _   = ax.fill_between(tpr, av_tnr-std_fpr, av_tnr+std_fpr, color=colorAlpha) # Note: std_tnr = std_fpr\n","      else:\n","        _   = ax.plot(tpr, av_fpr, color=colorList[i], linestyle='-', linewidth=2, label=legendLabel)\n","        _   = ax.fill_between(tpr, av_fpr-std_fpr, av_fpr+std_fpr, color=colorAlpha)\n","\n","    #-- Alternate legend --#\n","    ax.text(xpos, ypos, legendLabel, size=14, color=colorList[i])\n","    ypos -= delta_ypos\n","\n","  #-- Save figure if desired --#\n","  if saveFigPath!='':\n","    print(\"Saving figure to file: \", saveFigPath)\n","    plt.savefig(saveFigPath)\n","\n","  plt.show()\n"],"metadata":{"id":"BhiOokLleR9q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B1eZspIercys"},"source":["##### `plotInvROCcurve_errorBand() `"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BXqNqC3Mrcys"},"outputs":[],"source":["def plotInvROCcurve_errorBand(av_fprInvList, std_fprInvList, plotArgDict, minTPR=0.05):\n","  \"\"\"\n","  Plot FPR inverse curves with error bands from values precalculated using the calcROCmetrics(..., INTERPOLATE=True) function.\n","\n","    x-axis is the Signal (Acceptance) Efficiency <=> TPR <=> \\eps_S\n","    y-axis is the inverted the Background (Acceptance) Efficiency <=> FPR <=> \\eps_B\n","\n","  NOTE: This is often also called a \"ROC curve\" however this is misleading because\n","  the AUC is NOT the area under this curve, so it is NOT a ROC curve.\n","\n","  Inputs:\n","    av_fprList:   List of average FPR arrays for each background to signal type pairing; Length=nCases\n","                  av_fprList[i] is an array of shape (Q,) with Q>2\n","    std_fprList:  List of std FPR arrays for each background to signal type pairing; Length=nCases\n","                  std_fprList[i] is an array of shape (Q,) with Q>2\n","    plotArgDict:  Dictionary of plotting arguments (see example below)\n","    minTPR:       Minimum TPR value to avoid wonky-ness from low statistics; must be between 0. and 1.\n","\n","\n","  Example plotArgDict:\n","  # Define colors to use for 4 signal types\n","      SIGNAL_COLOR_ARR = np.array([ RGBAtoRGBAtuple((231, 186, 81, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((140, 162, 82, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((165, 81, 148, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((214, 97, 107, 1), TYPE='list')\n","                                  ])\n","  # Or use a matplotlib color map\n","      SIGNAL_COLOR_ARR = plt.colormaps['rainbow'].reversed()\n","\n","  # Then define dictionary as follows\n","      plotArgDict = {}\n","      plotArgDict['pltDim']    = (3,3)\n","      plotArgDict['xAxisLims'] = (0, 1.05)\n","      plotArgDict['xLabel']    = r'$\\epsilon_S$ (TPR)'\n","      plotArgDict['yAxisLims'] = (1, 1e4)\n","      plotArgDict['yLabel']    = r'$\\epsilon_B^{-1}$ (FPR$^{-1}$)'\n","      plotArgDict['title']     = r'$W_2^2(\\cdot, \\cdot)$ anomaly score performance'\n","      plotArgDict['CMAP']               = SIGNAL_COLOR_ARR\n","      plotArgDict['sigObjectNameList']  = [r'$A$', r'$h^0$', r'$h^\\pm$', r'$LQ$']\n","  \"\"\"\n","\n","  #-- Set base TPR --#\n","  base_tpr = np.linspace(0, 1, 101)\n","\n","  #-- Preliminary Figure Setup --#\n","  pltDim = plotArgDict['pltDim']\n","  fig_size = (3*pltDim[1], 3*pltDim[0])\n","\n","  fig = plt.figure(constrained_layout=False, figsize=fig_size)\n","\n","  gs = GridSpec(pltDim[0], pltDim[1], figure=fig, hspace=0.1)\n","  ax = fig.add_subplot(gs[:, :])\n","\n","  xmin, xmax = plotArgDict['xAxisLims']\n","  ax.set_xlim(xmin, xmax)\n","  ax.set(xlabel=plotArgDict['xLabel'])\n","  ax.xaxis.label.set_size(16)\n","\n","  ymin, ymax = plotArgDict['yAxisLims']\n","  ax.set_ylim(ymin, ymax)\n","  ax.set_yscale('log')\n","  ax.set(ylabel=plotArgDict['yLabel'])\n","  ax.yaxis.label.set_size(16)\n","\n","  ax.set_title(plotArgDict['title'], fontsize=20)\n","\n","  #-- Define color map --#\n","  nCases    = len(av_fprInvList)  # Number of cases to plot\n","  if isinstance(plotArgDict['CMAP'], np.ndarray) and (plotArgDict['CMAP'].shape[0]==nCases):\n","    colorList = plotArgDict['CMAP']\n","  else:\n","    CMAP      = plotArgDict['CMAP']\n","    colorList = CMAP(np.linspace(0,1,nCases)) # https://stackoverflow.com/questions/38208700/matplotlib-plot-lines-with-colors-through-colormap\n","\n","  #-- Loop over signal cases --#\n","  for i in range(nCases):\n","\n","    #-- Get inv ROC curve components --#\n","    av_fprInv, std_fprInv, tpr = av_fprInvList[i], std_fprInvList[i], base_tpr\n","\n","    #-- Trim inv ROC curve components to account for minTPR --#\n","    assert (minTPR >=0 and minTPR <=1)  # minTPR must be in range [0,1]\n","    minMask = minTPR <= tpr             # Sets TPR values less than minTPR to False\n","\n","    av_fprInv_trimmed   = av_fprInv[minMask]\n","    std_fprInv_trimmed  = std_fprInv[minMask]\n","    tpr_trimmed         = tpr[minMask]\n","\n","    #-- Get appropriate shading color --#\n","    colorAlpha = list(colorList[i])\n","    colorAlpha[3] = 0.5\n","    colorAlpha = tuple(colorAlpha)\n","\n","    #-- Plot inv ROC curve --#\n","    legendLabel = plotArgDict['sigObjectNameList'][i]\n","    _ = ax.plot(tpr_trimmed, av_fprInv_trimmed, color=colorList[i], linestyle='-', linewidth=2, label=legendLabel)\n","    _ = ax.fill_between(tpr_trimmed, av_fprInv_trimmed-std_fprInv_trimmed, av_fprInv_trimmed+std_fprInv_trimmed, color=colorAlpha)\n","\n","  #-- Show the plot with legend --#\n","  ax.legend(loc='upper right')\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"aYAyJYt-rcyt"},"source":["##### `plotSIcurve_errorBand() `"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6vzodiyRrcyt"},"outputs":[],"source":["def plotSIcurve_errorBand(av_SIList, std_SIList, plotArgDict, minTPR=0.05, NSIGFIGS=4):\n","  \"\"\"\n","   Plot the (regulated) Significance Improvement (SI) curve with error bands from values precalculated using the calcROCmetrics(..., INTERPOLATE=True) function.\n","\n","      x-axis is the Signal (Acceptance) Efficiency <=> TPR <=> \\eps_S\n","      y-axis is the (regulated) Significance Improvement <=> SI := TPR/sqrt(FPR + SIreg) <=> SI := eps_S/sqrt(eps_B + SIreg)\n","\n","  Inputs:\n","    av_SIList:    List of average SI arrays for each background to signal type pairing; Length=nCases\n","                  av_SIList[i] is an array of shape (Q,) with Q>2\n","    std_SIList:   List of std SI arrays for each background to signal type pairing; Length=nCases\n","                  std_SIList[i] is an array of shape (Q,) with Q>2\n","    plotArgDict:  Dictionary of plotting arguments (see example below)\n","    minTPR:       Minimum TPR value to avoid wonky-ness from low statistics; must be between 0. and 1.\n","    NSIGFIGS:     Number of significant figures to use when reporting max SI and corresponding TPR\n","\n","  Example plotArgDict:\n","  # Define colors to use for 4 signal types\n","      SIGNAL_COLOR_ARR = np.array([ RGBAtoRGBAtuple((231, 186, 81, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((140, 162, 82, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((165, 81, 148, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((214, 97, 107, 1), TYPE='list')\n","                                  ])\n","  # Or use a matplotlib color map\n","      SIGNAL_COLOR_ARR = plt.colormaps['rainbow'].reversed()\n","\n","  # Then define dictionary as follows\n","    plotArgDict = {}\n","    plotArgDict['pltDim']    = (3,3)\n","    plotArgDict['xAxisLims'] = (0, 1.05)\n","    plotArgDict['xLabel']    = r'$\\epsilon_S$ (TPR)'\n","    plotArgDict['yAxisLims'] = (0, 10)\n","    plotArgDict['yLabel']    = r'$\\epsilon_S/ \\sqrt{\\epsilon_B}$ (SI)'\n","    plotArgDict['title']     = r'SI Curve, $W_2^2(\\cdot, \\cdot)$ anomaly score'\n","    plotArgDict['CMAP']               = SIGNAL_COLOR_ARR\n","    plotArgDict['sigObjectNameList']  = [r'$A$', r'$h^0$', r'$h^\\pm$', r'$LQ$']\n","  \"\"\"\n","\n","  #-- Set base TPR --#\n","  base_tpr = np.linspace(0, 1, 101)\n","\n","  #-- Preliminary Figure Setup --#\n","  pltDim = plotArgDict['pltDim']\n","  fig_size = (3*pltDim[1], 3*pltDim[0])\n","\n","  fig = plt.figure(constrained_layout=False, figsize=fig_size)\n","\n","  gs = GridSpec(pltDim[0], pltDim[1], figure=fig, hspace=0.1)\n","  ax = fig.add_subplot(gs[:, :])\n","\n","  xmin, xmax = plotArgDict['xAxisLims']\n","  ax.set_xlim(xmin, xmax)\n","  ax.set(xlabel=plotArgDict['xLabel'])\n","  ax.xaxis.label.set_size(16)\n","\n","  ymin, ymax = plotArgDict['yAxisLims']\n","  ax.set_ylim(ymin, ymax)\n","  ax.set(ylabel=plotArgDict['yLabel'])\n","  ax.yaxis.label.set_size(16)\n","\n","  ax.set_title(plotArgDict['title'], fontsize=20)\n","\n","  #-- Define color map --#\n","  # https://stackoverflow.com/questions/38208700/matplotlib-plot-lines-with-colors-through-colormap\n","  nCases    = len(av_SIList)  # Number of cases to plot\n","  if isinstance(plotArgDict['CMAP'], np.ndarray) and (plotArgDict['CMAP'].shape[0]==nCases):\n","    colorList = plotArgDict['CMAP']\n","  else:\n","    CMAP      = plotArgDict['CMAP']\n","    colorList = CMAP(np.linspace(0,1,nCases))\n","\n","  #-- Loop over signal cases --#\n","  for i in range(nCases):\n","\n","    #-- Get SI curve components --#\n","    av_si, std_si, tpr = av_SIList[i], std_SIList[i], base_tpr\n","\n","    #-- Trim SI curve components to account for minTPR --#\n","    assert (minTPR >=0 and minTPR <=1)  # minTPR must be in range [0,1]\n","    minMask = minTPR <= tpr             # Sets TPR values less than minTPR to False\n","\n","    av_si_trimmed   = av_si[minMask]\n","    std_si_trimmed  = std_si[minMask]\n","    tpr_trimmed     = tpr[minMask]\n","\n","    #-- Get appropriate shading color --#\n","    colorAlpha = list(colorList[i])\n","    colorAlpha[3] = 0.5\n","    colorAlpha = tuple(colorAlpha)\n","\n","    #-- Plot SI curve --#\n","    legendLabel = plotArgDict['sigObjectNameList'][i]\n","    _ = ax.plot(tpr_trimmed, av_si_trimmed, color=colorList[i], linestyle='-', linewidth=2, label=legendLabel)\n","    _ = ax.fill_between(tpr_trimmed, av_si_trimmed-std_si_trimmed, av_si_trimmed+std_si_trimmed, color=colorAlpha)\n","\n","    # Report max in trimmed TPR range\n","    aMaxSI = np.argmax(av_si_trimmed)\n","    print(\"Max SI for %s is %s \\pm %s at TPR = %s\"%(legendLabel, roundToSigFig(av_si_trimmed[aMaxSI], NSIGFIGS), roundToSigFig(std_si_trimmed[aMaxSI], NSIGFIGS), roundToSigFig(tpr_trimmed[aMaxSI], NSIGFIGS)))\n","\n","  #-- Show the plot with legend --#\n","  if 'nLegendColumns' in plotArgDict.keys():\n","    ax.legend(loc='upper right', ncol=plotArgDict['nLegendColumns'])\n","  else:\n","    ax.legend(loc='upper right')\n","  plt.show()"]},{"cell_type":"markdown","source":["##### `plotSIcurve_errorBand_specificMethod() `"],"metadata":{"id":"a9ZZDwPHL8-q"}},{"cell_type":"code","source":["def plotSIcurve_errorBand_specificMethod(av_SIList, std_SIList, plotArgDict, minTPR=0.05, NSIGFIGS=4, saveFigPath=''):\n","  \"\"\"\n","  Plot the (regulated) Significance Improvement (SI) curve with error bands from values precalculated using the calcROCmetrics(..., INTERPOLATE=True)\n","  function. Instead of comparing one method on all signal types, we compare all methods on each signal type.\n","  Also adds ability to report max SI (and corresponding TPR) on plot.\n","\n","      x-axis is the Signal (Acceptance) Efficiency <=> TPR <=> \\eps_S\n","      y-axis is the (regulated) Significance Improvement <=> SI := TPR/sqrt(FPR + SIreg) <=> SI := eps_S/sqrt(eps_B + SIreg)\n","\n","\n","  Inputs:\n","    av_SIList:    List of average SI arrays for each background to signal type pairing; Length=nCases\n","                  av_SIList[i] is an array of shape (Q,) with Q>2\n","    std_SIList:   List of std SI arrays for each background to signal type pairing; Length=nCases\n","                  std_SIList[i] is an array of shape (Q,) with Q>2\n","    plotArgDict:  Dictionary of plotting arguments (see example below)\n","    minTPR:       Minimum TPR value to avoid wonky-ness from low statistics; must be between 0. and 1.\n","    NSIGFIGS:     Number of significant figures to use when reporting max SI and corresponding TPR\n","    saveFigPath:  Path to save figure; default is to not save figure\n","\n","  Example plotArgDict:\n","  # Define colors to use for 4 signal types\n","      SIGNAL_COLOR_ARR = np.array([ RGBAtoRGBAtuple((231, 186, 81, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((140, 162, 82, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((165, 81, 148, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((214, 97, 107, 1), TYPE='list')\n","                                  ])\n","  # Or use a matplotlib color map\n","      SIGNAL_COLOR_ARR = plt.colormaps['rainbow'].reversed()\n","\n","  # Then define dictionary as follows\n","    plotArgDict = {}\n","    plotArgDict['pltDim']    = (3,3)\n","    plotArgDict['xAxisLims'] = (0, 1.05)\n","    plotArgDict['xLabel']    = r'$\\epsilon_S$ (TPR)'\n","    plotArgDict['yAxisLims'] = (0, 10)\n","    plotArgDict['yLabel']    = r'$\\epsilon_S/ \\sqrt{\\epsilon_B}$ (SI)'\n","    plotArgDict['title']     = r'SI Curve, $W_2^2(\\cdot, \\cdot)$ anomaly score'\n","    plotArgDict['CMAP']               = SIGNAL_COLOR_ARR\n","    plotArgDict['sigObjectNameList']  = [r'$A$', r'$h^0$', r'$h^\\pm$', r'$LQ$']\n","\n","    plotArgDict['methodNameList']  = methodNameList\n","    plotArgDict['avMaxSIList']     = av_maxSIList\n","    plotArgDict['stdMaxSIList']    = std_maxSIList\n","    plotArgDict['corrTPRList']     = corr_tprList\n","    plotArgDict['delta_yposPerc'] = 0.04\n","    plotArgDict['xposPerc']       = 0.05\n","    plotArgDict['yposPerc']       = 0.2\n","  \"\"\"\n","\n","  #-- Set base TPR --#\n","  base_tpr = np.linspace(0, 1, 101)\n","\n","  #-- Preliminary Figure Setup --#\n","  pltDim = plotArgDict['pltDim']\n","  fig_size = (3*pltDim[1], 3*pltDim[0])\n","\n","  fig = plt.figure(constrained_layout=False, figsize=fig_size)\n","\n","  gs = GridSpec(pltDim[0], pltDim[1], figure=fig, hspace=0.1)\n","  ax = fig.add_subplot(gs[:, :])\n","\n","  xmin, xmax = plotArgDict['xAxisLims']\n","  ax.set_xlim(xmin, xmax)\n","  ax.set(xlabel=plotArgDict['xLabel'])\n","  ax.xaxis.label.set_size(16)\n","\n","  ymin, ymax = plotArgDict['yAxisLims']\n","  ax.set_ylim(ymin, ymax)\n","  ax.set(ylabel=plotArgDict['yLabel'])\n","  ax.yaxis.label.set_size(16)\n","\n","  #-- Define color map --#\n","  # https://stackoverflow.com/questions/38208700/matplotlib-plot-lines-with-colors-through-colormap\n","  nCases    = len(av_SIList)  # Number of cases to plot\n","  if isinstance(plotArgDict['CMAP'], np.ndarray) and (plotArgDict['CMAP'].shape[0]==nCases):\n","    colorList = plotArgDict['CMAP']\n","  else:\n","    CMAP      = plotArgDict['CMAP']\n","    colorList = CMAP(np.linspace(0,1,nCases))\n","\n","  #-- Preliminary alternate legend setting --#\n","  # Set initial position of text positions\n","  delta_ypos = plotArgDict['delta_yposPerc']*plotArgDict['yAxisLims'][1]\n","  xpos = plotArgDict['xposPerc']*plotArgDict['xAxisLims'][1]\n","  ypos = plotArgDict['yposPerc']*plotArgDict['yAxisLims'][1]\n","  ax.text(xpos, ypos, plotArgDict['title'], size=14, weight=\"bold\")\n","  ypos -= delta_ypos\n","\n","  #-- Loop over signal cases --#\n","  for i in range(nCases):\n","\n","    #-- Get SI curve components --#\n","    av_si, std_si, tpr = av_SIList[i], std_SIList[i], base_tpr\n","\n","    #-- Trim SI curve components to account for minTPR --#\n","    assert (minTPR >=0 and minTPR <=1)  # minTPR must be in range [0,1]\n","    minMask = minTPR <= tpr             # Sets TPR values less than minTPR to False\n","\n","    av_si_trimmed   = av_si[minMask]\n","    std_si_trimmed  = std_si[minMask]\n","    tpr_trimmed     = tpr[minMask]\n","\n","    #-- Get appropriate shading color --#\n","    colorAlpha = list(colorList[i])\n","    colorAlpha[3] = 0.5\n","    colorAlpha = tuple(colorAlpha)\n","\n","    #-- Plot SI curve --#\n","    maxSImean = roundToSigFig(plotArgDict['avMaxSIList'][i], NSIGFIGS)\n","    maxSIstd  = roundToSigFig(plotArgDict['stdMaxSIList'][i], NSIGFIGS)\n","    corrTPR   = roundToSigFig(plotArgDict['corrTPRList'][i], 2)\n","    legendLabel = plotArgDict['methodNameList'][i] + r', max SI: %s $\\pm$ %s ($\\epsilon_s$=%s)'%(maxSImean, maxSIstd, corrTPR)\n","\n","    if plotArgDict['methodNameList'][i] != '3D OT+One-class SVM': #!\n","\n","      _ = ax.plot(tpr_trimmed, av_si_trimmed, color=colorList[i], linestyle='-', linewidth=2, label=legendLabel)\n","      _ = ax.fill_between(tpr_trimmed, av_si_trimmed-std_si_trimmed, av_si_trimmed+std_si_trimmed, color=colorAlpha)\n","\n","    #-- Alternate legend --#\n","    ax.text(xpos, ypos, legendLabel, size=14, color=colorList[i])\n","    ypos -= delta_ypos\n","\n","  #-- Save figure if desired --#\n","  if saveFigPath!='':\n","    print(\"Saving figure to file: \", saveFigPath)\n","    plt.savefig(saveFigPath)\n","\n","  plt.show()"],"metadata":{"id":"rshA4KQS2liO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nYZz638jrcyt"},"source":["### Single test run"]},{"cell_type":"markdown","metadata":{"id":"aqI3UrC4rcyt"},"source":["##### `plotROCcurve() `"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vsu7SI6Xrcyt"},"outputs":[],"source":["def plotROCcurve(aucList, fprList, tprList, plotArgDict, TYPE='Classic'):\n","  \"\"\"\n","  Plot ROC curves from values precalculated using the calcROCmetrics() function.\n","\n","  Inputs:\n","    aucList:      List of AUC scores for each background to signal type pairing; Length=nCases\n","    fprList:      List of FPR arrays for each background to signal type pairing; Length=nCases\n","                  fprList[i] is an array of shape (Q,) with Q>2\n","    tprList:      List of TPR arrays for each background to signal type pairing; Length=nCases\n","                  tprList[i] is an array of shape (Q,) with Q>2\n","    plotArgDict:  Dictionary of plotting arguments (see example below)\n","    TYPE:         TYPE of ROC curve to plot; choices are 'Classic' (default) or 'Modern'\n","                  (see below for explanation)\n","\n","\n","  Explanation of TYPE choices:\n","    For all TYPE choices the x-axis is defined as the Signal (Acceptance)\n","    Efficiency <=> TPR <=> \\eps_S\n","\n","    if TYPE == 'Classic':\n","        y-axis is the Background Rejection Efficiency <=> TNR <=> 1 - FPR <=> 1 - \\eps_B\n","    otherwise:\n","        y-axis is the Background (Acceptance) Efficiency <=> FPR <=> \\eps_B\n","\n","\n","  Example plotArgDict:\n","  # Define colors to use for 4 signal types\n","      SIGNAL_COLOR_ARR = np.array([ RGBAtoRGBAtuple((231, 186, 81, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((140, 162, 82, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((165, 81, 148, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((214, 97, 107, 1), TYPE='list')\n","                                  ])\n","  # Or use a matplotlib color map\n","      SIGNAL_COLOR_ARR = plt.colormaps['rainbow'].reversed()\n","\n","  # Then define dictionary as follows (assuming TYPE == 'Classic')\n","      plotArgDict = {}\n","      plotArgDict['pltDim']    = (3,3)\n","      plotArgDict['xAxisLims'] = (0, 1.05)\n","      plotArgDict['xLabel']    = r'Signal Efficiency (TPR)' # OR r'$\\eps_S$'\n","      plotArgDict['yAxisLims'] = (0, 1.05)\n","      plotArgDict['yLabel']    = r'Background Rejection (TNR)' # OR r'$1 - \\eps_B$'\n","      plotArgDict['title']     = r'ROC curve, $W_2^2(\\cdot, \\cdot)$ anomaly score'\n","      plotArgDict['CMAP']               = SIGNAL_COLOR_ARR\n","      plotArgDict['sigObjectNameList']  = [r'$A$', r'$h^0$', r'$h^\\pm$', r'$LQ$']\n","  \"\"\"\n","\n","  #-- Preliminary Figure Setup --#\n","  pltDim = plotArgDict['pltDim']\n","  fig_size = (3*pltDim[1], 3*pltDim[0])\n","\n","  fig = plt.figure(constrained_layout=False, figsize=fig_size)\n","\n","  gs = GridSpec(pltDim[0], pltDim[1], figure=fig, hspace=0.1)\n","  ax = fig.add_subplot(gs[:, :])\n","\n","  xmin, xmax = plotArgDict['xAxisLims']\n","  ax.set_xlim(xmin, xmax)\n","  ax.set(xlabel=plotArgDict['xLabel'])\n","  ax.xaxis.label.set_size(16)\n","\n","  ymin, ymax = plotArgDict['yAxisLims']\n","  ax.set_ylim(ymin, ymax)\n","  ax.set(ylabel=plotArgDict['yLabel'])\n","  ax.yaxis.label.set_size(16)\n","\n","  ax.set_title(plotArgDict['title'], fontsize=20)\n","\n","  #-- Define color map --#\n","  # https://stackoverflow.com/questions/38208700/matplotlib-plot-lines-with-colors-through-colormap\n","  nCases    = len(aucList)  # Number of cases to plot\n","  if isinstance(plotArgDict['CMAP'], np.ndarray) and (plotArgDict['CMAP'].shape[0]==nCases):\n","    colorList = plotArgDict['CMAP']\n","  else:\n","    CMAP      = plotArgDict['CMAP']\n","    colorList = CMAP(np.linspace(0,1,nCases))\n","\n","  #-- Loop over signal cases --#\n","  for i in range(nCases):\n","\n","    #-- Get ROC curve components --#\n","    auc, fpr, tpr = aucList[i], fprList[i], tprList[i]\n","\n","    #-- Plot ROC curve --#\n","    legendLabel = plotArgDict['sigObjectNameList'][i]+': AUC='+str(auc)\n","    if TYPE == 'Classic':\n","      tnr = 1. - fpr\n","      _   = ax.plot(tpr, tnr, color=colorList[i], linestyle='-', linewidth=2, label=legendLabel)\n","    else:\n","      _   = ax.plot(tpr, fpr, color=colorList[i], linestyle='-', linewidth=2, label=legendLabel)\n","\n","  #-- Show the plot with legend --#\n","  if TYPE == 'Classic':\n","    ax.legend(loc='lower left')\n","  else:\n","    ax.legend(loc='upper left')\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"K8rTZTVYrcyu"},"source":["##### `plotInvROCcurve() `"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZQeMtzorcyu"},"outputs":[],"source":["def plotInvROCcurve(aucList, fprInvList, tprList, plotArgDict):\n","  \"\"\"\n","  Plot FPR inverse curves from values precalculated using the calcROCmetrics() function.\n","\n","    x-axis is the Signal (Acceptance) Efficiency <=> TPR <=> \\eps_S\n","    y-axis is the inverted the Background (Acceptance) Efficiency <=> FPR <=> \\eps_B\n","\n","  NOTE: This is often also called a \"ROC curve\" however this is misleading because\n","  the AUC is NOT the area under this curve, so it is NOT a ROC curve.\n","\n","  Inputs:\n","    aucList:      List of AUC scores for each background to signal type pairing; Length=nCases\n","    fprInvList:   List of inverse FPR arrays for each background to signal type pairing; Length=nCases\n","                  fprInvList[i] is a masked array of shape (Q,) with Q>2 with division by zero cases masked\n","    tprList:      List of TPR arrays for each background to signal type pairing; Length=nCases\n","                  tprList[i] is an array of shape (Q,) with Q>2\n","    plotArgDict:  Dictionary of plotting arguments (see example below)\n","\n","\n","  Example plotArgDict:\n","  # Define colors to use for 4 signal types\n","      SIGNAL_COLOR_ARR = np.array([ RGBAtoRGBAtuple((231, 186, 81, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((140, 162, 82, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((165, 81, 148, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((214, 97, 107, 1), TYPE='list')\n","                                  ])\n","  # Or use a matplotlib color map\n","      SIGNAL_COLOR_ARR = plt.colormaps['rainbow'].reversed()\n","\n","  # Then define dictionary as follows\n","      plotArgDict = {}\n","      plotArgDict['pltDim']    = (3,3)\n","      plotArgDict['xAxisLims'] = (0, 1.05)\n","      plotArgDict['xLabel']    = r'$\\epsilon_S$ (TPR)'\n","      plotArgDict['yAxisLims'] = (1, 1e4)\n","      plotArgDict['yLabel']    = r'$\\epsilon_B^{-1}$ (FPR$^{-1}$)'\n","      plotArgDict['title']     = r'$W_2^2(\\cdot, \\cdot)$ anomaly score performance'\n","      plotArgDict['CMAP']               = SIGNAL_COLOR_ARR\n","      plotArgDict['sigObjectNameList']  = [r'$A$', r'$h^0$', r'$h^\\pm$', r'$LQ$']\n","  \"\"\"\n","\n","  #-- Preliminary Figure Setup --#\n","  pltDim = plotArgDict['pltDim']\n","  fig_size = (3*pltDim[1], 3*pltDim[0])\n","\n","  fig = plt.figure(constrained_layout=False, figsize=fig_size)\n","\n","  gs = GridSpec(pltDim[0], pltDim[1], figure=fig, hspace=0.1)\n","  ax = fig.add_subplot(gs[:, :])\n","\n","  xmin, xmax = plotArgDict['xAxisLims']\n","  ax.set_xlim(xmin, xmax)\n","  ax.set(xlabel=plotArgDict['xLabel'])\n","  ax.xaxis.label.set_size(16)\n","\n","  ymin, ymax = plotArgDict['yAxisLims']\n","  ax.set_ylim(ymin, ymax)\n","  ax.set_yscale('log')\n","  ax.set(ylabel=plotArgDict['yLabel'])\n","  ax.yaxis.label.set_size(16)\n","\n","  ax.set_title(plotArgDict['title'], fontsize=20)\n","\n","  #-- Define color map --#\n","  nCases    = len(aucList)  # Number of cases to plot\n","  if isinstance(plotArgDict['CMAP'], np.ndarray) and (plotArgDict['CMAP'].shape[0]==nCases):\n","    colorList = plotArgDict['CMAP']\n","  else:\n","    CMAP      = plotArgDict['CMAP']\n","    colorList = CMAP(np.linspace(0,1,nCases)) # https://stackoverflow.com/questions/38208700/matplotlib-plot-lines-with-colors-through-colormap\n","\n","  #-- Loop over signal cases --#\n","  for i in range(nCases):\n","\n","    #-- Get ROC curve components --#\n","    auc, fprInv, tpr = aucList[i], fprInvList[i], tprList[i]\n","\n","    #-- Plot ROC curve --#\n","    legendLabel = plotArgDict['sigObjectNameList'][i]+': AUC='+str(auc)\n","    _ = ax.plot(tpr, fprInv, color=colorList[i], linestyle='-', linewidth=2, label=legendLabel)\n","\n","  #-- Show the plot with legend --#\n","  ax.legend(loc='upper right')\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"OwYOgS5Ercyu"},"source":["##### `plotSIcurve() `"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q9UKEGKsrcyv"},"outputs":[],"source":["def plotSIcurve(tprList, SIList, plotArgDict):\n","  \"\"\"\n","   Plot the (regulated) Significance Improvement (SI) curve from values\n","   precalculated using the calcROCmetrics() function.\n","\n","      x-axis is the Signal (Acceptance) Efficiency <=> TPR <=> \\eps_S\n","      y-axis is the (regulated) Significance Improvement <=> SI := TPR/sqrt(FPR + SIreg) <=> SI := eps_S/sqrt(eps_B + SIreg)\n","\n","  Inputs:\n","    tprList:      List of TPR arrays for each background to signal type pairing; Length=nCases\n","                  tprList[i] is an array of shape (Q,) with Q>2\n","    SIList:       List of SI arrays for each background to signal type pairing; Length=nCases\n","                  SIList[i] is an array of shape (Q,) with Q>2\n","    plotArgDict:  Dictionary of plotting arguments (see example below)\n","\n","\n","  Example plotArgDict:\n","  # Define colors to use for 4 signal types\n","      SIGNAL_COLOR_ARR = np.array([ RGBAtoRGBAtuple((231, 186, 81, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((140, 162, 82, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((165, 81, 148, 1), TYPE='list'),\n","                                    RGBAtoRGBAtuple((214, 97, 107, 1), TYPE='list')\n","                                  ])\n","  # Or use a matplotlib color map\n","      SIGNAL_COLOR_ARR = plt.colormaps['rainbow'].reversed()\n","\n","  # Then define dictionary as follows\n","    plotArgDict = {}\n","    plotArgDict['pltDim']    = (3,3)\n","    plotArgDict['xAxisLims'] = (0, 1.05)\n","    plotArgDict['xLabel']    = r'$\\epsilon_S$ (TPR)'\n","    plotArgDict['yAxisLims'] = (0, 10)\n","    plotArgDict['yLabel']    = r'$\\epsilon_S/ \\sqrt{\\epsilon_B}$ (SI)'\n","    plotArgDict['title']     = r'SI Curve, $W_2^2(\\cdot, \\cdot)$ anomaly score'\n","    plotArgDict['CMAP']               = SIGNAL_COLOR_ARR\n","    plotArgDict['sigObjectNameList']  = [r'$A$', r'$h^0$', r'$h^\\pm$', r'$LQ$']\n","  \"\"\"\n","\n","  #-- Preliminary Figure Setup --#\n","  pltDim = plotArgDict['pltDim']\n","  fig_size = (3*pltDim[1], 3*pltDim[0])\n","\n","  fig = plt.figure(constrained_layout=False, figsize=fig_size)\n","\n","  gs = GridSpec(pltDim[0], pltDim[1], figure=fig, hspace=0.1)\n","  ax = fig.add_subplot(gs[:, :])\n","\n","  xmin, xmax = plotArgDict['xAxisLims']\n","  ax.set_xlim(xmin, xmax)\n","  ax.set(xlabel=plotArgDict['xLabel'])\n","  ax.xaxis.label.set_size(16)\n","\n","  ymin, ymax = plotArgDict['yAxisLims']\n","  ax.set_ylim(ymin, ymax)\n","  ax.set(ylabel=plotArgDict['yLabel'])\n","  ax.yaxis.label.set_size(16)\n","\n","  ax.set_title(plotArgDict['title'], fontsize=20)\n","\n","  #-- Define color map --#\n","  # https://stackoverflow.com/questions/38208700/matplotlib-plot-lines-with-colors-through-colormap\n","  nCases    = len(tprList)  # Number of cases to plot\n","  if isinstance(plotArgDict['CMAP'], np.ndarray) and (plotArgDict['CMAP'].shape[0]==nCases):\n","    colorList = plotArgDict['CMAP']\n","  else:\n","    CMAP      = plotArgDict['CMAP']\n","    colorList = CMAP(np.linspace(0,1,nCases))\n","\n","  #-- Loop over signal cases --#\n","  for i in range(nCases):\n","\n","    #-- Get ROC curve components --#\n","    si, tpr = SIList[i], tprList[i]\n","\n","    #-- Plot ROC curve --#\n","    legendLabel = plotArgDict['sigObjectNameList'][i]\n","    _ = ax.plot(tpr, si, color=colorList[i], linestyle='-', linewidth=2, label=legendLabel)\n","\n","  #-- Show the plot with legend --#\n","  ax.legend(loc='upper right')\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"DU3JgR4r_5BF"},"source":["# Machine Learning Functions"]},{"cell_type":"markdown","metadata":{"id":"fwN6yBxGAAJ9"},"source":["## SVM Classification"]},{"cell_type":"markdown","source":["##### `SVM_ROC_Metrics()`"],"metadata":{"id":"5sTmS6wP94Wf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MoudF5IfAFCD"},"outputs":[],"source":["def SVM_ROC_Metrics(bkg_scores, sig_scores, C = 1.0, gamma = 'scale'):\n","  np.random.shuffle(bkg_scores)\n","  np.random.shuffle(sig_scores)\n","\n","  bkg_indicator = np.zeros(len(bkg_scores))\n","  sig_indicator = np.ones(len(sig_scores))\n","\n","  bkg_x_train, bkg_x_test, bkg_y_train, bkg_y_test = train_test_split(bkg_scores, bkg_indicator, test_size=0.25, random_state=123)\n","  bkg_x_train, bkg_x_test = np.array(bkg_x_train).reshape(-1, 1), np.array(bkg_x_test).reshape(-1, 1)\n","\n","  sig_x_train, sig_x_test, sig_y_train, sig_y_test = train_test_split(sig_scores, sig_indicator, test_size=0.25, random_state=123)\n","  sig_x_train, sig_x_test = np.array(sig_x_train).reshape(-1, 1), np.array(sig_x_test).reshape(-1, 1)\n","\n","  x_train = np.concatenate((bkg_x_train, sig_x_train), axis = 0)\n","  y_train = np.concatenate((bkg_y_train, sig_y_train))\n","\n","  x_test = np.concatenate((bkg_x_test, sig_x_test), axis = 0)\n","  y_test = np.concatenate((bkg_y_test, sig_y_test))\n","\n","  # x = np.concatenate((bkg_scores, sig_scores))\n","\n","  # y = [0] * len(bkg_scores) + [1] * len(sig_scores)\n","\n","  # x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=123)\n","  # x_train, x_test = np.array(x_train).reshape(-1, 1), np.array(x_test).reshape(-1, 1)\n","\n","  # Create the SVM with RBF kernel\n","  clf = SVC(kernel='rbf', C = C, gamma = gamma)\n","\n","  # Train the SVM\n","  clf.fit(x_train, y_train)\n","\n","  # Calibrate the SVC model\n","  calibrated_clf = CalibratedClassifierCV(clf, method='sigmoid', cv='prefit')\n","  calibrated_clf.fit(x_train, y_train)\n","\n","  # Get prediction probabilities for the test set\n","  y_prob = calibrated_clf.predict_proba(x_test)[:,1]\n","\n","  # # Get predicted probabilities for the positive class (1 - Signal)\n","  # y_prob = clf.predict_proba(x_test)[:, 1]\n","\n","  # Compute ROC curve and AUC\n","  fpr, tpr, _ = roc_curve(y_test, y_prob)\n","  roc_auc = roc_auc_score(y_test, y_prob)\n","  tnr = 1-fpr\n","\n","  return tnr,tpr,roc_auc"]},{"cell_type":"markdown","source":["##### `SVM_Classification_With_Best_Hyperparameters()`"],"metadata":{"id":"kJKp0OIm99qK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0ByrzEgFiIL"},"outputs":[],"source":["def SVM_Classification_With_Best_Hyperparameters(bkg_scores, sig_scores):\n","    C_range = [1,10]\n","    gamma_range = [1,10]\n","    auc_list = []\n","    tnr_list = []\n","    tpr_list = []\n","\n","    for C in C_range:\n","      for gamma in gamma_range:\n","        tnr,tpr,roc_auc = SVM_ROC_Metrics(bkg_scores, sig_scores, C, gamma)\n","        tnr_list.append(tnr)\n","        tpr_list.append(tpr)\n","        auc_list.append(roc_auc)\n","\n","    max_index = auc_list.index(max(auc_list))\n","\n","    return tnr_list[max_index], tpr_list[max_index], auc_list[max_index]"]},{"cell_type":"markdown","metadata":{"id":"bZEsW88eXKbG"},"source":["## kNN Classification"]},{"cell_type":"markdown","source":["##### `kNN_with_score_list()`"],"metadata":{"id":"o60sFCpg-FMB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IiFG6jUVXNZa"},"outputs":[],"source":["def kNN_with_score_list(bkg_score, sig_score, train_number, val_number, test_number, sig_bkg_ratio, neighbor_list):\n","  np.random.seed(123)\n","  np.random.shuffle(bkg_score)\n","  np.random.shuffle(sig_score)\n","\n","  bkg_indicator = np.zeros(len(bkg_score))\n","  sig_indicator = np.ones(len(sig_score))\n","\n","  x_train = np.concatenate((bkg_score[:train_number],sig_score[:train_number*sig_bkg_ratio]))\n","  x_train = x_train.reshape(-1,1)\n","  y_train = np.concatenate((bkg_indicator[:train_number],sig_indicator[:train_number*sig_bkg_ratio]))\n","\n","  x_val = np.concatenate((bkg_score[train_number:train_number+val_number],sig_score[train_number*sig_bkg_ratio:(train_number+val_number)*sig_bkg_ratio]))\n","  x_val = x_val.reshape(-1,1)\n","  y_val = np.concatenate((bkg_indicator[train_number:train_number+val_number],sig_indicator[train_number*sig_bkg_ratio:(train_number+val_number)*sig_bkg_ratio]))\n","\n","  x_trainval = np.concatenate((bkg_score[:train_number+val_number],sig_score[:(train_number+val_number)*sig_bkg_ratio]))\n","  x_trainval = x_trainval.reshape(-1,1)\n","  y_trainval = np.concatenate((bkg_indicator[:train_number+val_number],sig_indicator[:(train_number+val_number)*sig_bkg_ratio]))\n","\n","  x_test = np.concatenate((bkg_score[train_number+val_number:train_number+val_number+test_number],sig_score[(train_number+val_number)*sig_bkg_ratio:(train_number+val_number+test_number)*sig_bkg_ratio]))\n","  x_test = x_test.reshape(-1,1)\n","  y_test = np.concatenate((bkg_indicator[train_number+val_number:train_number+val_number+test_number],sig_indicator[(train_number+val_number)*sig_bkg_ratio:(train_number+val_number+test_number)*sig_bkg_ratio]))\n","\n","  # print(len(bkg_score),len(sig_score))\n","  # print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_trainval.shape, y_trainval.shape, x_test.shape, y_test.shape)\n","\n","  models, pred_vals = [], []\n","  for neighbor in neighbor_list:\n","    # print(\"Processing neighbor k=\", neighbor)\n","    model = KNeighborsClassifier(n_neighbors=neighbor)\n","    model.fit(x_train,y_train)\n","    pred_val = model.predict_proba(x_val)[:,1]\n","    models.append(model)\n","    pred_vals.append(pred_val)\n","\n","  auc_list = []\n","  for pred in pred_vals:\n","    auc = roc_auc_score(y_val, pred)\n","    auc_list.append(auc)\n","\n","  max_index = auc_list.index(max(auc_list))\n","  best_k = neighbor_list[max_index]\n","  # print(max(auc_list))\n","  # print(best_k)\n","  # print(auc_list)\n","\n","  best_model = KNeighborsClassifier(n_neighbors=best_k)\n","  best_model.fit(x_trainval, y_trainval)\n","  best_pred = best_model.predict_proba(x_test)[:,1]\n","  best_auc = roc_auc_score(y_test, best_pred)\n","\n","  return best_auc, best_k"]},{"cell_type":"markdown","source":["##### `kNN_with_distance_matrix()`"],"metadata":{"id":"icTZjyMA-Kcq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y0CDrjUnDFba"},"outputs":[],"source":["def kNN_with_distance_matrix(l_matrix, labels, train_number, val_number, test_number, neighbor_list, AUC_list = False):\n","    '''\n","    Inputs:\n","\n","        l_matrix:          The distance matrix(2D np array) of the two types of jets. Shape:(njets, njets)\n","                          (NOTE: This matrix must be symmetric and tracless for it to be a meaningful OT distance matrix)\n","\n","        labels:           An array of labels for which type of particle each row/column belongs to. Shape(njets,)\n","\n","        train_number:     The number of jets used for training the kNN model, set to be the first \"train_number\" of rows/cols\n","\n","        val_number:       The number of jets used for the validation process(determining the best number of neighbors), set to be the \"val_number\" of rows/cols right after the training rows/cols\n","\n","        test_number:      The number of jets used for the testing process, set to be the \"test_number\" of rows/cols right after the validation rows/cols\n","\n","        neighbor_list:   The list of neighbors wanted to be used for the kNN classification\n","\n","        AUC_list:         Default is False. When set to True, return the whole list of AUCs during the validation process\n","\n","    Outputs:\n","\n","        best_auc:         The AUC computed using the best-k-neighbor returned by the validation process.\n","\n","        auc_list:         The entire list of AUC computed during the validation process. Only returned if AUC_list is set to be True\n","\n","    '''\n","\n","    # Split the distance matrix and labels into training, validation and testing sets\n","    l_matrix_train         = l_matrix[:train_number, :train_number]\n","    l_matrix_val_train     = l_matrix[train_number:train_number+val_number, :train_number]\n","    l_matrix_trainval      = l_matrix[:train_number+val_number, :train_number+val_number]\n","    l_matrix_test_trainval = l_matrix[train_number+val_number:test_number + train_number + val_number, :train_number+val_number]\n","\n","    # Split the labels into training, validation and testing sets\n","    train_labels           = labels[:train_number]\n","    val_labels             = labels[train_number:train_number+val_number]\n","    trainval_labels        = labels[:train_number+val_number]\n","    test_labels            = labels[train_number+val_number:test_number+train_number+val_number]\n","\n","    models, pred_vals = [], []\n","\n","    for neighbor in tqdm(neighbor_list, desc='Fitting Models'):\n","        model = KNeighborsClassifier(n_neighbors=neighbor, metric='precomputed')\n","        model.fit(l_matrix_train, train_labels)\n","        pred_val = model.predict_proba(l_matrix_val_train)[:,1]\n","        models.append(model)\n","        pred_vals.append(pred_val)\n","\n","    auc_list = []\n","\n","    # Compute the AUC for each k-neighbor model\n","    for pred in pred_vals:\n","        auc = roc_auc_score(val_labels, pred)\n","        auc_list.append(auc)\n","\n","    max_index = auc_list.index(max(auc_list))\n","\n","    # Get the best k-neighbor model\n","    best_k_neighbor = neighbor_list[max_index]\n","\n","    best_model = KNeighborsClassifier(n_neighbors=best_k_neighbor, metric='precomputed')\n","\n","    # Train the best k-neighbor model\n","    best_model.fit(l_matrix_trainval, trainval_labels)\n","\n","    # Get the prediction probabilities for the testing set\n","    best_pred = best_model.predict_proba(l_matrix_test_trainval)[:,1]\n","\n","    kNN_metrics = kNN_ROC_metrics(test_labels, best_pred, Interpolate = True)\n","\n","    best_auc = roc_auc_score(test_labels, best_pred)\n","\n","    if AUC_list == True:\n","        return best_auc, best_k_neighbor, best_model, auc_list\n","\n","    else:\n","        return best_auc, best_k_neighbor, best_model, kNN_metrics"]},{"cell_type":"markdown","source":["##### `kNN_cross_validation()`"],"metadata":{"id":"HNhT8htn-QM-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"X7DYtm85rcyx"},"outputs":[],"source":["def kNN_cross_validation(l_matrix, labels, neighbor_list, k_fold):\n","    '''\n","    This function is basically a wrapper for the kNN_with_distance_matrix function, but with the addition of k-fold cross validation.\n","\n","    Inputs:\n","\n","        l_matrix:          The distance matrix(2D np array) of the bkg and sig events. Shape:(n, n)\n","\n","        labels:            An array of labels for which type of event(bkg/sig) each row/column belongs to. Shape(n,)\n","\n","        neighbor_list:     The list of neighbors wanted to be used for the kNN classification\n","\n","        k_fold:            The number of folds wanted to be used for the k-fold cross validation\n","\n","    Outputs:\n","\n","        np.mean(auc_list): The mean of the AUCs computed during the k-fold cross validation\n","\n","        np.std(auc_list):  The standard deviation of the AUCs computed during the k-fold cross validation\n","\n","        np.mean(best_k_list): The mean of the best-k-neighbors computed during the k-fold cross validation\n","\n","        np.std(best_k_list): The standard deviation of the best-k-neighbors computed during the k-fold cross validation\n","\n","        metrics_dict:      A dictionary containing all the metrics computed during the k-fold cross validation.\n","    '''\n","    length = l_matrix.shape[0]\n","    folded_length = length//k_fold\n","    index_list = []\n","\n","    for i in range(0, k_fold):\n","        index_list.append(list(range(i*folded_length, (i+1)*folded_length)))\n","\n","    auc_list = []\n","    best_k_list = []\n","    metrics_dict = {}\n","\n","    # perform k-fold cross validation\n","    for i in range(0, k_fold):\n","        new_index = index_list[i]\n","        for j in range(1,k_fold):\n","            new_index.extend(index_list[(i+j)%k_fold])\n","        l_matrix_new = l_matrix[new_index,:][:,new_index]\n","        labels_new = labels[new_index]\n","        best_auc, best_k, _, kNN_metrics = kNN_with_distance_matrix(l_matrix_new, labels_new, folded_length*(k_fold-2), folded_length, folded_length, neighbor_list, AUC_list=False)\n","        auc_list.append(best_auc)\n","        best_k_list.append(best_k)\n","        metrics_dict['repeat'+str(i)] = kNN_metrics\n","\n","    return np.mean(auc_list), np.std(auc_list), np.mean(best_k_list), np.std(best_k_list), metrics_dict"]},{"cell_type":"markdown","source":["##### `rNN_with_distance_matrix()`"],"metadata":{"id":"2Bm9WRDE-Ulx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"K5rG7TIBrcyx"},"outputs":[],"source":["def rNN_with_distance_matrix(l_matrix, labels, train_number, val_number, test_number, radius_list, AUC_list = False):\n","    '''\n","    Inputs:\n","\n","        l_matrix:          The distance matrix(2D np array) of the two types of jets. Shape:(njets, njets)\n","                          (NOTE: This matrix must be symmetric and tracless for it to be a meaningful OT distance matrix)\n","\n","        labels:           An array of labels for which type of particle each row/column belongs to. Shape(njets,)\n","\n","        train_number:     The number of jets used for training the kNN model, set to be the first \"train_number\" of rows/cols\n","\n","        val_number:       The number of jets used for the validation process(determining the best number of neighbors), set to be the \"val_number\" of rows/cols right after the training rows/cols\n","\n","        test_number:      The number of jets used for the testing process, set to be the \"test_number\" of rows/cols right after the validation rows/cols\n","\n","        radius_list:      The list of radii wanted to be used for the kNN classification\n","\n","        AUC_list:         Default is False. When set to True, return the whole list of AUCs during the validation process\n","\n","    Outputs:\n","\n","        best_auc:         The AUC computed using the best-k-neighbor returned by the validation process.\n","\n","        auc_list:         The entire list of AUC computed during the validation process. Only returned if AUC_list is set to be True\n","\n","    '''\n","\n","    l_matrix_train         = l_matrix[:train_number, :train_number]\n","    l_matrix_val_train     = l_matrix[train_number:train_number+val_number, :train_number]\n","    l_matrix_trainval      = l_matrix[:train_number+val_number, :train_number+val_number]\n","    l_matrix_test_trainval = l_matrix[train_number+val_number:test_number + train_number + val_number, :train_number+val_number]\n","\n","    train_labels           = labels[:train_number]\n","    val_labels             = labels[train_number:train_number+val_number]\n","    trainval_labels        = labels[:train_number+val_number]\n","    test_labels            = labels[train_number+val_number:test_number+train_number+val_number]\n","\n","    models, pred_vals = [], []\n","\n","    for radius in tqdm(radius_list, desc='Fitting Models'):\n","        model = RadiusNeighborsClassifier(radius=radius, metric='precomputed')\n","        model.fit(l_matrix_train, train_labels)\n","        pred_val = model.predict_proba(l_matrix_val_train)[:,1]\n","        models.append(model)\n","        pred_vals.append(pred_val)\n","\n","    auc_list = []\n","\n","    for pred in pred_vals:\n","        auc = roc_auc_score(val_labels, pred)\n","        auc_list.append(auc)\n","\n","    max_index = auc_list.index(max(auc_list))\n","\n","    best_radius = radius_list[max_index]\n","\n","    best_model = RadiusNeighborsClassifier(radius=best_radius, metric='precomputed')\n","\n","    best_model.fit(l_matrix_trainval, trainval_labels)\n","\n","    best_pred = best_model.predict_proba(l_matrix_test_trainval)[:,1]\n","\n","    best_auc = roc_auc_score(test_labels, best_pred)\n","\n","    if AUC_list == True:\n","        return best_auc, best_radius, best_model, auc_list\n","\n","    else:\n","        return best_auc, best_radius, best_model"]},{"cell_type":"markdown","source":["##### `rNN_cross_validation()`"],"metadata":{"id":"DOQDeI0U-Ypg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zBF1GXytrcyx"},"outputs":[],"source":["def rNN_cross_validation(l_matrix, labels, radius_list, k_fold = 5):\n","    '''\n","    This function is basically a wrapper function around the rNN_with_distance_matrix function. It performs a k-fold cross validation\n","    for the rNN model and returns the mean and standard deviation of the AUC and the best radius.\n","\n","    Inputs:\n","\n","        l_matrix:          The distance matrix(2D np array) of bkg and sig events. Shape:(n, n)\n","\n","        labels:            An array of labels for which type of event(bkg/sig) each row/column belongs to. Shape(n,)\n","\n","        radius_list:       The list of radii wanted to be used for the rNN classification\n","\n","        k_fold:            The number of folds wanted to be used for the cross validation process. Default is 5.\n","\n","    Outputs:\n","\n","        mean_auc:          The mean of the AUCs computed during the cross validation process\n","\n","        std_auc:           The standard deviation of the AUCs computed during the cross validation process\n","\n","        mean_best_radius:  The mean of the best radii computed during the cross validation process\n","\n","        std_best_radius:   The standard deviation of the best radii computed during the cross validation process\n","    '''\n","    length = l_matrix.shape[0]\n","    folded_length = length//k_fold\n","    index_list = []\n","\n","    for i in range(0, k_fold):\n","        index_list.append(list(range(i*folded_length, (i+1)*folded_length)))\n","\n","    auc_list = []\n","    best_radius_list = []\n","\n","    for i in range(0, k_fold):\n","        new_index = index_list[i]\n","        for j in range(1,k_fold):\n","            new_index.extend(index_list[(i+j)%k_fold])\n","        l_matrix_new = l_matrix[new_index,:][:,new_index]\n","        labels_new = labels[new_index]\n","        best_auc, best_radius, best_model = rNN_with_distance_matrix(l_matrix_new, labels_new, folded_length*(k_fold-2), folded_length, folded_length, radius_list, AUC_list=False)\n","        auc_list.append(best_auc)\n","        best_radius_list.append(best_radius)\n","\n","    return np.mean(auc_list), np.std(auc_list), np.mean(best_radius_list), np.std(best_radius_list)"]},{"cell_type":"markdown","source":["##### `SVM_with_distance_matrix()`"],"metadata":{"id":"tlHM0HZa-cs4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HSAMU4t_rcyy"},"outputs":[],"source":["def SVM_with_distance_matrix(l_matrix, labels, train_number, val_number, test_number, gamma_list, C_list, kernel = 'rbf', AUC_list = False):\n","    '''\n","    Inputs:\n","\n","        l_matrix:          The distance matrix(2D np array) of bkg and sig of events. Shape:(n, n)\n","\n","        labels:            An array of labels for which type of event(bkg/sig) each row/column belongs to. Shape(n,)\n","\n","        train_number:      The number of events used for training the SVM model, set to be the first \"train_number\" of rows/cols\n","\n","        val_number:        The number of events used for the validation process(determining the best hyperparameters), set to be the \"val_number\" of rows/cols right after the training rows/cols\n","\n","        test_number:       The number of events used for the testing process, set to be the \"test_number\" of rows/cols right after the validation rows/cols\n","\n","        gamma_list:        The list of gamma values wanted to be used for the SVM classification\n","\n","        C_list:            The list of C values wanted to be used for the SVM classification\n","\n","        kernel:            The kernel used for the SVM classification. Default is 'rbf'\n","\n","        AUC_list:          Default is False. When set to True, return the whole list of AUCs during the validation process\n","\n","    Outputs:\n","\n","        best_auc:          The AUC computed using the best hyperparameters returned by the validation process.\n","\n","        best_gamma:        The best gamma hyperparameter returned by the validation process.\n","\n","        best_C:            The best C hyperparameter returned by the validation process.\n","\n","        best_model:        The best SVM model returned by the validation process.\n","\n","        auc_list:          The entire list of AUC computed during the validation process. Only returned if AUC_list is set to be True\n","    '''\n","    models, pred_vals = [], []\n","\n","    i = 0\n","    train_labels           = labels[:train_number]\n","    val_labels             = labels[train_number:train_number+val_number]\n","    trainval_labels        = labels[:train_number+val_number]\n","    test_labels            = labels[train_number+val_number:test_number+train_number+val_number]\n","\n","    if kernel == 'rbf':\n","        for gamma in gamma_list:\n","        # for gamma in tqdm(gamma_list, desc='Gamma Loop'):\n","            l_matrix_gamma = np.exp(-gamma*l_matrix)\n","\n","            l_matrix_train         = l_matrix_gamma[:train_number, :train_number]\n","            l_matrix_val_train     = l_matrix_gamma[train_number:train_number+val_number, :train_number]\n","            l_matrix_trainval      = l_matrix_gamma[:train_number+val_number, :train_number+val_number]\n","            l_matrix_test_trainval = l_matrix_gamma[train_number+val_number:test_number + train_number + val_number, :train_number+val_number]\n","            models.append([])\n","            pred_vals.append([])\n","\n","            for C in C_list:\n","                model = SVC(gamma=gamma, C=C, kernel='precomputed')\n","                model.fit(l_matrix_train, train_labels)\n","                pred_val = model.predict(l_matrix_val_train)\n","                models[i].append(model)\n","                pred_vals[i].append(pred_val)\n","\n","            i += 1\n","\n","    else:\n","        l_matrix_train         = l_matrix[:train_number, :train_number]\n","        l_matrix_val_train     = l_matrix[train_number:train_number+val_number, :train_number]\n","        l_matrix_trainval      = l_matrix[:train_number+val_number, :train_number+val_number]\n","        l_matrix_test_trainval = l_matrix[train_number+val_number:test_number + train_number + val_number, :train_number+val_number]\n","\n","        train_labels           = labels[:train_number]\n","        val_labels             = labels[train_number:train_number+val_number]\n","        trainval_labels        = labels[:train_number+val_number]\n","        test_labels            = labels[train_number+val_number:test_number+train_number+val_number]\n","\n","    auc_list = []\n","\n","    i = 0\n","\n","    for pred_row in pred_vals:\n","        auc_list.append([])\n","        for pred in pred_row:\n","            auc = roc_auc_score(val_labels, pred)\n","            auc_list[i].append(auc)\n","        i += 1\n","\n","    auc_array = np.array(auc_list)\n","    max_index = np.unravel_index(auc_array.argmax(), auc_array.shape)\n","\n","    best_gamma = gamma_list[max_index[0]]\n","    best_C = C_list[max_index[1]]\n","\n","    l_matrix_gamma = np.exp(-best_gamma*l_matrix)\n","\n","    l_matrix_train         = l_matrix_gamma[:train_number, :train_number]\n","    l_matrix_val_train     = l_matrix_gamma[train_number:train_number+val_number, :train_number]\n","    l_matrix_trainval      = l_matrix_gamma[:train_number+val_number, :train_number+val_number]\n","    l_matrix_test_trainval = l_matrix_gamma[train_number+val_number:test_number + train_number + val_number, :train_number+val_number]\n","\n","    best_model = SVC(gamma=best_gamma, C = best_C, kernel='precomputed')\n","\n","    best_model.fit(l_matrix_trainval, trainval_labels)\n","\n","    best_pred = best_model.predict(l_matrix_test_trainval)\n","\n","    best_auc = roc_auc_score(test_labels, best_pred)\n","\n","    if AUC_list == True:\n","        return best_auc, best_gamma, best_C, best_model, auc_list\n","\n","    else:\n","        return best_auc, best_gamma, best_C, best_model"]},{"cell_type":"markdown","source":["##### `SVM_cross_validation()`"],"metadata":{"id":"m9KILrvx-hdp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"iYIyU9OVrcyy"},"outputs":[],"source":["def SVM_cross_validation(l_matrix, labels, gamma_list, C_list, kernel = 'rbf', k_fold = 5):\n","    '''\n","    This function is basically a wrapper function around the SVM_with_distance_matrix function. It performs a k-fold cross validation\n","    for the SVM model and returns the mean and standard deviation of the AUC and the best hyperparameters.\n","\n","    Inputs:\n","        l_matrix:          The distance matrix(2D np array) of the signal and bkg samples. Shape:(n, n)\n","\n","        labels:            An array of labels for which type of event(bkg/sig) each row/column belongs to. Shape(n,)\n","\n","        gamma_list:        The list of gamma values wanted to be used for the SVM classification\n","\n","        C_list:            The list of C values wanted to be used for the SVM classification\n","\n","        kernel:            The kernel wanted to be used for the SVM classification. Default is 'rbf'\n","\n","        k_fold:            The number of folds wanted to be used for the cross validation. Default is 5\n","\n","    Outputs:\n","        auc_list:          The list of AUCs computed during the cross validation process\n","\n","        best_gamma_list:   The list of best gamma values computed during the cross validation process\n","\n","        best_C_list:       The list of best C values computed during the cross validation process\n","    '''\n","    length = l_matrix.shape[0]\n","    folded_length = length//k_fold\n","    index_list = []\n","\n","    for i in range(0, k_fold):\n","        index_list.append(list(range(i*folded_length, (i+1)*folded_length)))\n","\n","    auc_list = []\n","    best_gamma_list = []\n","    best_C_list = []\n","\n","    for i in range(0, k_fold):\n","        new_index = index_list[i]\n","        for j in range(1,k_fold):\n","            new_index.extend(index_list[(i+j)%k_fold])\n","        l_matrix_new = l_matrix[new_index,:][:,new_index]\n","        labels_new = labels[new_index]\n","\n","        best_auc, best_gamma, best_C, best_model = SVM_with_distance_matrix(l_matrix_new, labels_new, folded_length*(k_fold-2), folded_length, folded_length, gamma_list, C_list, AUC_list=False)\n","\n","        auc_list.append(best_auc)\n","        best_gamma_list.append(best_gamma)\n","        best_C_list.append(best_C)\n","\n","    return auc_list, best_gamma_list, best_C_list"]},{"cell_type":"markdown","source":["##### `OneClassSVM_with_distance_matrix()`"],"metadata":{"id":"K8ZqW6Y5-l1u"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OHPX5aylrcyy"},"outputs":[],"source":["def OneClassSVM_with_distance_matrix(train_matrix, test_matrix, test_labels, gamma, nu):\n","    '''\n","    Inputs:\n","\n","        train_matrix:     The train distance matrix(2D np array). Shape:(n, n)\n","                          (NOTE: This matrix must be symmetric and tracless for it to be a meaningful OT distance matrix)\n","\n","        test_matrix:      The distance matrix(2D np array) of the two types of jets. Shape:(m, n)\n","                          (NOTE: This matrix is usually not symmetric and tracelsss)\n","\n","        test_labels:      An array of binary labels for which type of particle each row/column belongs to. Shape(m,)\n","\n","        gamma:            The gamma parameter for the RBF kernel\n","\n","        nu:               The nu parameter for the OneClassSVM model\n","\n","    Outputs:\n","\n","        auc, f1_score, ROC_metrics: The performance metrics of the OneClassSVM model\n","    '''\n","    train_matrix_gamma = np.exp(-gamma*train_matrix)\n","    test_matrix_gamma = np.exp(-gamma*test_matrix)\n","\n","    model = svm.OneClassSVM(nu = nu, kernel='precomputed')\n","    model.fit(train_matrix_gamma)\n","\n","    pred = model.predict(test_matrix_gamma)\n","\n","    ROC_metrics = kNN_ROC_metrics(test_labels, pred, Interpolate = True)\n","\n","    auc = roc_auc_score(test_labels, pred)\n","    f1_score = metrics.f1_score(test_labels, pred)\n","\n","    return auc, f1_score, ROC_metrics"]},{"cell_type":"markdown","source":["##### `kNN_ROC_metrics()`"],"metadata":{"id":"HW7jahrk-ptL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"O4_LP75srcyy"},"outputs":[],"source":["def kNN_ROC_metrics(labels, prediction_proba, SIreg=0.0001, Interpolate=False):\n","    '''\n","    Calculate ROC metrics for kNN classifier\n","    Inputs:\n","        labels:             An array of binary labels for background and signal.\n","        prediction_proba:   An array of probabilities for each row/column being a signal jet.\n","        SIreg:              The regularization parameter for the Significance Improvement (SI) curve. Default is 0.0001\n","        Interpolate:        Default is False. When set to True, interpolate the ROC curve to have 101 points.\n","    Outputs:\n","        auc, fpr, tpr, si, fprInv, f1_score: The AUC, FPR, TPR, SI, FPR^{-1}, and F1 score of the ROC curve.\n","    '''\n","    fpr_raw, tpr_raw, _ = roc_curve(labels, prediction_proba)\n","\n","    if Interpolate:\n","        base_tpr = np.linspace(0, 1, 101) # 0.00, 0.01, ..., 1.0\n","        tpr = base_tpr\n","        fpr = np.interp(base_tpr, tpr_raw, fpr_raw)\n","    else:\n","        tpr = tpr_raw\n","        fpr = fpr_raw\n","\n","    auc = roc_auc_score(labels, prediction_proba)\n","\n","    fpr_sqrt = np.sqrt(fpr + SIreg)\n","    si = tpr/fpr_sqrt\n","\n","    fpr_masked = ma.masked_where(fpr==0., fpr)\n","    fprInv = 1./fpr_masked\n","\n","    P   = np.count_nonzero(labels == 1) # Number of signal (labels==1)\n","    N   = np.count_nonzero(labels == 0) # Number of background (labels==0)\n","    tp  = P*tpr\n","    fp  = N*fpr\n","    fn  = P*(1-tpr) # P*fnr\n","    f1_score = (2*tp)/(2*tp + fp + fn)\n","\n","    return auc, fpr, tpr, si, fprInv, f1_score"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}