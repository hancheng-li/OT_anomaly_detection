{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "import numpy.ma as ma\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "%matplotlib inline\n",
    "\n",
    "import h5py\n",
    "import ot\n",
    "from numpy.random import Generator, PCG64\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigAliasList    = ['sig_A', 'sig_h0', 'sig_hch', 'sig_LQ']\n",
    "sigFilenameList = ['Ato4l_lepFilter_13TeV_filtered.h5', 'hToTauTau_13TeV_PU20_filtered.h5', 'hChToTauNu_13TeV_PU20_filtered.h5', 'leptoquark_LOWMASS_lepFilter_13TeV_filtered.h5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Set base directory and data directory path --#\n",
    "basePath   = '/Users/hanchengli/Dropbox/AnomalyDetection/OnML4Jets2021DataChallenge/'\n",
    "# basePath   = '/Users/bobli/Dropbox/AnomalyDetection/OnML4Jets2021DataChallenge/'\n",
    "dataPath   = 'Data/'\n",
    "\n",
    "bkgPath    = basePath+dataPath+'background_for_training.h5'\n",
    "augPath    = '/Users/hanchengli/Desktop/Research/anomalyAugmented_background_for_training.h5'\n",
    "# augPath    = '/Users/bobli/Desktop/Research/anomalyAugmented_background_for_training.h5'\n",
    "sigPathList = []\n",
    "for x in sigFilenameList:\n",
    "  sigPathList.append(basePath+dataPath+x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hanchengli/Library/CloudStorage/Dropbox/AnomalyDetection/OnML4Jets2021DataChallenge/anomaly_detection_code/functions\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Dropbox/AnomalyDetection/OnML4Jets2021DataChallenge/anomaly_detection_code/functions\n",
    "%run centralFunctions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to store all the data\n",
    "dataDict = {}\n",
    "dataDict['bkg'] = h5py.File(bkgPath, 'r')\n",
    "dataDict['aug'] = h5py.File(augPath, 'r')\n",
    "\n",
    "for i in range(len(sigAliasList)):\n",
    "  alias   = sigAliasList[i]\n",
    "  sigPath = sigPathList[i]\n",
    "  dataDict[alias] = h5py.File(sigPath, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the data in dictionary as numpy arrays\n",
    "bkg_data = dataDict['bkg']['Particles'][:, :, 0:3]\n",
    "aug_data = dataDict['aug']['augBkg'][:, :, 0:3]\n",
    "sig_data = {}\n",
    "\n",
    "for alias in sigAliasList:\n",
    "  sig_data[alias] = dataDict[alias]['Particles'][:, :, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set basic parameters\n",
    "nEvents = 1000\n",
    "random_state = Generator(PCG64(1))\n",
    "np.random.seed(100)\n",
    "OTSCHEME = {}\n",
    "OTSCHEME['normPT'] = False\n",
    "OTSCHEME['balanced'] = True\n",
    "OTSCHEME['noZeroPad'] = False\n",
    "OTSCHEME['individualOT'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to store all the randomly sampled events\n",
    "events = {}\n",
    "events['bkg'] = randomDataSample(bkg_data, nEvents + nEvents * 5, random_state)\n",
    "random_state = Generator(PCG64(2))\n",
    "events['aug'] = randomDataSample(aug_data, nEvents, random_state)\n",
    "\n",
    "for alias in sigAliasList:\n",
    "    events[alias] = randomDataSample(sig_data[alias], nEvents * 5, random_state)\n",
    "\n",
    "del bkg_data, sig_data, aug_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000000/4000000 [05:53<00:00, 11305.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# create the test dataset\n",
    "train_events = np.concatenate((events['bkg'][:nEvents], events['aug']))\n",
    "train_labels = np.concatenate((np.zeros(nEvents), np.ones(nEvents)))\n",
    "\n",
    "# randomly permute the test dataset\n",
    "permutation = np.random.permutation(len(train_labels))\n",
    "train_events = train_events[permutation]\n",
    "train_labels = train_labels[permutation]\n",
    "\n",
    "# calculate the train distance matrix\n",
    "train_matrix = calcOTDistance(train_events, train_events, OTSCHEME, '3D', Matrix=True)\n",
    "\n",
    "# list of neighbor numbers\n",
    "neighbor_list = list(range(5, 500,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Models: 100%|██████████| 50/50 [00:01<00:00, 47.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the best hyperparameters\n",
    "best_auc, best_k, best_model, _ = kNN_with_distance_matrix(train_matrix, train_labels, nEvents * 3 // 2, nEvents * 2 // 5, nEvents // 10, neighbor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000000/4000000 [10:47<00:00, 6181.81it/s]\n",
      "100%|██████████| 4000000/4000000 [10:59<00:00, 6062.42it/s]\n",
      "100%|██████████| 4000000/4000000 [10:20<00:00, 6447.74it/s]\n",
      "100%|██████████| 4000000/4000000 [10:32<00:00, 6328.31it/s]\n",
      "100%|██████████| 4000000/4000000 [10:29<00:00, 6355.21it/s]\n",
      "100%|██████████| 4000000/4000000 [10:27<00:00, 6372.39it/s]\n",
      "100%|██████████| 4000000/4000000 [10:32<00:00, 6321.30it/s]\n",
      "100%|██████████| 4000000/4000000 [10:30<00:00, 6347.98it/s]\n",
      "100%|██████████| 4000000/4000000 [10:23<00:00, 6415.49it/s]\n",
      "100%|██████████| 4000000/4000000 [10:30<00:00, 6348.90it/s]\n",
      "100%|██████████| 4000000/4000000 [10:38<00:00, 6262.18it/s]\n",
      "100%|██████████| 4000000/4000000 [10:31<00:00, 6338.06it/s]\n",
      "100%|██████████| 4000000/4000000 [10:40<00:00, 6246.48it/s]\n",
      "100%|██████████| 4000000/4000000 [10:36<00:00, 6286.87it/s]\n",
      "100%|██████████| 4000000/4000000 [17:17<00:00, 3853.63it/s] \n",
      "100%|██████████| 4000000/4000000 [10:47<00:00, 6175.00it/s]\n",
      "100%|██████████| 4000000/4000000 [10:47<00:00, 6178.52it/s]\n",
      "100%|██████████| 4000000/4000000 [10:16<00:00, 6487.69it/s]\n",
      "100%|██████████| 4000000/4000000 [10:14<00:00, 6510.28it/s]\n",
      "100%|██████████| 4000000/4000000 [10:14<00:00, 6509.50it/s]\n"
     ]
    }
   ],
   "source": [
    "aucs = np.zeros((len(sigAliasList),5))\n",
    "scoreDict = {}\n",
    "\n",
    "# iterate over all signals and repeat the process 5 times to get the metrics\n",
    "for j, alias in enumerate(sigAliasList):\n",
    "    scoreDict[alias] = {}\n",
    "    for i in range(0, 5): \n",
    "        test_events = np.concatenate((events['bkg'][nEvents*(i+1):nEvents*(i+2)], events[alias][nEvents*i:nEvents*(i+1)]))\n",
    "        test_labels = np.concatenate((np.zeros(nEvents), np.ones(nEvents)))\n",
    "        test_matrix = calcOTDistance_non_square(test_events, train_events, OTSCHEME, '3D', Matrix=True)\n",
    "        model = KNeighborsClassifier(n_neighbors=best_k, metric='precomputed')\n",
    "        model.fit(train_matrix, train_labels)\n",
    "        \n",
    "        pred = model.predict_proba(test_matrix)\n",
    "        kNN_metrics = kNN_ROC_metrics(test_labels, pred[:, 1], Interpolate=True)\n",
    "        auc = roc_auc_score(test_labels, pred[:, 1])\n",
    "        \n",
    "        scoreDict[alias]['repeat'+str(i)] = kNN_metrics\n",
    "        aucs[j, i] = auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sig_A': 0.8324266999999999, 'sig_h0': 0.6864171, 'sig_hch': 0.8134343000000002, 'sig_LQ': 0.7569604}\n",
      "{'sig_A': 0.005851705653909785, 'sig_h0': 0.008349107349890724, 'sig_hch': 0.006917888020198079, 'sig_LQ': 0.007814717143953435}\n"
     ]
    }
   ],
   "source": [
    "mean_aucs = {}\n",
    "std_aucs = {}\n",
    "for i, alias in enumerate(sigAliasList):\n",
    "    mean_aucs[alias] = np.mean(aucs, axis=1)[i]\n",
    "    std_aucs[alias] = np.std(aucs, axis=1)[i]\n",
    "\n",
    "print(mean_aucs)\n",
    "\n",
    "print(std_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.83191   0.8421575 0.8237975 0.833035  0.8312335]\n",
      " [0.6968705 0.693912  0.6832435 0.684681  0.6733785]\n",
      " [0.8202505 0.8143665 0.809467  0.8023495 0.820738 ]\n",
      " [0.7497665 0.7652115 0.7600545 0.745738  0.7640315]]\n"
     ]
    }
   ],
   "source": [
    "print(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hanchengli/Library/CloudStorage/Dropbox/AnomalyDetection/OnML4Jets2021DataChallenge/anomaly_detection_code/distance_matrix_classification/kNN\n"
     ]
    }
   ],
   "source": [
    "transposed_dict = {\n",
    "    col: {row: scoreDict[row][col] for row in scoreDict}\n",
    "    for col in scoreDict[next(iter(scoreDict))]\n",
    "}\n",
    "# Convert NumPy arrays in the lists to regular lists\n",
    "converted_dict = {\n",
    "    row: {\n",
    "        col: [transposed_dict[row][col][0]] + [arr.tolist() for arr in transposed_dict[row][col][1:]]\n",
    "        for col in transposed_dict[row]\n",
    "    }\n",
    "    for row in transposed_dict\n",
    "}\n",
    "%cd ~/Dropbox/AnomalyDetection/OnML4Jets2021DataChallenge/anomaly_detection_code/distance_matrix_classification/kNN\n",
    "with open('kNN_3D_anomalyaug.json', 'w') as json_file:\n",
    "    json.dump(converted_dict, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
